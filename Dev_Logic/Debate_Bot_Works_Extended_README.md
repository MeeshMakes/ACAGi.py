# Project Documentation

## Table of Contents
- [Overview](#overview)
- [Python Modules](#python-modules)
- [Other Files](#other-files)

## Overview
This README was generated automatically by analyzing the project contents. Python modules are parsed for docstrings, classes, and functions. Image files are embedded as previews. Executable files (.exe) are listed by name; their contents are intentionally skipped.

## Python Modules

- `Debate_AI_Old\Analyze_All.py`
- `Debate_AI_Old\debate 4\Debate_AI\Analyze_All.py`
- `Debate_AI_Old\debate 4\Debate_AI\Analyze_folders.py`
- `Debate_AI_Old\debate 4\Debate_AI\bots.py`
- `Debate_AI_Old\debate 4\Debate_AI\Debate_AI.py`
- `Debate_AI_Old\debate 4\Debate_AI\Quick_error.py`
- `Debate_AI_Old\debate 4\Debate_AI\schema_viewer.py`
- `Debate_AI_Old\debate 4\Debate_AI\session_manager.py`
- `Debate_AI_Old\debate 4\Debate_AI\tts.py`
- `Debate_AI_Old\Debate_AI.py`
- `Debate_AI_Old\Debate_AI\Analyze_folders.py`
- `Debate_AI_Old\Debate_AI\Debate_AI.py`
- `Debate_AI_Old\Debate_AI\Debate_AI_Full.py`
- `Debate_AI_Old\Debate_AI\engines\Analyze_folders.py`
- `Debate_AI_Old\Debate_AI\engines\config_panel_ui.py`
- `Debate_AI_Old\Debate_AI\engines\debate_engine.py`
- `Debate_AI_Old\Debate_AI\engines\debate_rules.py`
- `Debate_AI_Old\Debate_AI\engines\history_manager.py`
- `Debate_AI_Old\Debate_AI\engines\jarvis_engine.py`
- `Debate_AI_Old\Debate_AI\engines\jarvis_injector.py`
- `Debate_AI_Old\Debate_AI\engines\prompt_router.py`
- `Debate_AI_Old\Debate_AI\engines\prompt_templates.py`
- `Debate_AI_Old\Debate_AI\engines\session_manager.py`
- `Debate_AI_Old\Debate_AI\engines\structure_controller.py`
- `Debate_AI_Old\Debate_AI\engines\structure_panel_ui.py`
- `Debate_AI_Old\Debate_AI\Jarvis_Driver.py`
- `Debate_AI_Old\Debate_AI\monitor_debate.py`
- `Debate_AI_Old\Debate_AI\tts.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\Analyze_All.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\Analyze_folders.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\Debate_AI.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\debate_core.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monitor_debate.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\old\Debate_AI.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\Quick_error.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\ui_components.py`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\utils.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\Analyze_All.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\Analyze_folders.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\engine.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\history.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\jarvis_engine.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\prompt_router.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\structure.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\Debate_AI.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\Jarvis_Driver.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\tts.py`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\ui\config_panel.py`
- `Debate_AI_Old\Debate_AIu.py`
- `Debate_AI_Old\idea_bot.py`
- `Debate_AI_Old\monitor_debate.py`
- `Debate_AI_Old\old\tts.py`
- `Debate_AI_Old\working_debate.py`
- `Dual_AI\Analyze_All.py`
- `Dual_AI\Analyze_folders.py`
- `Dual_AI\Dual_AI.py`
- `Dual_AI\Quick_error.py`
- `Talking_Bots\Analyze_All.py`
- `Talking_Bots\Analyze_folders.py`
- `Talking_Bots\run_all.py`
- `Talking_Bots\talking_bots.py`
- `Talking_Bots\tts.py`

## Other Files

- `Debate_AI_Old\Conversation 2.txt`
- `Debate_AI_Old\debate 4\Debate_AI\analyze.txt`
- `Debate_AI_Old\debate 4\Debate_AI\api\api_key.txt`
- `Debate_AI_Old\debate 4\Debate_AI\quick_error.log`
- `Debate_AI_Old\Debate_AI\analyze.txt`
- `Debate_AI_Old\Debate_AI\api\api_key.txt`
- `Debate_AI_Old\Debate_AI\api\oher\api_key.txt`
- `Debate_AI_Old\Debate_AI\api\oher\old.txt`
- `Debate_AI_Old\Debate_AI\data\history.log`
- `Debate_AI_Old\Debate_AI\data\structure.json`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\debate_engine.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\debate_rules.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\history_manager.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\jarvis_engine.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\jarvis_injector.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\prompt_router.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\__pycache__\prompt_templates.cpython-313.pyc`
- `Debate_AI_Old\Debate_AI\engines\analyze.txt`
- `Debate_AI_Old\Debate_AI\quick_error.log`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\api\api_key.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\conversations\debate_log.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\error.log`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\formats\self_diagnostic_debate.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Blue_Closing.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Blue_Opening.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Blue_Rebuttal 1.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Blue_Rebuttal 2.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Red_Closing.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Red_Opening.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Red_Rebuttal 1.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\monologue\Red_Rebuttal 2.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\quick_error.log`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752819240\Blue_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752819240\index.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752819240\log.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752819240\Red_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752820834\Blue_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752820834\index.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752820834\log.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752820834\Red_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830014\Blue_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830014\epoch_001\debate_plan.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830014\index.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830014\log.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830014\Red_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830924\Blue_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830924\epoch_001\debate_plan.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830924\log.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752830924\Red_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752832207\Blue_sem.jsonl`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752832207\epoch_001\talking_points.json`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752832207\log.txt`
- `Debate_AI_Old\Debate_AI_3\Debate_AI\sessions\sess_1752832207\Red_sem.jsonl`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\api\api_key.txt`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\bots\david_log.txt`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\bots\jarvis_log.txt`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\bots\zira_log.txt`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\data\history.log`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\data\structure.json`
- `Debate_AI_Old\Debate_AI_fail\Debate_AI\quick_error.log`
- `Debate_AI_Old\Good Logic Conversation.txt`
- `Debate_AI_Old\GOOD LOGIC.txt`
- `Debate_AI_Old\old_info.txt`
- `Debate_AI_Old\The best logic ever.txt`
- `Dual_AI\analyze.txt`
- `Dual_AI\api\api_key.txt`
- `Dual_AI\data\conversation.txt`
- `Dual_AI\data\schema.json`
- `Dual_AI\data\tts_control.txt`
- `Talking_Bots\analyze.txt`
- `Talking_Bots\config\settings.json`
- `Talking_Bots\config\tts_settings.json`
- `Talking_Bots\logs\debate.log`
- `Talking_Bots\logs\talking_bots.log`
- `Talking_Bots\logs\tts_daemon.log`
- `Talking_Bots\tts_queue.txt`


## Detailed Module Analyses


## Module `Debate_AI_Old\Analyze_All.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get the script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")

        for root, dirs, files in os.walk(start_path):
            # Ensure only directories below the script's location are analyzed
            if not root.startswith(start_path):
                continue  # Skip any directory outside the scope

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and analyze.txt from files list
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file_path = os.path.join(root, f)
                file.write(f"{file_indent}|-- {f}\n")

                # Capture file content
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f_content:
                        file_data = f_content.read()
                        file.write(f"\n{file_indent}    --- File Content ---\n")
                        file.write(f"{file_data}\n")
                        file.write(f"{file_indent}    -------------------\n\n")
                except Exception as e:
                    file.write(f"{file_indent}    [Error reading file: {str(e)}]\n\n")

    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.abspath(os.path.dirname(__file__))  # Get the script's location
    analyze_folders(script_directory)  # Analyze only from script's directory downward
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI.py`

```python
#!/usr/bin/env python3
"""
Debate_AI.py – Debate Simulator with Dockable & Splitter UI
• Dockable panels for David (Blue), Zira (Red), Session control
• Central vertical splitter: Conversation (top) & Debate Graph (bottom)
• Color‑coded panels & bubbles
• Provider/model selectors per agent
• Session metadata display (session name + epoch)
• Real‑time Node creation and vector insertion
• Rolling window of last N pairs + semantic recall
• Dynamic schema management (global & session)
• Full micro‑loop in DebateWorker with error handling
• Configurable recall and TTS controls, persisted to config/user_config.json
• “Tools” menu stubs for code analysis & dataset creation
• Automatic session start, archiving into sessions/archive/
• TTS integration and styled conversation window
• Voice‑selection dropdowns for each bot
• Ensures David (Blue) speaks first, then Zira (Red)
"""

import sys, os, json, uuid, time, shutil
from datetime import datetime

# Qt imports
from PyQt6.QtWidgets import (
    QApplication, QMainWindow, QWidget, QSplitter,
    QVBoxLayout, QHBoxLayout, QLabel, QPushButton,
    QComboBox, QLineEdit, QSpinBox, QCheckBox,
    QScrollArea, QFrame, QTreeWidget, QTreeWidgetItem,
    QDockWidget, QFileDialog, QMessageBox
)
from PyQt6.QtGui import QAction
from PyQt6.QtCore import Qt, QThread, pyqtSignal, QCoreApplication

# ensure local modules importable
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)

# --- CONFIG & PERSISTENCE ---
CONFIG_DIR   = os.path.join(SCRIPT_DIR, "config")
USER_CONF    = os.path.join(CONFIG_DIR, "user_config.json")
DEFAULT_CONF = os.path.join(CONFIG_DIR, "default_config.json")
_default_cfg = {
    "rolling_pairs": 10,
    "recall_top_k": 5,
    "recall_index": "both",
    "store_conclusions_global": False,
    "tts_enabled": True,
    "debug": False
}

def load_config():
    cfg = _default_cfg.copy()
    try:
        with open(DEFAULT_CONF, "r", encoding="utf-8") as f:
            cfg.update(json.load(f))
    except: pass
    try:
        with open(USER_CONF, "r", encoding="utf-8") as f:
            cfg.update(json.load(f))
    except:
        os.makedirs(CONFIG_DIR, exist_ok=True)
        with open(USER_CONF, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
    return cfg

def save_config():
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(USER_CONF, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)

config = load_config()
ROLLING_PAIRS = config["rolling_pairs"]

# --- OPTIONAL MODULES ---
try:
    from schema_engine import SchemaEngine
except ImportError:
    SchemaEngine = None

try:
    import code_analyzer
except ImportError:
    code_analyzer = None

try:
    import dataset_generator
except ImportError:
    dataset_generator = None

# --- TTS LOADING ---
TTSManager = None
for name in ("tts","TTS"):
    path = os.path.join(SCRIPT_DIR, "ui", f"{name}.py")
    if not os.path.isfile(path): continue
    import importlib.util
    spec = importlib.util.spec_from_file_location(name, path)
    mod = importlib.util.module_from_spec(spec)
    try:
        spec.loader.exec_module(mod)
        TTSManager = getattr(mod, "TTSManager", None)
        if TTSManager: break
    except: pass
if TTSManager is None:
    raise ModuleNotFoundError("ui/tts.py missing TTSManager")

# --- CORE MANAGERS ---
from graph_manager      import GraphContextManager, Node
from session_manager    import SessionManager
from vector_memory      import VectorMemoryManager
from persona_engine     import PersonaEngine

# --- DATA PATHS ---
DATA_DIR        = os.path.join(SCRIPT_DIR, "data")
GLOBAL_MEM_DIR  = os.path.join(DATA_DIR, "global_memory")
DATASETS_DIR    = os.path.join(DATA_DIR, "datasets")
LOGS_DIR        = os.path.join(DATA_DIR, "logs")
SESSIONS_DIR    = os.path.join(SCRIPT_DIR, "sessions")
ACTIVE_DIR      = os.path.join(SESSIONS_DIR, "active")
ARCHIVE_DIR     = os.path.join(SESSIONS_DIR, "archive")

for d in (DATA_DIR, GLOBAL_MEM_DIR, DATASETS_DIR, LOGS_DIR, ACTIVE_DIR, ARCHIVE_DIR):
    os.makedirs(d, exist_ok=True)


# --- DebateWorker ---
class DebateWorker(QThread):
    new_node        = pyqtSignal(Node)
    debate_finished = pyqtSignal()

    def __init__(self, topic, personaA, personaB,
                 graph_mgr, session_mgr, global_mgr, vector_mgr,
                 use_web, rounds, store_global):
        super().__init__()
        self.topic         = topic
        self.personaA      = personaA
        self.personaB      = personaB
        self.graph_mgr     = graph_mgr
        self.session_mgr   = session_mgr
        self.global_mgr    = global_mgr
        self.vector_mgr    = vector_mgr
        self.use_web       = use_web
        self.rounds        = rounds
        self.store_global  = store_global
        self._stop         = False

    def stop(self):
        self._stop = True

    def run(self):
        for _ in range(self.rounds):
            if self._stop: break
            # start new epoch
            self.session_mgr.start_epoch()

            for side, persona in (('A', self.personaA), ('B', self.personaB)):
                if self._stop: break

                # build prompt
                hist = self.graph_mgr.serialize_history().splitlines()
                recent = "\n".join(hist[-2*config["rolling_pairs"]:])
                recalls = self.vector_mgr.semantic_search(
                    query=self.topic,
                    top_k=config["recall_top_k"],
                    partitions=config["recall_index"]
                )
                semantic = "\n".join(
                    f"• {(payload.get('snippet') or text).replace('\\n',' ')[:150]}…"
                    for _,_,_,payload,text in recalls
                )
                prompt = (
                    f"Debate Topic: {self.topic}\n\n"
                    f"Recent conversation:\n{recent}\n\n"
                    f"Relevant memory:\n{semantic}\n\n"
                    f"{persona.label}, please make your argument."
                ) + ("\n\n(You may consult the web.)" if self.use_web else "")

                # call LLM
                try:
                    out = persona.call_llm(prompt)
                    status = "open"
                except Exception as e:
                    out = f"[LLM Error: {e}]"
                    status = "error"

                # detect schema drift
                intent = None
                drift  = False
                if "[New Intent]:" in out:
                    parts = out.split("[New Intent]:",1)[1].splitlines()
                    intent = parts[0].strip()
                    drift  = True

                node = Node(
                    id=str(uuid.uuid4()),
                    author=persona.label,
                    role=side,
                    type="claim" if side=='A' else "rebuttal",
                    content=out,
                    tags=[f"topic:{self.topic}", f"epoch:{self.session_mgr.current_epoch:04d}"],
                    references=[],
                    status=status,
                    timestamp=datetime.utcnow().isoformat(),
                    intent=intent,
                    schema_drift=drift
                )

                # persist
                self.graph_mgr.add_node(node)
                self.session_mgr.save_node(node)

                # embeddings
                try:
                    self.vector_mgr.insert_node(node)
                    if status=="open" and self.store_global:
                        self.global_mgr.insert_node(node)
                except Exception as e:
                    err = Node(
                        id=str(uuid.uuid4()), author="System", role=side,
                        type="error", content=f"[Embedding Err: {e}]",
                        tags=[], references=[], status="error",
                        timestamp=datetime.utcnow().isoformat(),
                        intent=None, schema_drift=False
                    )
                    self.new_node.emit(err)

                self.new_node.emit(node)
                self.msleep(300)

        # archive
        sid = self.session_mgr.session_id
        if sid:
            src = os.path.join(ACTIVE_DIR, sid)
            dst = os.path.join(ARCHIVE_DIR, sid)
            if os.path.isdir(src):
                shutil.move(src, dst)

        self.debate_finished.emit()


# --- Main Window ---
class DebateSimulatorApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate Simulator – Dockable Edition")
        self.resize(1400, 900)

        # Core managers
        self.graph_mgr    = GraphContextManager()
        self.session_mgr  = SessionManager(ACTIVE_DIR)
        self.global_mgr   = VectorMemoryManager(base_path=GLOBAL_MEM_DIR)
        self.vector_mgr   = None   # set per-session
        self.schema_mgr   = SchemaEngine() if SchemaEngine else None

        # Personas
        self.personaA = PersonaEngine.load("blue",  None)
        self.personaA.label = "David"
        self.personaB = PersonaEngine.load("red",   None)
        self.personaB.label = "Zira"

        # TTS
        self.tts = TTSManager()
        # monkey‑patch missing methods
        if not hasattr(self.tts, "set_voice"):
            def _sv(this, sp, vn):
                for v in this.engine.getProperty("voices"):
                    if vn.lower() in v.name.lower():
                        this.voice_map[sp] = v.id
            self.tts.set_voice = _sv.__get__(self.tts)
        if not hasattr(self.tts, "toggle_auto_read"):
            self.tts.auto_read_enabled = True
            def _to(this,e): this.auto_read_enabled=e
            self.tts.toggle_auto_read = _to.__get__(self.tts)

        # State
        self.worker       = None
        self.curr_session = None
        self.use_web      = False
        self.debug        = config["debug"]

        # Build UI
        for d in (ACTIVE_DIR, ARCHIVE_DIR):
            os.makedirs(d, exist_ok=True)
        self._build_menu()
        self._build_docks()
        self._init_layout()


    # ----- Menu bar -----
    def _build_menu(self):
        mb = self.menuBar(); mb.clear()
        file = mb.addMenu("File")
        file.addAction("Export Graph…", self._export_graph)
        file.addSeparator()
        dbg = QAction("Toggle Debug", self, checkable=True)
        dbg.setChecked(self.debug); dbg.triggered.connect(self._toggle_debug)
        file.addAction(dbg)
        file.addSeparator()
        file.addAction("Reset Settings", self._reset_config)
        file.addAction("Exit",          self.close)

        tools = mb.addMenu("Tools")
        tools.addAction("Analyze Folder…", self._analyze_folder)
        tools.addAction("Build Dataset…",  self._build_dataset)


    def _toggle_debug(self,_):
        self.debug = not self.debug; config["debug"]=self.debug; save_config()
        self._build_menu()


    # ----- Docks & Panels -----
    def _build_docks(self):
        # Session dock
        sess = QWidget(); sl=QHBoxLayout(sess)
        sl.addWidget(QLabel("Session:")); self.session_lbl=QLabel("—"); sl.addWidget(self.session_lbl)
        sl.addSpacing(20)
        sl.addWidget(QLabel("Epoch:"));   self.epoch_lbl  = QLabel("0"); sl.addWidget(self.epoch_lbl)
        sl.addStretch()
        # rolling pairs
        sl.addWidget(QLabel("Rolling Pairs:"))
        self.roll_sb = QSpinBox(); self.roll_sb.setRange(1,50)
        self.roll_sb.setValue(config["rolling_pairs"]); self.roll_sb.valueChanged.connect(self._on_roll_change)
        sl.addWidget(self.roll_sb)
        # recall k
        sl.addWidget(QLabel("Recall Top K:"))
        self.k_sb = QSpinBox(); self.k_sb.setRange(1,20)
        self.k_sb.setValue(config["recall_top_k"]); self.k_sb.valueChanged.connect(self._on_k_change)
        sl.addWidget(self.k_sb)
        # recall index
        sl.addWidget(QLabel("Recall Index:"))
        self.idx_cb=QComboBox(); self.idx_cb.addItems(["open","concluded","both"])
        self.idx_cb.setCurrentText(config["recall_index"]); self.idx_cb.currentTextChanged.connect(self._on_idx_change)
        sl.addWidget(self.idx_cb)
        # store global
        sl.addWidget(QLabel("Store Conclusions Globally:"))
        self.gl_cb = QCheckBox(); self.gl_cb.setChecked(config["store_conclusions_global"])
        self.gl_cb.stateChanged.connect(self._on_store_global); sl.addWidget(self.gl_cb)
        # TTS
        sl.addWidget(QLabel("TTS:"))
        self.tts_cb=QCheckBox(); self.tts_cb.setChecked(config["tts_enabled"])
        self.tts_cb.stateChanged.connect(self._on_tts_change); sl.addWidget(self.tts_cb)

        dock=QDockWidget("Session Control",self)
        dock.setWidget(sess); dock.setAllowedAreas(Qt.TopDockWidgetArea)
        self.addDockWidget(Qt.TopDockWidgetArea,dock)

        # David dock
        blue=QWidget(); bl=QVBoxLayout(blue); blue.setStyleSheet("background:#E3F2FD;")
        bl.addWidget(QLabel("<b>David (Blue)</b>"))
        bl.addWidget(QLabel("Provider:")); self.pA_cb=QComboBox(); self.pA_cb.addItems(["OpenAI","Ollama"])
        self.pA_cb.currentTextChanged.connect(lambda p:self._update_models(p,self.mA_cb,self.personaA))
        bl.addWidget(self.pA_cb)
        bl.addWidget(QLabel("Model:")); self.mA_cb=QComboBox(); bl.addWidget(self.mA_cb)
        bl.addWidget(QLabel("Topic:")); self.topic_le=QLineEdit(); bl.addWidget(self.topic_le)
        bl.addWidget(QLabel("Session Name:")); self.name_le=QLineEdit(); bl.addWidget(self.name_le)
        bl.addWidget(QLabel("Rounds:")); self.r_sb=QSpinBox(); self.r_sb.setRange(1,50); self.r_sb.setValue(3); bl.addWidget(self.r_sb)
        bl.addWidget(QLabel("Use Web")); self.web_chk=QCheckBox(); self.web_chk.stateChanged.connect(lambda s:setattr(self,"use_web",s==Qt.Checked)); bl.addWidget(self.web_chk)
        self.start_btn=QPushButton("Start Debate"); self.start_btn.clicked.connect(self.start_debate); bl.addWidget(self.start_btn)
        self.stop_btn =QPushButton("Stop Debate");  self.stop_btn.clicked.connect(self.stop_debate); self. stop_btn.setEnabled(False); bl.addWidget(self.stop_btn)
        # TTS controls
        bl.addWidget(QLabel("Auto Read")); self.auto_cb=QCheckBox(); self.auto_cb.setChecked(self.tts.auto_read_enabled)
        self.auto_cb.stateChanged.connect(lambda s:self.tts.toggle_auto_read(s==Qt.Checked)); bl.addWidget(self.auto_cb)
        bl.addWidget(QLabel("Read")); btn=QPushButton("Read"); btn.clicked.connect(self.tts.manual_read); bl.addWidget(btn)
        bl.addWidget(QLabel("Stop"));btn2=QPushButton("Stop");btn2.clicked.connect(self.tts.stop_monitoring);bl.addWidget(btn2)
        bl.addWidget(QLabel("Voice")); self.vA_cb=QComboBox()
        for v in self.tts.engine.getProperty("voices"): self.vA_cb.addItem(v.name)
        self.vA_cb.currentTextChanged.connect(lambda nm:self.tts.set_voice("David",nm)); bl.addWidget(self.vA_cb)

        dockB=QDockWidget("David Controls",self)
        dockB.setWidget(blue); dockB.setAllowedAreas(Qt.LeftDockWidgetArea)
        self.addDockWidget(Qt.LeftDockWidgetArea,dockB)

        # Zira dock (mirrors David)
        red=QWidget(); rl=QVBoxLayout(red); red.setStyleSheet("background:#FFEBEE;")
        rl.addWidget(QLabel("<b>Zira (Red)</b>"))
        rl.addWidget(QLabel("Provider:")); self.pB_cb=QComboBox(); self.pB_cb.addItems(["OpenAI","Ollama"])
        self.pB_cb.currentTextChanged.connect(lambda p:self._update_models(p,self.mB_cb,self.personaB))
        rl.addWidget(self.pB_cb)
        rl.addWidget(QLabel("Model:")); self.mB_cb=QComboBox(); rl.addWidget(self.mB_cb)
        rl.addWidget(QLabel("Voice:")); self.vB_cb=QComboBox()
        for v in self.tts.engine.getProperty("voices"): self.vB_cb.addItem(v.name)
        self.vB_cb.currentTextChanged.connect(lambda nm:self.tts.set_voice("Zira",nm)); rl.addWidget(self.vB_cb)

        dockR=QDockWidget("Zira Controls",self)
        dockR.setWidget(red); dockR.setAllowedAreas(Qt.RightDockWidgetArea)
        self.addDockWidget(Qt.RightDockWidgetArea,dockR)

        # Schema dock
        if self.schema_mgr:
            sd=QDockWidget("Schemas",self)
            w=QWidget(); l=QVBoxLayout(w)
            tree=QTreeWidget(); tree.setHeaderLabels(["ID","Type","Version"])
            for s in self.schema_mgr.list_schemas():
                tree.addTopLevelItem(QTreeWidgetItem([s.id, s.type, str(s.version)]))
            l.addWidget(tree); sd.setWidget(w)
            sd.setAllowedAreas(Qt.RightDockWidgetArea)
            self.addDockWidget(Qt.RightDockWidgetArea,sd)


    # ----- Config handlers -----
    def _on_roll_change(self, v):
        config["rolling_pairs"]=v; save_config()
    def _on_k_change(self, v):
        config["recall_top_k"]=v; save_config()
    def _on_idx_change(self, t):
        config["recall_index"]=t; save_config()
    def _on_store_global(self,s):
        config["store_conclusions_global"]= (s==Qt.Checked); save_config()
    def _on_tts_change(self,s):
        config["tts_enabled"]= (s==Qt.Checked); save_config()

    def _update_models(self, provider, combo, persona):
        combo.clear()
        persona.provider = provider
        combo.addItems(persona.available_models())


    # ----- Tools menu -----
    def _analyze_folder(self):
        d = QFileDialog.getExistingDirectory(self,"Select Folder", SCRIPT_DIR)
        if not d: return
        if code_analyzer:
            code_analyzer.analyze_directory(d)
            QMessageBox.information(self,"Analyze","Done.")
        else:
            QMessageBox.warning(self,"Analyze","Module missing.")

    def _build_dataset(self):
        if dataset_generator:
            dataset_generator.launch_ui()
        else:
            QMessageBox.warning(self,"Dataset","Module missing.")


    # ----- Debate lifecycle -----
    def start_debate(self):
        topic = self.topic_le.text().strip()
        if not topic:
            QMessageBox.warning(self,"Error","Enter a topic."); return
        name  = self.name_le.text().strip() or f"{topic.replace(' ','_')}_{int(time.time())}"

        # start session
        self.session_mgr.start_session(name)
        self.session_lbl.setText(name); self.epoch_lbl.setText("0")
        self.curr_session = name
        sess_dir = os.path.join(ACTIVE_DIR, name)
        os.makedirs(sess_dir, exist_ok=True)

        # per-session vector store
        self.vector_mgr = VectorMemoryManager(base_path=os.path.join(sess_dir,"vector"))
        # inject into personas
        self.personaA.vector_mgr = self.personaB.vector_mgr = self.vector_mgr

        # TTS
        self.tts.set_session_path(sess_dir)
        hist = os.path.join(sess_dir,"history.txt")
        open(hist,'w',encoding="utf-8").close()
        if config["tts_enabled"]:
            self.tts.start_monitoring()

        # clear UI
        for i in reversed(range(self.chat_layout.count())):
            w = self.chat_layout.itemAt(i).widget()
            if w: w.setParent(None)
        self.tree.clear()

        # apply personas
        self.personaA.provider = self.pA_cb.currentText()
        self.personaA.model    = self.mA_cb.currentText()
        self.personaB.provider = self.pB_cb.currentText()
        self.personaB.model    = self.mB_cb.currentText()

        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)

        self.worker = DebateWorker(
            topic,
            self.personaA, self.personaB,
            self.graph_mgr, self.session_mgr,
            self.global_mgr, self.vector_mgr,
            self.use_web, self.r_sb.value(),
            config["store_conclusions_global"]
        )
        self.worker.new_node.connect(self._on_new_node)
        self.worker.debate_finished.connect(self._on_finished)
        self.worker.start()


    def stop_debate(self):
        if self.worker: self.worker.stop()
        self.tts.stop_monitoring()
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)


    # ----- Node arrives -----
    def _on_new_node(self, node: Node):
        self.epoch_lbl.setText(f"{self.session_mgr.current_epoch:04d}")

        # chat bubble
        w = QWidget(); hl = QHBoxLayout(w)
        bub = QWidget(); bl = QVBoxLayout(bub)
        color = "#E3F2FD" if node.role=='A' else "#FFEBEE"
        bub.setStyleSheet(f"background:{color};padding:8px;border-radius:4px;")
        bl.addWidget(QLabel(f"<b>{node.author}</b>"))
        lbl = QLabel(node.content); lbl.setWordWrap(True)
        bl.addWidget(lbl)
        if node.role=='A':
            hl.addWidget(bub); hl.addStretch()
        else:
            hl.addStretch(); hl.addWidget(bub)
        self.chat_layout.addWidget(w)
        QCoreApplication.processEvents()
        self.scroll.verticalScrollBar().setValue(self.scroll.verticalScrollBar().maximum())

        # append history.txt
        sess_dir = os.path.join(ACTIVE_DIR, self.curr_session)
        with open(os.path.join(sess_dir,"history.txt"),"a",encoding="utf-8") as f:
            f.write(f"[{node.author}]\n")
            for line in node.content.splitlines():
                f.write(line+"\n")
            f.write("\n")

        # update graph pane
        item = QTreeWidgetItem([node.id,node.author,node.type,node.status,node.timestamp])
        brush = Qt.GlobalColor.green if node.status=='closed' else Qt.GlobalColor.darkYellow
        for i in range(5): item.setForeground(i, brush)
        self.tree.addTopLevelItem(item)

        # snapshot last_graph.json
        with open(os.path.join(SCRIPT_DIR,"last_graph.json"),"w",encoding="utf-8") as f:
            json.dump(self.graph_mgr.to_dict(), f, indent=2)


    def _on_finished(self):
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)


    # ----- Export & Reset -----
    def _export_graph(self):
        path, _ = QFileDialog.getSaveFileName(self,"Save Graph",SCRIPT_DIR,"JSON (*.json)")
        if path:
            with open(path,"w",encoding="utf-8") as f:
                json.dump(self.graph_mgr.to_dict(), f, indent=2)

    def _reset_config(self):
        try: os.remove(USER_CONF)
        except: pass
        QMessageBox.information(self,"Reset","Please restart to apply changes.")
        sys.exit(0)


    # ----- Layout init -----
    def _init_layout(self):
        spl = QSplitter(Qt.Orientation.Vertical)

        # conversation pane
        conv = QWidget(); cl = QVBoxLayout(conv)
        conv.setStyleSheet("background:#E1BEE7;")
        self.chat_cont = QFrame(); self.chat_cont.setFrameShape(QFrame.Shape.NoFrame)
        self.chat_layout = QVBoxLayout(self.chat_cont)
        self.scroll      = QScrollArea()
        self.scroll.setWidgetResizable(True)
        self.scroll.setWidget(self.chat_cont)
        cl.addWidget(self.scroll)
        spl.addWidget(conv)

        # debate graph pane
        graph = QWidget(); gl = QVBoxLayout(graph)
        self.tree = QTreeWidget(); self.tree.setHeaderLabels(["ID","Author","Type","Status","Timestamp"])
        gl.addWidget(self.tree)
        spl.addWidget(graph)

        self.setCentralWidget(spl)


# --- ENTRYPOINT ---
if __name__=="__main__":
    app = QApplication(sys.argv)
    w   = DebateSimulatorApp()
    w.show()
    sys.exit(app.exec())
```

Debate_AI.py – Debate Simulator with Dockable & Splitter UI
• Dockable panels for David (Blue), Zira (Red), Session control
• Central vertical splitter: Conversation (top) & Debate Graph (bottom)
• Color‑coded panels & bubbles
• Provider/model selectors per agent
• Session metadata display (session name + epoch)
• Real‑time Node creation and vector insertion
• Rolling window of last N pairs + semantic recall
• Dynamic schema management (global & session)
• Full micro‑loop in DebateWorker with error handling
• Configurable recall and TTS controls, persisted to config/user_config.json
• “Tools” menu stubs for code analysis & dataset creation
• Automatic session start, archiving into sessions/archive/
• TTS integration and styled conversation window
• Voice‑selection dropdowns for each bot
• Ensures David (Blue) speaks first, then Zira (Red)
**Classes:** DebateWorker, DebateSimulatorApp
**Functions:** load_config(), save_config()


## Module `Debate_AI_Old\Debate_AIu.py`

```python
#!/usr/bin/env python3
# Debate_AI.py - Fully Enhanced Implementation with Adaptive Arbiter and Robust Flow
# Complete system with dynamic control, strict narrative, resolution negotiation, and rich UI

import os
import sys
import json
import time
import shutil
import threading
import subprocess
import requests
import openai
import numpy as np
import asyncio
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget,
    QVBoxLayout, QHBoxLayout, QLabel,
    QLineEdit, QTextEdit, QPushButton,
    QComboBox, QSlider, QDockWidget,
    QCheckBox, QListWidget, QMessageBox,
    QSpinBox, QStatusBar, QFileDialog,
    QProgressBar
)
from PyQt5.QtGui import QColor, QPalette, QTextCharFormat, QTextCursor
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal, QStateMachine, QState, QFinalState

# ─── Ensure local modules importable ─────────────────────────────────────────
sys.path.insert(0, os.path.dirname(__file__))

# Import TTS and Monitor
try:
    from tts import TTSManager
    from monitor_debate import DebateMonitor
except ImportError as e:
    QMessageBox.critical(None, "Error", f"Import error: {e}")
    sys.exit(1)

# ─── Create API directory and key file if needed ─────────────────────────────
os.makedirs("api", exist_ok=True)
API_KEY_PATH = os.path.join("api", "api_key.txt")
if not os.path.exists(API_KEY_PATH):
    with open(API_KEY_PATH, "w") as f:
        f.write("your-openai-api-key-here")

# ─── Load API Key & OpenAI client ────────────────────────────────────────────
try:
    with open(API_KEY_PATH, "r", encoding="utf-8") as f:
        OPENAI_API_KEY = f.read().strip()
    if OPENAI_API_KEY and OPENAI_API_KEY != "your-openai-api-key-here":
        openai.api_key = OPENAI_API_KEY
        openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    else:
        openai_client = None
except Exception as e:
    print(f"OpenAI setup error: {e}")
    openai_client = None

# ─── Dynamic Model Fetching ─────────────────────────────────────────────────
def fetch_openai_models():
    if not openai_client:
        return ["gpt-4o-mini", "gpt-3.5-turbo"]
    try:
        resp = openai_client.models.list()
        return [m.id for m in resp.data if "gpt" in m.id.lower()]
    except:
        return ["gpt-4o-mini", "gpt-3.5-turbo"]

def fetch_ollama_models():
    try:
        output = subprocess.check_output(["ollama", "list"], universal_newlines=True)
        models = [line.split()[0] for line in output.splitlines()[1:] if line.strip()]
        return models if models else ["llama2:7b"]
    except:
        return ["llama2:7b"]

# ─── Utility Functions ──────────────────────────────────────────────────────
def cosine_sim(a, b):
    a, b = np.array(a), np.array(b)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))

def safe_gui_call(func, *args, **kwargs):
    QTimer.singleShot(0, lambda: func(*args, **kwargs))

def parse_mad_lib(text, required_slots=["Stance", "reason", "opponent's point", "argument"]):
    """Parse text for Mad Lib structure and validate slots"""
    slots = {slot: None for slot in required_slots}
    for slot in required_slots:
        start = text.find(f"[{slot}]")
        if start != -1:
            end = text.find("]", start)
            if end != -1:
                slots[slot] = text[start + len(f"[{slot}]"):end].strip()
    return all(slots.values()), slots

# ─── Debate Worker Thread with State Machine ────────────────────────────────
class DebateWorker(QThread):
    message_ready = pyqtSignal(str, str, str)  # bot, type, message
    status_update = pyqtSignal(str)
    epoch_complete = pyqtSignal(int)
    error_occurred = pyqtSignal(str)
    topic_updated = pyqtSignal(str, str)  # topic, summary
    progress_update = pyqtSignal(int)  # Percentage

    def __init__(self, parent):
        super().__init__()
        self.parent = parent
        self.running = False
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        self.current_bot = "Red"
        self.talking_points = []
        self.current_point_idx = 0
        self.rebuttal_count = 0
        self.max_rebuttals = 0
        self.state_machine = None
        self.deadlock_count = 0
        self.MAX_DEADLOCKS = 2

    def start_debate(self):
        self.running = True
        self._setup_state_machine()
        self.start()

    def stop_debate(self):
        self.running = False
        self.executor.shutdown(wait=False)
        self.loop.call_soon_threadsafe(self.loop.stop)
        if self.state_machine:
            self.state_machine.stop()

    def _setup_state_machine(self):
        self.state_machine = QStateMachine()
        init_state = QState()
        rebuttal_state = QState()
        resolution_state = QState()
        final_state = QFinalState()

        init_state.addTransition(self._init_complete, rebuttal_state)
        rebuttal_state.addTransition(self._rebuttal_complete, resolution_state if self.current_point_idx >= len(self.talking_points) - 1 else rebuttal_state)
        resolution_state.addTransition(self._resolution_complete, final_state)

        init_state.entered.connect(self._enter_init)
        rebuttal_state.entered.connect(self._enter_rebuttal)
        resolution_state.entered.connect(self._enter_resolution)
        final_state.entered.connect(self._enter_final)

        self.state_machine.addState(init_state)
        self.state_machine.addState(rebuttal_state)
        self.state_machine.addState(resolution_state)
        self.state_machine.addState(final_state)
        self.state_machine.setInitialState(init_state)

    def _init_complete(self):
        return self.current_point_idx > 0

    def _rebuttal_complete(self):
        return self.rebuttal_count >= self.max_rebuttals or self._detect_deadlock()

    def _resolution_complete(self):
        return True

    def _enter_init(self):
        asyncio.run_coroutine_threadsafe(self._initialize_debate(), self.loop)

    def _enter_rebuttal(self):
        asyncio.run_coroutine_threadsafe(self._execute_rebuttal(), self.loop)

    def _enter_resolution(self):
        asyncio.run_coroutine_threadsafe(self._execute_resolution(), self.loop)

    def _enter_final(self):
        asyncio.run_coroutine_threadsafe(self._finalize_epoch(), self.loop)
        self.running = False

    def run(self):
        try:
            self.loop.run_until_complete(self._debate_flow())
        except Exception as e:
            self.error_occurred.emit(str(e))
        finally:
            self.loop.close()

    async def _debate_flow(self):
        if not await self._validate_preconditions():
            self.error_occurred.emit("Preconditions failed")
            return

        self.parent.epoch += 1
        eid = f"{self.parent.epoch:03d}"
        edir = os.path.join(self.parent.session_dir, f"epoch_{eid}")
        os.makedirs(edir, exist_ok=True)

        self.status_update.emit(f"Starting Epoch {self.parent.epoch}")
        self.progress_update.emit(0)

        if self.parent.log_path:
            open(self.parent.log_path, "w", encoding="utf-8").close()

        if self.parent.chk_enable_arbiter.isChecked():
            self.status_update.emit("Arbiter generating talking points...")
            talking_points = await self._call_arbiter_plan()
            if talking_points and talking_points.get("stages"):
                self.talking_points = talking_points["stages"]
                self.max_rebuttals = self.parent.rounds_spin.value()
                with open(os.path.join(edir, "talking_points.json"), "w", encoding="utf-8") as f:
                    json.dump({"talking_points": self.talking_points, "epoch": self.parent.epoch}, f, indent=2)
                self.message_ready.emit("Arbiter", "Plan", json.dumps(self.talking_points))
            else:
                self.error_occurred.emit("Failed to generate talking points")
                return

        self.state_machine.start()
        while self.state_machine.running():
            await asyncio.sleep(0.1)

        self.epoch_complete.emit(self.parent.epoch)
        self.status_update.emit(f"Epoch {self.parent.epoch} completed")
        self.progress_update.emit(100)

    async def _validate_preconditions(self):
        """Validate debate readiness"""
        if not self.parent.topic_edit.text().strip():
            return False
        if not self.parent.persona_red.toPlainText().strip() or not self.parent.persona_blue.toPlainText().strip():
            return False
        return True

    async def _initialize_debate(self):
        """Initialize debate with first turn"""
        if not self.talking_points:
            self.error_occurred.emit("No talking points available")
            return
        point = self.talking_points[self.current_point_idx]
        await self._execute_turn(self.current_bot, point["name"], point["questions"][0])
        self.current_bot = "Blue" if self.current_bot == "Red" else "Red"
        self.current_point_idx += 1
        self.progress_update.emit(10)

    async def _execute_rebuttal(self):
        """Execute rebuttal phase with alternation and interjection"""
        if not self.talking_points or self.current_point_idx >= len(self.talking_points):
            self.error_occurred.emit("No more stages")
            return

        point = self.talking_points[self.current_point_idx]
        stage_type = point["name"]
        self.status_update.emit(f"Stage: {stage_type}")
        self.message_ready.emit("System", "Stage", f"--- {stage_type} ---")

        await self._execute_turn(self.current_bot, stage_type, point["questions"][0])
        self.rebuttal_count += 1

        if self.rebuttal_count >= self.max_rebuttals or self._detect_deadlock():
            await self._arbiter_interjection()
            self.rebuttal_count = 0
            self.current_bot = "Red"
            self.current_point_idx += 1
        else:
            self.current_bot = "Blue" if self.current_bot == "Red" else "Red"

        self.progress_update.emit(min(50, int((self.current_point_idx / len(self.talking_points)) * 50)))

    async def _execute_resolution(self):
        """Execute resolution phase"""
        if self.current_point_idx >= len(self.talking_points):
            self.error_occurred.emit("No resolution stage")
            return

        point = self.talking_points[self.current_point_idx]
        await self._execute_resolution_turn(self.current_bot, point["name"])
        self.current_bot = "Blue" if self.current_bot == "Red" else "Red"
        self.current_point_idx += 1
        self.progress_update.emit(75)

    async def _execute_turn(self, bot, stage_type, question):
        """Execute a single debate turn with narrative enforcement"""
        try:
            history = await asyncio.get_event_loop().run_in_executor(
                self.executor, self.parent._get_history_slice
            )
            rag_context = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._get_semantic_context(bot)
            )
            persona = self.parent._get_persona(bot)

            mono_prompt = self._build_monologue_prompt(bot, stage_type, persona, history, rag_context, question)
            monologue = await self._call_model(mono_prompt)
            if monologue:
                valid, slots = parse_mad_lib(monologue)
                if not valid:
                    correction = self._correct_mad_lib(monologue, slots)
                    monologue = correction if correction else monologue
                self.message_ready.emit(bot, "Monologue", monologue)
            else:
                raise ValueError("Monologue generation failed")

            pub_prompt = self._build_public_prompt(bot, stage_type, persona, monologue, history)
            public_msg = await self._call_model(pub_prompt)
            if public_msg:
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent._log_message(bot, public_msg)
                )
                self.message_ready.emit(bot, "Public", public_msg)
            else:
                raise ValueError("Public response generation failed")

            if (bot == "Red" and self.parent.chk_enable_tts_red.isChecked()) or \
               (bot == "Blue" and self.parent.chk_enable_tts_blue.isChecked()):
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent.tts.speak(bot, public_msg)
                )
            await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._semantic_save(bot, monologue, public_msg, stage_type)
            )
            await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._check_contradiction(bot, public_msg)
            )
            await self._scorebot_analysis(bot, stage_type)
        except Exception as e:
            self.error_occurred.emit(f"Turn execution error: {str(e)}")

    async def _execute_resolution_turn(self, bot, stage_type):
        """Execute a resolution turn with negotiation"""
        try:
            history = await asyncio.get_event_loop().run_in_executor(
                self.executor, self.parent._get_history_slice
            )
            rag_context = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._get_semantic_context(bot)
            )
            persona = self.parent._get_persona(bot)

            resolution_prompt = self._build_resolution_prompt(bot, stage_type, persona, history, rag_context)
            monologue = await self._call_model(resolution_prompt)
            if monologue:
                valid, slots = parse_mad_lib(monologue, ["Resolution type", "reason", "evidence"])
                if not valid:
                    correction = self._correct_mad_lib(monologue, slots, is_resolution=True)
                    monologue = correction if correction else monologue
                self.message_ready.emit(bot, "Monologue", monologue)
            else:
                raise ValueError("Resolution monologue generation failed")

            pub_prompt = self._build_public_prompt(bot, stage_type, persona, monologue, history)
            public_msg = await self._call_model(pub_prompt)
            if public_msg:
                resolution_quality = await self._evaluate_resolution_quality(bot, public_msg)
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent._log_message(bot, f"{public_msg} (Quality: {resolution_quality:.2f})")
                )
                self.message_ready.emit(bot, "Public", public_msg)
            else:
                raise ValueError("Resolution public response generation failed")

            if (bot == "Red" and self.parent.chk_enable_tts_red.isChecked()) or \
               (bot == "Blue" and self.parent.chk_enable_tts_blue.isChecked()):
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent.tts.speak(bot, public_msg)
                )
            await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._semantic_save(bot, monologue, public_msg, stage_type, is_resolution=True)
            )
        except Exception as e:
            self.error_occurred.emit(f"Resolution turn error: {str(e)}")

    async def _arbiter_interjection(self):
        """Arbiter provides a new guiding question with adaptive adjustment"""
        if self.current_point_idx + 1 >= len(self.talking_points):
            self.error_occurred.emit("No more stages for interjection")
            return

        coherence_red = await asyncio.get_event_loop().run_in_executor(
            self.executor, lambda: self.parent._calculate_coherence("Red")
        )
        coherence_blue = await asyncio.get_event_loop().run_in_executor(
            self.executor, lambda: self.parent._calculate_coherence("Blue")
        )

        if coherence_red < 0.3 or coherence_blue < 0.3 or self.deadlock_count >= self.MAX_DEADLOCKS:
            self._adjust_talking_points(coherence_red, coherence_blue)
            self.deadlock_count = 0

        next_point = self.talking_points[self.current_point_idx]
        question = next_point["questions"][0]
        self.message_ready.emit("Arbiter", "Interjection", f"Next question: {question} (Adjusted for coherence)")
        self.current_bot = "Red"

    def _detect_deadlock(self):
        """Detect deadlock based on low coherence or repetition"""
        coherence_red = self.parent._calculate_coherence("Red")
        coherence_blue = self.parent._calculate_coherence("Blue")
        if coherence_red < 0.2 or coherence_blue < 0.2:
            self.deadlock_count += 1
            return True
        return False

    def _adjust_talking_points(self, coherence_red, coherence_blue):
        """Adjust talking points based on coherence"""
        if coherence_red < 0.3:
            self.talking_points.insert(self.current_point_idx + 1, {
                "name": f"Rebuttal Clarification {self.rebuttal_count + 1}",
                "questions": [f"Red, clarify your stance due to low coherence ({coherence_red:.2f})"],
                "outcomes": "Red clarifies position",
                "max_duration": 300
            })
        if coherence_blue < 0.3:
            self.talking_points.insert(self.current_point_idx + 1, {
                "name": f"Rebuttal Clarification {self.rebuttal_count + 1}",
                "questions": [f"Blue, clarify your stance due to low coherence ({coherence_blue:.2f})"],
                "outcomes": "Blue clarifies position",
                "max_duration": 300
            })

    def _build_monologue_prompt(self, bot, stage_type, persona, history, rag_context, question):
        """Build prompt with strict Mad Lib enforcement"""
        opponent = "Blue" if bot == "Red" else "Red"
        opponent_history = "\n".join([m for m in history.split("\n") if f"[{opponent}]" in m])
        
        return f"""[INTERNAL MONOLOGUE - {bot}]
Stage: {stage_type}
Persona: {persona}
Arbiter Question: {question}

Recent History:
{history}

Opponent History:
{opponent_history}

Relevant Context (RAG):
{rag_context}

Generate your internal reasoning in this strict Mad Lib format: [Stance] because [reason], countering [opponent's point] with [argument]."""

    def _build_public_prompt(self, bot, stage_type, persona, monologue, history):
        """Build prompt for public response"""
        return f"""[PUBLIC RESPONSE - {bot}]
Stage: {stage_type}
Persona: {persona}

Internal Analysis:
{monologue}

Recent History:
{history}

Present a clear, persuasive argument addressing the Arbiter's question and engaging with the opponent's points."""

    def _build_resolution_prompt(self, bot, stage_type, persona, history, rag_context):
        """Build prompt for resolution phase with negotiation"""
        return f"""[INTERNAL MONOLOGUE - {bot} - RESOLUTION]
Stage: {stage_type}
Persona: {persona}

Recent History:
{history}

Relevant Context (RAG):
{rag_context}

Propose a resolution in this format: [Resolution type] because [reason], supported by [evidence]. Negotiate with the opponent’s last stance to seek agreement, disagreement, or conditional consensus."""

    def _correct_mad_lib(self, text, slots, is_resolution=False):
        """Correct missing Mad Lib slots with Arbiter guidance"""
        required_slots = ["Resolution type", "reason", "evidence"] if is_resolution else ["Stance", "reason", "opponent's point", "argument"]
        missing = [s for s in required_slots if not slots.get(s)]
        if missing:
            prompt = f"""The following monologue is missing Mad Lib slots: {missing}. Correct it to fit the format.
Text: {text}
Provide a corrected version with all slots filled."""
            return self.parent._call_model(prompt)
        return None

    def _get_persona(self, bot):
        """Get persona text for bot (delegate to parent)"""
        return self.parent._get_persona(bot)

    async def _call_arbiter_plan(self):
        """Generate Arbiter debate plan with context-aware adjustments"""
        topic = self.parent.topic_edit.text().strip()
        epoch = self.parent.epoch
        prior_summary = ""
        if self.parent.epoch > 1 and os.path.exists(os.path.join(self.parent.session_dir, f"epoch_{epoch-1:03d}", "summary_{epoch-1:03d}.txt")):
            with open(os.path.join(self.parent.session_dir, f"epoch_{epoch-1:03d}", "summary_{epoch-1:03d}.txt"), "r", encoding="utf-8") as f:
                prior_summary = f.read()

        prompt = f"""As the Arbiter AI, create a structured debate plan for Epoch {epoch}.

Topic: {topic}
Prior Summary (if any): {prior_summary}

Generate a debate plan with 3-5 key stages, each with:
1. Name (e.g., Opening, Rebuttal 1, Closing)
2. A list of 1-2 focus questions to guide the discussion
3. Expected outcomes for Red and Blue bots
4. Maximum duration in seconds (optional)

Incorporate prior conclusions or adjust questions based on the prior summary. Return the plan in JSON format."""
        
        plan_text = self.parent._call_model(prompt)
        if not plan_text or not plan_text.strip():
            self.error_occurred.emit("Arbiter plan generation failed")
            return {"stages": [
                {"name": "Opening", "questions": ["What is your stance?"], "outcomes": "Establish positions", "max_duration": 300},
                {"name": "Rebuttal 1", "questions": ["How do you counter the opponent?"], "outcomes": "Challenge arguments", "max_duration": 300},
                {"name": "Closing", "questions": ["What is your final resolution?"], "outcomes": "Conclude debate", "max_duration": 300}
            ]}
        try:
            return json.loads(plan_text)
        except json.JSONDecodeError:
            self.error_occurred.emit("Invalid JSON from Arbiter plan")
            return {"stages": [
                {"name": "Opening", "questions": ["What is your stance?"], "outcomes": "Establish positions", "max_duration": 300},
                {"name": "Rebuttal 1", "questions": ["How do you counter the opponent?"], "outcomes": "Challenge arguments", "max_duration": 300},
                {"name": "Closing", "questions": ["What is your final resolution?"], "outcomes": "Conclude debate", "max_duration": 300}
            ]}

    async def _call_model(self, prompt):
        """Call the selected AI model with error recovery"""
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._call_model(prompt)
            )
            return result if result and result.strip() else self._generate_fallback_response(prompt)
        except Exception as e:
            self.error_occurred.emit(f"Model call error: {str(e)}")
            return self._generate_fallback_response(prompt)

    def _generate_fallback_response(self, prompt):
        """Generate a fallback response if model fails"""
        return f"[Fallback] Unable to process: {prompt[:50]}... Please check API or model settings."

    async def _scorebot_analysis(self, bot, stage_type):
        """Perform adaptive ScoreBot analysis"""
        try:
            coherence = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._calculate_coherence(bot)
            )
            contradictions = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._count_contradictions(bot)
            )
            relevance = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._calculate_relevance(bot)
            )
            weight = 1.0 if "Closing" not in stage_type else 2.0  # Higher weight for resolution
            score = (coherence * 0.4 + (1 - contradictions / 5) * 0.3 + relevance * 0.3) * weight

            score_entry = {
                "bot": bot,
                "stage": stage_type,
                "coherence": coherence,
                "contradictions": contradictions,
                "relevance": relevance,
                "score": score,
                "timestamp": time.time()
            }
            self.parent.score_updates.append(score_entry)

            color = "#FF4444" if bot == "Red" else "#4444FF"
            score_text = (
                f"<span style='color:{color}'>{stage_type}/{bot}</span> → "
                f"Score: {score:.2f}, Coherence: {coherence:.2f}, Contradictions: {contradictions}, "
                f"Relevance: {relevance:.2f}"
            )
            safe_gui_call(self.parent._update_scorebot_display, score_text)
        except Exception as e:
            self.error_occurred.emit(f"ScoreBot error: {str(e)}")

    async def _evaluate_resolution_quality(self, bot, text):
        """Evaluate the quality of a resolution"""
        prompt = f"""Evaluate the quality of this resolution statement on a scale of 0 to 1:
{text}
Consider clarity, evidence strength, and negotiation effort. Return a single float value."""
        quality = await self._call_model(prompt)
        try:
            return float(quality.strip())
        except ValueError:
            return 0.5  # Default quality if parsing fails

    async def _update_topic_index(self, bot, stage_type):
        """Update topic index with subtopics"""
        await self.parent._update_topic_index(bot, stage_type)

    async def _finalize_epoch(self, epoch_id, epoch_dir, stages):
        """Finalize epoch with detailed logging"""
        try:
            if self.parent.log_path and os.path.exists(self.parent.log_path):
                shutil.copy(self.parent.log_path, os.path.join(epoch_dir, f"transcript_{epoch_id}.txt"))

            if self.parent.sem_model and self.parent.log_path:
                try:
                    with open(self.parent.log_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content:
                        embedding = self.parent.sem_model.encode(content).tolist()
                        with open(os.path.join(epoch_dir, f"vectors_{epoch_id}.json"), "w", encoding="utf-8") as f:
                            json.dump(embedding, f)
                except Exception as e:
                    self._update_status(f"Vector save error: {str(e)}")

            if self.parent.score_updates:
                with open(os.path.join(epoch_dir, f"scores_{epoch_id}.json"), "w", encoding="utf-8") as f:
                    json.dump(self.parent.score_updates, f, indent=2)

            if self.parent.chk_enable_arbiter.isChecked():
                summary = await self._generate_arbiter_summary(epoch_id, stages)
                with open(os.path.join(epoch_dir, f"summary_{epoch_id}.txt"), "w", encoding="utf-8") as f:
                    f.write(summary)
                self.message_ready.emit("Arbiter", "Summary", summary)

            # Log detailed diagnostics
            with open(os.path.join(epoch_dir, "diagnostics.log"), "w", encoding="utf-8") as f:
                f.write(f"Epoch {epoch_id} completed at {datetime.now()}\n")
                f.write(f"Talking Points: {json.dumps(self.talking_points, indent=2)}\n")
                f.write(f"Final Scores: {json.dumps(self.parent.score_updates[-2:], indent=2)}\n")
        except Exception as e:
            self._update_status(f"Epoch finalization error: {str(e)}")

    async def _generate_arbiter_summary(self, epoch_id, stages):
        """Generate Arbiter summary with epoch context"""
        try:
            transcript = ""
            if self.parent.log_path and os.path.exists(self.parent.log_path):
                with open(self.parent.log_path, "r", encoding="utf-8") as f:
                    transcript = f.read()

            prompt = f"""As the Arbiter AI, provide a comprehensive summary of Epoch {epoch_id}.

Debate Transcript (last 2000 characters):
{transcript[-2000:]}

Analyze:
1. Key arguments from Red and Blue
2. Strongest points made by each
3. Areas of agreement and disagreement
4. Logical consistency of each bot
5. Recommendations for the next epoch, incorporating prior vectors or conclusions

Provide a structured summary in markdown format."""
            
            return self.parent._call_model(prompt)
        except Exception as e:
            return f"Summary generation error: {str(e)}"

# =============================================================================
# Main Application
# =============================================================================
class DebateAI(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate AI - Enhanced System")
        self.resize(1600, 1000)
        
        # ── State variables
        self.session = None
        self.session_dir = None
        self.epoch = 0
        self.log_path = None
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        self.running = False
        
        # ── Initialize semantic model
        self.sem_model = None
        try:
            from sentence_transformers import SentenceTransformer
            self.sem_model = SentenceTransformer("all-MiniLM-L6-v2")
        except ImportError:
            print("SentenceTransformer not available - semantic features disabled")
        
        # ── Initialize tools
        try:
            self.tts = TTSManager()
        except Exception as e:
            print(f"TTS initialization error: {e}")
            self.tts = None
            QMessageBox.warning(self, "TTS Error", "TTS failed to initialize. TTS features disabled.")
        
        self.monitor = DebateMonitor(log_path="", warning_callback=self._on_monitor_warning)
        
        # ── Initialize worker thread
        self.worker = DebateWorker(self)
        self.worker.message_ready.connect(self._handle_message)
        self.worker.status_update.connect(self._update_status)
        self.worker.epoch_complete.connect(self._on_epoch_complete)
        self.worker.error_occurred.connect(self._handle_error)
        self.worker.topic_updated.connect(self._update_topic_index)
        self.worker.progress_update.connect(self._update_progress)
        
        # ── Load debate formats
        self.formats = self._load_debate_formats()
        
        # ── Text formats
        self.red_format = QTextCharFormat()
        self.red_format.setForeground(QColor("#FF4444"))
        self.blue_format = QTextCharFormat()
        self.blue_format.setForeground(QColor("#4444FF"))
        self.arbiter_format = QTextCharFormat()
        self.arbiter_format.setForeground(QColor("#800080"))
        self.system_format = QTextCharFormat()
        self.system_format.setForeground(QColor("#666666"))
        self.resolution_format = QTextCharFormat()
        self.resolution_format.setForeground(QColor("#006400"))
        
        # ── Build UI
        self._build_ui()
        
        # ── Apply styling
        self._apply_theme()
    
    def _load_debate_formats(self):
        """Load debate format templates"""
        formats = []
        formats_dir = "formats"
        os.makedirs(formats_dir, exist_ok=True)
        
        self_diag_file = os.path.join(formats_dir, "self_diagnostic_debate.json")
        if not os.path.exists(self_diag_file):
            default_format = {
                "name": "Self-Diagnostic Debate",
                "description": "Debate about improving the Debate_AI system itself",
                "stages": [
                    {"bot": "Red", "type": "Architectural Analysis", "max_duration": 300},
                    {"bot": "Blue", "type": "Feature Evaluation", "max_duration": 300},
                    {"bot": "Red", "type": "Failure Mode Review", "max_duration": 300},
                    {"bot": "Blue", "type": "Innovation Proposals", "max_duration": 300},
                    {"bot": "Red", "type": "Implementation Strategy", "max_duration": 300},
                    {"bot": "Blue", "type": "Summary & Recommendations", "max_duration": 300}
                ]
            }
            with open(self_diag_file, "w", encoding="utf-8") as f:
                json.dump(default_format, f, indent=2)
        
        for filename in os.listdir(formats_dir):
            if filename.endswith(".json"):
                try:
                    with open(os.path.join(formats_dir, filename), "r", encoding="utf-8") as f:
                        format_data = json.load(f)
                        formats.append(format_data)
                except Exception as e:
                    print(f"Error loading format {filename}: {e}")
        
        return formats
    
    def _build_ui(self):
        """Build the complete user interface with resolution indicator"""
        self.chat = QTextEdit()
        self.chat.setReadOnly(True)
        self.chat.setStyleSheet("background-color: #E6C8E6; color: #4A0E4E; font-size: 14px;")
        self.setCentralWidget(self.chat)
        
        self._create_monologue_docks()
        self._create_persona_docks()
        self._create_control_docks()
        self._create_analysis_docks()
        self._create_session_docks()
        
        self.status_bar = QStatusBar()
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        self.progress_bar.setMaximumWidth(200)
        self.resolution_label = QLabel("Resolution Status: Pending")
        self.resolution_label.setStyleSheet("color: #006400;")
        self.status_bar.addWidget(self.resolution_label)
        self.status_bar.addPermanentWidget(self.progress_bar)
        self.setStatusBar(self.status_bar)
        
        self._connect_signals()
        self._initialize_defaults()
    
    def _create_monologue_docks(self):
        """Create monologue display docks"""
        dock_red_mono = QDockWidget("Red Monologue", self)
        self.red_mono = QTextEdit()
        self.red_mono.setReadOnly(True)
        self.red_mono.setStyleSheet("background-color: #FFE6E6; color: #8B0000; font-size: 14px;")
        dock_red_mono.setWidget(self.red_mono)
        self.addDockWidget(Qt.LeftDockWidgetArea, dock_red_mono)
        
        dock_blue_mono = QDockWidget("Blue Monologue", self)
        self.blue_mono = QTextEdit()
        self.blue_mono.setReadOnly(True)
        self.blue_mono.setStyleSheet("background-color: #E6F3FF; color: #000080; font-size: 14px;")
        dock_blue_mono.setWidget(self.blue_mono)
        self.addDockWidget(Qt.RightDockWidgetArea, dock_blue_mono)
    
    def _create_persona_docks(self):
        """Create persona editing docks"""
        dock_red_persona = QDockWidget("Red Persona", self)
        widget_red = QWidget()
        layout_red = QVBoxLayout()
        self.persona_red = QTextEdit()
        self.persona_red.setMaximumHeight(100)
        self.persona_red.setStyleSheet("background-color: #FFF0F0; color: #8B0000; font-size: 14px;")
        layout_red.addWidget(QLabel("Red Bot Persona:"))
        layout_red.addWidget(self.persona_red)
        widget_red.setLayout(layout_red)
        dock_red_persona.setWidget(widget_red)
        self.addDockWidget(Qt.LeftDockWidgetArea, dock_red_persona)
        
        dock_blue_persona = QDockWidget("Blue Persona", self)
        widget_blue = QWidget()
        layout_blue = QVBoxLayout()
        self.persona_blue = QTextEdit()
        self.persona_blue.setMaximumHeight(100)
        self.persona_blue.setStyleSheet("background-color: #E6FFFF; color: #000080; font-size: 14px;")
        layout_blue.addWidget(QLabel("Blue Bot Persona:"))
        layout_blue.addWidget(self.persona_blue)
        widget_blue.setLayout(layout_blue)
        dock_blue_persona.setWidget(widget_blue)
        self.addDockWidget(Qt.RightDockWidgetArea, dock_blue_persona)
    
    def _create_control_docks(self):
        """Create control and TTS docks"""
        dock_struct = QDockWidget("Debate Structure", self)
        widget_struct = QWidget()
        layout_struct = QVBoxLayout()
        self.topic_edit = QLineEdit()
        self.topic_edit.setPlaceholderText("Enter debate topic or resolution...")
        layout_struct.addWidget(QLabel("Topic / Resolution"))
        layout_struct.addWidget(self.topic_edit)
        self.format_combo = QComboBox()
        self.format_combo.addItem("Classic Debate", None)
        for fmt in self.formats:
            self.format_combo.addItem(fmt["name"], fmt)
        layout_struct.addWidget(QLabel("Debate Format"))
        layout_struct.addWidget(self.format_combo)
        self.rounds_spin = QSpinBox()
        self.rounds_spin.setRange(0, 10)
        self.rounds_spin.setValue(2)
        layout_struct.addWidget(QLabel("Rebuttal Rounds"))
        layout_struct.addWidget(self.rounds_spin)
        self.chk_enable_arbiter = QCheckBox("Enable Arbiter AI")
        self.chk_enable_scorebot = QCheckBox("Enable ScoreBot")
        self.chk_enable_selfdiag = QCheckBox("Enable Self-Diagnostic Mode")
        layout_struct.addWidget(self.chk_enable_arbiter)
        layout_struct.addWidget(self.chk_enable_scorebot)
        layout_struct.addWidget(self.chk_enable_selfdiag)
        widget_struct.setLayout(layout_struct)
        dock_struct.setWidget(widget_struct)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_struct)
        
        dock_models = QDockWidget("Providers & Models", self)
        widget_models = QWidget()
        layout_models = QVBoxLayout()
        self.provider_combo = QComboBox()
        self.provider_combo.addItems(["OpenAI", "Ollama"])
        layout_models.addWidget(QLabel("Provider:"))
        layout_models.addWidget(self.provider_combo)
        self.model_combo = QComboBox()
        layout_models.addWidget(QLabel("Model:"))
        layout_models.addWidget(self.model_combo)
        btn_refresh = QPushButton("Refresh Models")
        layout_models.addWidget(btn_refresh)
        widget_models.setLayout(layout_models)
        dock_models.setWidget(widget_models)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_models)
        
        dock_settings = QDockWidget("Model Settings", self)
        widget_settings = QWidget()
        layout_settings = QVBoxLayout()
        self.temp_slider = QSlider(Qt.Horizontal)
        self.temp_slider.setRange(0, 100)
        self.temp_slider.setValue(70)
        self.temp_label = QLabel("Temperature: 0.70")
        layout_settings.addWidget(self.temp_label)
        layout_settings.addWidget(self.temp_slider)
        self.token_spin = QSpinBox()
        self.token_spin.setRange(64, 8192)
        self.token_spin.setValue(512)
        layout_settings.addWidget(QLabel("Max Tokens:"))
        layout_settings.addWidget(self.token_spin)
        widget_settings.setLayout(layout_settings)
        dock_settings.setWidget(widget_settings)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_settings)
        
        dock_controls = QDockWidget("Controls & TTS", self)
        widget_controls = QWidget()
        layout_controls = QVBoxLayout()
        layout_buttons = QHBoxLayout()
        self.btn_start = QPushButton("Start Debate")
        self.btn_pause = QPushButton("Pause Debate")
        self.btn_reset = QPushButton("Reset Session")
        self.btn_start.setStyleSheet("background-color: #90EE90; color: #000000;")
        self.btn_pause.setStyleSheet("background-color: #FFD700; color: #000000;")
        self.btn_reset.setStyleSheet("background-color: #FF4040; color: #FFFFFF;")
        layout_buttons.addWidget(self.btn_start)
        layout_buttons.addWidget(self.btn_pause)
        layout_buttons.addWidget(self.btn_reset)
        layout_tts = QVBoxLayout()
        tts_row1 = QHBoxLayout()
        self.chk_enable_tts_red = QCheckBox("Enable TTS Red")
        self.chk_enable_tts_blue = QCheckBox("Enable TTS Blue")
        self.voice_red = QComboBox()
        self.voice_blue = QComboBox()
        if self.tts:
            for vid, name in self.tts.list_voices():
                self.voice_red.addItem(name, vid)
                self.voice_blue.addItem(name, vid)
        self.btn_test_red = QPushButton("▶ Test Red Voice")
        self.btn_test_blue = QPushButton("▶ Test Blue Voice")
        tts_row1.addWidget(self.chk_enable_tts_red)
        tts_row1.addWidget(self.voice_red)
        tts_row1.addWidget(self.btn_test_red)
        tts_row1.addWidget(self.chk_enable_tts_blue)
        tts_row1.addWidget(self.voice_blue)
        tts_row1.addWidget(self.btn_test_blue)
        tts_row2 = QHBoxLayout()
        self.rate_slider = QSlider(Qt.Horizontal)
        self.rate_slider.setRange(100, 300)
        self.rate_slider.setValue(150)
        self.rate_label = QLabel("TTS Rate: 150")
        tts_row2.addWidget(self.rate_label)
        tts_row2.addWidget(self.rate_slider)
        self.volume_slider = QSlider(Qt.Horizontal)
        self.volume_slider.setRange(0, 100)
        self.volume_slider.setValue(80)
        self.volume_label = QLabel("TTS Volume: 0.80")
        tts_row2.addWidget(self.volume_label)
        tts_row2.addWidget(self.volume_slider)
        layout_tts.addLayout(tts_row1)
        layout_tts.addLayout(tts_row2)
        layout_controls.addLayout(layout_buttons)
        layout_controls.addLayout(layout_tts)
        widget_controls.setLayout(layout_controls)
        dock_controls.setWidget(widget_controls)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_controls)
        
        self.tabifyDockWidget(dock_models, dock_settings)
        self.tabifyDockWidget(dock_models, dock_controls)
        dock_models.raise_()
    
    def _create_analysis_docks(self):
        """Create analysis-related docks"""
        dock_topics = QDockWidget("Topics & Subtopics", self)
        self.idx_list = QListWidget()
        self.idx_list.setStyleSheet("background-color: #F0F0F0; color: #000000;")
        dock_topics.setWidget(self.idx_list)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_topics)
        
        dock_score = QDockWidget("ScoreBot Analysis", self)
        self.score_list = QListWidget()
        self.score_list.setStyleSheet("background-color: #F0F0F0; color: #000000;")
        dock_score.setWidget(self.score_list)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_score)
        
        dock_summary = QDockWidget("Arbiter Summary", self)
        self.summary_panel = QTextEdit()
        self.summary_panel.setReadOnly(True)
        self.summary_panel.setStyleSheet("background-color: #F0E6FF; color: #4A0E4E; font-size: 14px;")
        dock_summary.setWidget(self.summary_panel)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_summary)
    
    def _create_session_docks(self):
        """Create session management docks"""
        dock_session = QDockWidget("Session Workflows", self)
        widget_session = QWidget()
        layout_session = QHBoxLayout()
        self.btn_new = QPushButton("New Session")
        self.btn_save = QPushButton("Save Session")
        self.btn_load = QPushButton("Load Session")
        self.btn_export = QPushButton("Export Transcript")
        for btn in (self.btn_new, self.btn_save, self.btn_load, self.btn_export):
            btn.setStyleSheet("background-color: #D3D3D3; color: #000000;")
            layout_session.addWidget(btn)
        widget_session.setLayout(layout_session)
        dock_session.setWidget(widget_session)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_session)
    
    def _apply_theme(self):
        """Apply consistent theme colors"""
        palette = self.palette()
        palette.setColor(QPalette.Window, QColor("#F0F0F0"))
        self.setPalette(palette)
    
    def _connect_signals(self):
        """Connect all UI signals"""
        self.format_combo.currentIndexChanged.connect(self._on_format_change)
        self.chk_enable_selfdiag.stateChanged.connect(self._apply_selfdiag_defaults)
        self.provider_combo.currentTextChanged.connect(self._refresh_model_list)
        self.temp_slider.valueChanged.connect(
            lambda v: self.temp_label.setText(f"Temperature: {v/100:.2f}")
        )
        self.btn_new.clicked.connect(self.new_session)
        self.btn_save.clicked.connect(self.save_session)
        self.btn_load.clicked.connect(self.load_session)
        self.btn_export.clicked.connect(self.export_transcript)
        self.btn_start.clicked.connect(self.start_debate)
        self.btn_pause.clicked.connect(self.pause_debate)
        self.btn_reset.clicked.connect(self.reset_debate)
        self.btn_test_red.clicked.connect(self._test_red_voice)
        self.btn_test_blue.clicked.connect(self._test_blue_voice)
        self.voice_red.currentIndexChanged.connect(
            lambda: self.tts and self.tts.set_bot_voice("Red", self.voice_red.currentData())
        )
        self.voice_blue.currentIndexChanged.connect(
            lambda: self.tts and self.tts.set_bot_voice("Blue", self.voice_blue.currentData())
        )
        self.rate_slider.valueChanged.connect(
            lambda v: self._update_tts_rate(v)
        )
        self.volume_slider.valueChanged.connect(
            lambda v: self._update_tts_volume(v)
        )
    
    def _update_tts_rate(self, value):
        """Update TTS rate and label"""
        self.rate_label.setText(f"TTS Rate: {value}")
        if self.tts:
            try:
                self.tts.set_rate(value)
            except Exception as e:
                self._update_status(f"TTS rate update error: {e}")
    
    def _update_tts_volume(self, value):
        """Update TTS volume and label"""
        self.volume_label.setText(f"TTS Volume: {value/100:.2f}")
        if self.tts:
            try:
                self.tts.set_volume(value / 100.0)
            except Exception as e:
                self._update_status(f"TTS volume update error: {e}")
    
    def _test_red_voice(self):
        """Test Red bot's voice"""
        if self.tts:
            try:
                self.tts.speak("Red", "This is a test of the Red bot's voice.")
            except Exception as e:
                self._update_status(f"Red TTS test error: {e}")
        else:
            self._update_status("TTS not available")
    
    def _test_blue_voice(self):
        """Test Blue bot's voice"""
        if self.tts:
            try:
                self.tts.speak("Blue", "This is a test of the Blue bot's voice.")
            except Exception as e:
                self._update_status(f"Blue TTS test error: {e}")
        else:
            self._update_status("TTS not available")
    
    def _initialize_defaults(self):
        """Initialize UI with default values"""
        self._refresh_model_list()
        self._on_format_change(self.format_combo.currentIndex())
        if self.tts:
            self._update_tts_rate(150)
            self._update_tts_volume(80)
    
    def _on_format_change(self, idx):
        """Handle format selection change"""
        if self.format_combo.currentText() == "Classic Debate":
            self.topic_edit.setText(
                "The role of political ideology in shaping scientific consensus."
            )
            self.persona_red.setPlainText(
                "You are RED, a conservative physicist component of Debate_AI. "
                "You prioritize rigorous empirical validation and proven frameworks."
            )
            self.persona_blue.setPlainText(
                "You are BLUE, a progressive physicist component of Debate_AI. "
                "You champion innovative hypotheses and adaptive experimentation."
            )
    
    def _apply_selfdiag_defaults(self, state):
        """Apply self-diagnostic mode defaults"""
        if state == Qt.Checked:
            self.topic_edit.setText(
                "Self-Diagnostic Analysis: Improve Debate_AI's architecture, UX, and AI-driven debate flow."
            )
            self.persona_red.setPlainText(
                "You are RED, an engineer subsystem of Debate_AI tasked with diagnosing system architecture and logic integrity."
            )
            self.persona_blue.setPlainText(
                "You are BLUE, an innovation subsystem of Debate_AI focused on feature expansion and UX enhancement."
            )
            idx = self.format_combo.findText("Self-Diagnostic Debate")
            if idx != -1:
                self.format_combo.setCurrentIndex(idx)
        else:
            self.persona_red.clear()
            self.persona_blue.clear()
    
    def _refresh_model_list(self):
        """Refresh available models"""
        prov = self.provider_combo.currentText()
        self.model_combo.clear()
        self._update_status(f"Refreshing {prov} models...")
        if prov == "OpenAI":
            for m in fetch_openai_models():
                self.model_combo.addItem(m, ("openai", m))
        else:
            for m in fetch_ollama_models():
                self.model_combo.addItem(m, ("ollama", m))
        self._update_status(f"{prov} models updated")
    
    def new_session(self):
        """Create a new session with vector reset"""
        if not self.sem_model:
            self._update_status("Semantic model unavailable, session creation limited.")
            return
        ts = int(time.time())
        self.session = f"sess_{ts}"
        self.session_dir = os.path.join("sessions", self.session)
        os.makedirs(self.session_dir, exist_ok=True)
        os.makedirs(os.path.join(self.session_dir, "backups"), exist_ok=True)
        self.log_path = os.path.join(self.session_dir, "log.txt")
        self.epoch = 0
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        self._update_status(f"New session created: {self.session}")

    def save_session(self):
        """Save current session"""
        if not self.session:
            QMessageBox.warning(self, "No Session", "Create or load a session first.")
            return
        with open(os.path.join(self.session_dir, "index.json"), "w", encoding="utf-8") as f:
            json.dump({
                "semantic_store": self.semantic_store,
                "topic_index": self.topic_index,
                "score_updates": self.score_updates
            }, f, indent=2)
        self._update_status("Session saved")
    
    def load_session(self):
        """Load existing session"""
        path = QFileDialog.getExistingDirectory(self, "Select Session Folder", "sessions")
        if not path:
            return
        self.session = os.path.basename(path)
        self.session_dir = path
        self.log_path = os.path.join(path, "log.txt")
        try:
            with open(os.path.join(path, "index.json"), "r", encoding="utf-8") as f:
                data = json.load(f)
            self.semantic_store = data.get("semantic_store", {"Red": [], "Blue": []})
            self.topic_index = data.get("topic_index", [])
            self.score_updates = data.get("score_updates", [])
        except:
            self.semantic_store = {"Red": [], "Blue": []}
            self.topic_index = []
            self.score_updates = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        for topic in self.topic_index:
            self.idx_list.addItem(f"{topic['bot']}: {topic['text']}")
        for score in self.score_updates:
            color = "#FF4444" if score['bot'] == "Red" else "#4444FF"
            self.score_list.addItem(
                f"<span style='color:{color}'>{score['stage']}/{score['bot']}</span> → "
                f"Score: {score.get('score', 0):.2f}, Coherence: {score['coherence']:.2f}, Contradictions: {score['contradictions']}, "
                f"Relevance: {score['relevance']:.2f}"
            )
        self._update_status(f"Loaded session: {self.session}")
    
    def export_transcript(self):
        """Export current transcript"""
        if not self.log_path or not os.path.exists(self.log_path):
            QMessageBox.warning(self, "No Transcript", "No transcript to export.")
            return
        dest, _ = QFileDialog.getSaveFileName(self, "Export Transcript", "debate.txt", "Text Files (*.txt)")
        if dest:
            shutil.copy(self.log_path, dest)
            self._update_status("Transcript exported")
    
    def start_debate(self):
        """Start the debate with recovery option"""
        if not self.session:
            QMessageBox.warning(self, "Error", "Create or load a session first.")
            return
        if self.running:
            return
        
        self.running = True
        self.monitor.log_path = self.log_path
        self.monitor.start()
        
        if self.chk_enable_selfdiag.isChecked():
            try:
                self._run_project_analysis()
            except Exception as e:
                self._handle_error(f"Project analysis error: {str(e)}", recover=True)
        
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.worker.start_debate()
    
    def pause_debate(self):
        """Pause the debate"""
        self.running = False
        self.monitor.stop()
        self.worker.stop_debate()
        self.progress_bar.setVisible(False)
        if self.tts:
            self.tts.stop()
        self._update_status("Debate paused")
    
    def reset_debate(self):
        """Reset the debate"""
        self.pause_debate()
        if self.session_dir and os.path.exists(self.session_dir):
            shutil.rmtree(self.session_dir)
        self.session = None
        self.epoch = 0
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        self._update_status("Session reset")
    
    def _run_project_analysis(self):
        """Analyze project codebase for self-diagnostic mode"""
        if not self.sem_model:
            raise ValueError("Semantic model not available for analysis")
        root = os.path.dirname(__file__)
        for subdir, _, files in os.walk(root):
            if subdir.endswith("sessions") or subdir.endswith("formats"):
                continue
            for fname in files:
                if fname.endswith(".py"):
                    full_path = os.path.join(subdir, fname)
                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        summary = self._call_model(
                            f"Summarize the structure and purpose of {fname} in two sentences.\n\n{content}"
                        )
                        for bot in ("Red", "Blue"):
                            self._semantic_save(bot, summary, "", "Diagnostic", is_diagnostic=True)
                        ts = datetime.now().strftime("%H:%M:%S")
                        safe_gui_call(self.red_mono.append, f"{ts} [Analysis] {fname}: {summary}")
                        safe_gui_call(self.blue_mono.append, f"{ts} [Analysis] {fname}: {summary}")
                    except Exception as e:
                        self._update_status(f"Error analyzing {fname}: {str(e)}")
    
    def _handle_message(self, bot, msg_type, message):
        """Handle incoming messages with resolution indicator"""
        ts = datetime.now().strftime("%H:%M:%S")
        if msg_type == "Monologue":
            target = self.red_mono if bot == "Red" else self.blue_mono
            safe_gui_call(target.append, f"{ts} [Monologue] {message}")
        elif msg_type == "Public":
            fmt = self.resolution_format if "Closing" in self.worker.talking_points[self.worker.current_point_idx - 1]["name"] else \
                  self.red_format if bot == "Red" else self.blue_format
            safe_gui_call(self._append_chat, f"{ts} [{bot}] {message}", fmt)
            safe_gui_call(self.idx_list.addItem, f"{ts} {bot}: {message[:100]}...")
            if "Closing" in self.worker.talking_points[self.worker.current_point_idx - 1]["name"]:
                safe_gui_call(self.resolution_label.setText, "Resolution Status: In Progress")
        elif msg_type in ["Plan", "Summary", "Interjection"]:
            safe_gui_call(self.summary_panel.append, f"{ts} [{bot} {msg_type}] {message}")
        elif msg_type == "Stage":
            safe_gui_call(self._append_chat, f"{ts} {message}", self.system_format)
    
    def _append_chat(self, text, fmt):
        """Append text to chat with format"""
        cursor = self.chat.textCursor()
        cursor.movePosition(QTextCursor.End)
        cursor.insertText(text + "\n", fmt)
        self.chat.setTextCursor(cursor)
        self.chat.ensureCursorVisible()
    
    def _update_status(self, message):
        """Update status bar"""
        self.status_bar.showMessage(message, 5000)
    
    def _update_scorebot_display(self, score_text):
        """Update ScoreBot display"""
        self.score_list.addItem(score_text)
        self.score_list.scrollToBottom()
    
    def _update_topic_index(self, topic, summary):
        """Update topic index display"""
        self.idx_list.addItem(f"{topic}: {summary}")
        self.idx_list.scrollToBottom()
    
    def _update_progress(self, value):
        """Update progress bar"""
        self.progress_bar.setValue(value)
        if value == 100:
            self.resolution_label.setText("Resolution Status: Complete")

    def _on_monitor_warning(self, msg):
        """Handle monitor warnings"""
        safe_gui_call(self._append_chat, f"[Monitor] {msg}", self.system_format)
        self._update_status(msg)
    
    def _on_epoch_complete(self, epoch):
        """Handle epoch completion"""
        self.running = False
        self.monitor.stop()
        self.progress_bar.setValue(100)
        QTimer.singleShot(2000, lambda: self.progress_bar.setVisible(False))
        safe_gui_call(self._append_chat, f"✅ Epoch {epoch} complete", self.system_format)
        self.resolution_label.setText("Resolution Status: Complete")
    
    def _handle_error(self, error, recover=False):
        """Handle errors with recovery option"""
        error_msg = f"Error occurred at {datetime.now().strftime('%H:%M:%S')}:\n{error}"
        msg_box = QMessageBox(self)
        msg_box.setWindowTitle("Error")
        msg_box.setText("An error has occurred. Choose an action.")
        msg_box.setDetailedText(error_msg)
        copy_button = msg_box.addButton("Copy", QMessageBox.ActionRole)
        if recover:
            continue_button = msg_box.addButton("Continue", QMessageBox.AcceptRole)
        close_button = msg_box.addButton("Close", QMessageBox.RejectRole)
        msg_box.exec_()
        if msg_box.clickedButton() == copy_button:
            QApplication.clipboard().setText(error_msg)
        elif recover and msg_box.clickedButton() == continue_button:
            self._update_status("Attempting to recover...")
        elif msg_box.clickedButton() == close_button:
            self.close()
    
    def _get_history_slice(self):
        """Get recent conversation history"""
        if not self.log_path or not os.path.exists(self.log_path):
            return "No previous history."
        
        try:
            with open(self.log_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
            depth = self.rounds_spin.value() * 2 + 2
            return "".join(lines[-depth:]).strip()
        except:
            return "No previous history."

    def _get_semantic_context(self, bot, top_k=3):
        """Get semantic context using RAG with vector cache"""
        if not self.sem_model or not self.semantic_store.get(bot):
            return "No semantic context available."
        
        try:
            hist_emb = self.sem_model.encode(self._get_history_slice())
            sims = sorted(
                [(cosine_sim(hist_emb, r["emb"]), r["text"]) for r in self.semantic_store[bot] if r.get("emb")],
                key=lambda x: x[0],
                reverse=True
            )
            context = "\n".join(f"{s:.2f}: {t[:200]}..." for s, t in sims[:top_k])
            if self.chk_enable_selfdiag.isChecked():
                diag_sims = sorted(
                    [(cosine_sim(hist_emb, r["emb"]), r["text"]) for r in self.semantic_store[bot] if r.get("is_diagnostic", False) and r.get("emb")],
                    key=lambda x: x[0],
                    reverse=True
                )[:1]
                if diag_sims:
                    context += f"\n[Diagnostic] {diag_sims[0][1][:200]}..."
            return context
        except Exception as e:
            return f"No semantic context available (error: {str(e)})"
    
    def _semantic_save(self, bot, monologue, public_msg, stage_type, is_diagnostic=False, is_resolution=False):
        """Save text to semantic store with type differentiation"""
        if not self.sem_model:
            return
        
        try:
            if is_resolution:
                content_type = "resolution"
                prompt = f"Convert the following resolution into a Q/A format:\n\n{public_msg}"
                qa_content = self._call_model(prompt)
            elif "Closing" in stage_type:
                content_type = "conclusion"
                prompt = f"Convert the following conclusion into a Q/A format:\n\n{public_msg}"
                qa_content = self._call_model(prompt)
            else:
                content_type = "speculation" if "Rebuttal" in stage_type or "Opening" in stage_type else "raw"
                qa_content = public_msg if content_type == "raw" else monologue
            
            embedding = self.sem_model.encode(qa_content).tolist()
            record = {
                "time": time.time(),
                "text": qa_content,
                "raw_text": public_msg if content_type == "raw" else None,
                "type": content_type,
                "is_diagnostic": is_diagnostic,
                "is_resolution": is_resolution,
                "emb": embedding,
                "tokens": len(qa_content.split()),
                "stage": stage_type
            }
            
            self.semantic_store[bot].append(record)
            
            if self.session_dir:
                sem_file = os.path.join(self.session_dir, f"{bot}_sem.jsonl")
                try:
                    with open(sem_file, "a", encoding="utf-8") as f:
                        f.write(json.dumps(record) + "\n")
                except Exception as e:
                    self._update_status(f"Error saving semantic data: {str(e)}")
        except Exception as e:
            self._update_status(f"Semantic save error: {str(e)}")
    
    def _log_message(self, bot, message):
        """Log message to file"""
        if not self.log_path:
            return
        
        try:
            with open(self.log_path, "a", encoding="utf-8") as f:
                timestamp = datetime.now().strftime("%H:%M:%S")
                f.write(f"{timestamp} [{bot}] {message}\n")
        except Exception as e:
            self._update_status(f"Log error: {str(e)}")
    
    def _check_contradiction(self, bot, text):
        """Check for contradictions in bot's statements"""
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return
        
        try:
            current_emb = self.sem_model.encode(text).tolist()
            for prev_entry in self.semantic_store[bot][-5:-1]:
                prev_emb = prev_entry.get('emb', [])
                if prev_emb and cosine_sim(current_emb, prev_emb) < 0.1:
                    self._on_monitor_warning(f"{bot} may have contradicted previous statements")
                    break
        except Exception as e:
            self._update_status(f"Contradiction check error: {str(e)}")
    
    def _calculate_coherence(self, bot):
        """Calculate coherence score for bot"""
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return 0.5
        
        entries = self.semantic_store[bot]
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0.5
        
        sims = [cosine_sim(last_emb, r.get('emb', [])) for r in entries[:-1] if r.get('emb')]
        return float(sum(sims) / len(sims)) if sims else 0.5
    
    def _count_contradictions(self, bot):
        """Count potential contradictions"""
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return 0
        
        entries = self.semantic_store[bot]
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0
        
        return sum(1 for r in entries[:-1] if r.get('emb') and cosine_sim(last_emb, r['emb']) < 0.1)
    
    def _calculate_relevance(self, bot):
        """Calculate relevance score"""
        topic = self.topic_edit.text().strip().lower()
        if not topic or not self.sem_model:
            return 0.5
        
        entries = self.semantic_store.get(bot, [])
        if not entries:
            return 0.5
        
        topic_emb = self.sem_model.encode(topic)
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0.5
        
        return cosine_sim(topic_emb, last_emb)
    
    def _call_model(self, prompt):
        """Synchronous model call with detailed logging"""
        prov_model = self.model_combo.currentData() or ("openai", "gpt-4o-mini")
        provider, model = prov_model
        try:
            if provider == "openai" and openai_client:
                response = openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temp_slider.value() / 100.0,
                    max_tokens=self.token_spin.value()
                )
                return response.choices[0].message.content.strip()
            elif provider == "ollama":
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=30
                )
                return response.json().get("response", "").strip()
            else:
                return "[Model not available]"
        except Exception as e:
            with open(os.path.join(self.session_dir, "errors.log"), "a", encoding="utf-8") as f:
                f.write(f"{datetime.now()}: {str(e)} - Prompt: {prompt[:100]}\n")
            return f"[Model error: {str(e)}]"

    async def _finalize_epoch(self, epoch_id, epoch_dir, stages):
        """Finalize epoch with detailed logging"""
        try:
            if self.log_path and os.path.exists(self.log_path):
                shutil.copy(self.log_path, os.path.join(epoch_dir, f"transcript_{epoch_id}.txt"))

            if self.sem_model and self.log_path:
                try:
                    with open(self.log_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content:
                        embedding = self.sem_model.encode(content).tolist()
                        with open(os.path.join(epoch_dir, f"vectors_{epoch_id}.json"), "w", encoding="utf-8") as f:
                            json.dump(embedding, f)
                except Exception as e:
                    self._update_status(f"Vector save error: {str(e)}")

            if self.score_updates:
                with open(os.path.join(epoch_dir, f"scores_{epoch_id}.json"), "w", encoding="utf-8") as f:
                    json.dump(self.score_updates, f, indent=2)

            if self.chk_enable_arbiter.isChecked():
                summary = await self._generate_arbiter_summary(epoch_id, stages)
                with open(os.path.join(epoch_dir, f"summary_{epoch_id}.txt"), "w", encoding="utf-8") as f:
                    f.write(summary)
                self.message_ready.emit("Arbiter", "Summary", summary)

            with open(os.path.join(epoch_dir, "diagnostics.log"), "w", encoding="utf-8") as f:
                f.write(f"Epoch {epoch_id} completed at {datetime.now()}\n")
                f.write(f"Talking Points: {json.dumps(self.worker.talking_points, indent=2)}\n")
                f.write(f"Final Scores: {json.dumps(self.score_updates[-2:], indent=2)}\n")
        except Exception as e:
            self._update_status(f"Epoch finalization error: {str(e)}")

    async def _generate_arbiter_summary(self, epoch_id, stages):
        """Generate Arbiter summary with epoch context"""
        try:
            transcript = ""
            if self.log_path and os.path.exists(self.log_path):
                with open(self.log_path, "r", encoding="utf-8") as f:
                    transcript = f.read()

            prompt = f"""As the Arbiter AI, provide a comprehensive summary of Epoch {epoch_id}.

Debate Transcript (last 2000 characters):
{transcript[-2000:]}

Analyze:
1. Key arguments from Red and Blue
2. Strongest points made by each
3. Areas of agreement and disagreement
4. Logical consistency of each bot
5. Recommendations for the next epoch, incorporating prior vectors or conclusions

Provide a structured summary in markdown format."""
            
            return self._call_model(prompt)
        except Exception as e:
            return f"Summary generation error: {str(e)}"

# ─── Global Exception Handler ───────────────────────────────────────────────
def except_hook(cls, exception, traceback):
    error_msg = f"Unhandled exception at {datetime.now().strftime('%H:%M:%S')}:\n{''.join(traceback.format_exception(exception
```



## Module `Debate_AI_Old\idea_bot.py`

```python
# idea_bot.py

import os, json, sys, threading
from PyQt6.QtWidgets import (
    QApplication, QDialog, QVBoxLayout, QHBoxLayout, QTextBrowser,
    QLineEdit, QPushButton, QLabel, QComboBox, QMessageBox
)
from PyQt6.QtCore import Qt
import speech_recognition as sr
import requests
import openai

# ------------- CONFIG PATHS ----------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
API_KEY_PATH = os.path.join(BASE_DIR, "api", "api_key.txt")
OUTPUT_JSON = os.path.join(BASE_DIR, "staging", "idea_output.json")
os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)

# ------------- LOAD API ----------------
def load_api_key():
    try:
        with open(API_KEY_PATH, "r") as f:
            return f.read().strip()
    except:
        return None

OPENAI_API_KEY = load_api_key()
if OPENAI_API_KEY:
    openai.api_key = OPENAI_API_KEY

# ------------- MAIN IDEA DIALOG ----------------
class IdeaBotDialog(QDialog):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("IDEA Assistant – Brainstorm")
        self.setMinimumSize(500, 600)
        self.setStyleSheet("background-color: #222; color: #ddd;")
        self.history = []
        self.provider = "OpenAI"
        self.model = "gpt-4o"

        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout(self)

        # Model Provider
        provider_layout = QHBoxLayout()
        self.provider_combo = QComboBox()
        self.provider_combo.addItems(["OpenAI", "Ollama"])
        self.provider_combo.currentTextChanged.connect(self.update_models)
        self.model_combo = QComboBox()
        self.update_models("OpenAI")
        provider_layout.addWidget(QLabel("Provider:"))
        provider_layout.addWidget(self.provider_combo)
        provider_layout.addWidget(QLabel("Model:"))
        provider_layout.addWidget(self.model_combo)
        layout.addLayout(provider_layout)

        # History viewer
        self.chat_display = QTextBrowser()
        self.chat_display.setStyleSheet("background-color: #111; color: #ccc;")
        layout.addWidget(self.chat_display)

        # Input row
        input_row = QHBoxLayout()
        self.input_field = QLineEdit()
        self.input_field.setPlaceholderText("Enter idea or question...")
        self.input_field.returnPressed.connect(self.submit_input)
        input_row.addWidget(self.input_field)

        self.mic_button = QPushButton("🎤")
        self.mic_button.clicked.connect(self.record_voice)
        input_row.addWidget(self.mic_button)

        self.submit_button = QPushButton("Send")
        self.submit_button.clicked.connect(self.submit_input)
        input_row.addWidget(self.submit_button)

        layout.addLayout(input_row)

        # Summary/send row
        button_row = QHBoxLayout()
        self.summarize_button = QPushButton("🧠 Summarize")
        self.summarize_button.clicked.connect(self.summarize)
        button_row.addWidget(self.summarize_button)

        self.send_button = QPushButton("📤 Send to Debate")
        self.send_button.clicked.connect(self.send_to_debate)
        button_row.addWidget(self.send_button)

        layout.addLayout(button_row)

    def update_models(self, provider):
        self.model_combo.clear()
        if provider == "Ollama":
            self.model_combo.addItems([
                "phi4:latest", "mistral:latest", "deepseek-r1:14b", "codellama:latest"
            ])
        else:
            self.model_combo.addItems([
                "gpt-4o", "gpt-4", "gpt-3.5-turbo"
            ])

    def append_message(self, role, msg):
        self.history.append((role, msg))
        self.chat_display.append(f"<b>{role}:</b> {msg}")

    def submit_input(self):
        user_input = self.input_field.text().strip()
        if not user_input:
            return
        self.append_message("User", user_input)
        self.input_field.clear()
        threading.Thread(target=self.get_response, args=(user_input,), daemon=True).start()

    def get_response(self, prompt):
        provider = self.provider_combo.currentText()
        model = self.model_combo.currentText()

        try:
            if provider == "OpenAI":
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}]
                )
                reply = response["choices"][0]["message"]["content"]
            else:
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
                resp = requests.post("http://localhost:11434/api/chat", json=payload).json()
                reply = resp.get("message", {}).get("content") or resp.get("response", "[Ollama: No valid response]")
        except Exception as e:
            reply = f"[Error] {str(e)}"

        self.append_message(model, reply)

    def summarize(self):
        self.append_message("System", "[Summarizing brainstorm…]")
        all_text = "\n".join(f"{role}: {msg}" for role, msg in self.history)
        prompt = (
            "You are summarizing a brainstorming chat log into a debate schema.\n"
            "Return this format:\n"
            "{\n  \"category\": \"...\",\n  \"topic\": \"...\",\n  \"subtopics\": [\"...\", \"...\"]\n}\n\n"
            f"Chat Log:\n{all_text}"
        )
        threading.Thread(target=self.get_summary, args=(prompt,), daemon=True).start()

    def get_summary(self, prompt):
        provider = self.provider_combo.currentText()
        model = self.model_combo.currentText()

        try:
            if provider == "OpenAI":
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}]
                )
                reply = response["choices"][0]["message"]["content"]
            else:
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
                resp = requests.post("http://localhost:11434/api/chat", json=payload).json()
                reply = resp.get("message", {}).get("content") or resp.get("response", "[No valid Ollama summary]")
        except Exception as e:
            reply = f"[Error] {str(e)}"

        self.append_message("Summary", reply)

    def send_to_debate(self):
        latest = self.history[-1][1] if self.history else ""
        try:
            parsed = json.loads(latest)
            with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
                json.dump(parsed, f, indent=2)
            self.append_message("System", "✅ Sent to debate system.")
        except Exception as e:
            QMessageBox.warning(self, "Error", f"Failed to parse summary JSON: {e}")

    def record_voice(self):
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            self.append_message("System", "🎙 Listening...")
            try:
                audio = recognizer.listen(source, timeout=6)
                text = recognizer.recognize_google(audio)
                self.input_field.setText(text)
                self.append_message("STT", text)
            except Exception as e:
                self.append_message("STT Error", str(e))


# --------- MAIN FOR TESTING ----------
if __name__ == "__main__":
    app = QApplication(sys.argv)
    dlg = IdeaBotDialog()
    dlg.show()
    sys.exit(app.exec())
```

**Classes:** IdeaBotDialog
**Functions:** load_api_key()


## Module `Debate_AI_Old\monitor_debate.py`

```python
import subprocess
import time
import os
import sys

# Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DEBATE_SCRIPT = os.path.join(BASE_DIR, "Debate_AI.py")
LOG_FILE = os.path.join(BASE_DIR, "quick_error.log")

def monitor_debate():
    # Start log
    with open(LOG_FILE, 'w', encoding='utf-8') as log:
        log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Monitoring started for Debate_AI.py\n")

    # Launch Debate_AI.py with the same Python interpreter
    try:
        process = subprocess.Popen(
            [sys.executable, DEBATE_SCRIPT],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=BASE_DIR
        )

        # Read and log output/error
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            while True:
                out = process.stdout.readline()
                err = process.stderr.readline()

                if err:
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                    line = f"[{timestamp}] ERROR: {err.strip()}\n"
                    print(line, end='')
                    log.write(line)
                    log.flush()

                if out:
                    print(f"[OUTPUT] {out.strip()}")

                if process.poll() is not None:
                    break

            # Final exit log
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
            log.write(f"[{timestamp}] Debate_AI.py has exited with code {process.returncode}.\n")

    except Exception as exc:
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Exception: {exc}\n")

if __name__ == "__main__":
    print("Monitoring Debate_AI.py for errors...")
    monitor_debate()
    print("Debate_AI.py has stopped. Monitor exiting.")
```

**Functions:** monitor_debate()


## Module `Debate_AI_Old\working_debate.py`

```python
#!/usr/bin/env python3
"""
Debate_AI.py – Debate Simulator with Dockable & Splitter UI
• Dockable panels for David (Blue), Zira (Red), Session control  
• Central vertical splitter: Conversation (top) & Debate Graph (bottom)  
• Color‑coded panels & bubbles  
• Provider/model selectors per agent  
• Session metadata display (session name + epoch)  
• Real‑time Node creation and vector insertion  
• Rolling window of last N pairs + semantic recall + schema‑drift  
• Full micro‑loop in DebateWorker  
• Automatic session start on “Start Debate”  
• TTS integration and styled conversation window  
• Voice‑selection dropdowns for each bot  
• Ensures David (Blue) speaks first, then Zira (Red)
"""

import sys
import os
import importlib.util
import uuid
import json
import time
from datetime import datetime

import requests
import openai
import pyttsx3
import types

from PyQt6.QtWidgets import (
    QApplication, QMainWindow, QWidget, QSplitter,
    QVBoxLayout, QHBoxLayout, QLabel, QPushButton,
    QComboBox, QLineEdit, QSpinBox, QCheckBox,
    QScrollArea, QFrame, QTreeWidget, QTreeWidgetItem,
    QDockWidget, QFileDialog, QMessageBox
)
from PyQt6.QtGui  import QAction
from PyQt6.QtCore import Qt, QThread, pyqtSignal, QCoreApplication

# Ensure this script’s directory is on the import path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)

# Dynamically load TTSManager from TTS.py or tts.py
TTSManager = None
for name in ("TTS", "tts"):
    path = os.path.join(SCRIPT_DIR, f"{name}.py")
    if not os.path.isfile(path):
        continue
    spec = importlib.util.spec_from_file_location(name, path)
    module = importlib.util.module_from_spec(spec)
    try:
        spec.loader.exec_module(module)
        TTSManager = getattr(module, "TTSManager", None)
        if TTSManager:
            break
    except Exception:
        continue

if TTSManager is None:
    raise ModuleNotFoundError(
        "Could not load TTSManager from TTS.py or tts.py in the script directory."
    )

from graph_manager      import GraphContextManager, Node
from session_manager    import SessionManager
from vector_memory      import VectorMemoryManager
from persona_engine     import PersonaEngine

SESSIONS_DIR  = os.path.join(SCRIPT_DIR, "sessions")
ROLLING_PAIRS = 10


class DebateWorker(QThread):
    new_node        = pyqtSignal(Node)
    debate_finished = pyqtSignal()

    def __init__(self, topic, personaA, personaB,
                 graph_mgr, session_mgr, vector_mgr,
                 use_web, rounds):
        super().__init__()
        self.topic        = topic
        self.personaA     = personaA
        self.personaB     = personaB
        self.graph_mgr    = graph_mgr
        self.session_mgr  = session_mgr
        self.vector_mgr   = vector_mgr
        self.use_web      = use_web
        self.rounds       = rounds
        self._stop        = False

    def stop(self):
        self._stop = True

    def run(self):
        for _ in range(self.rounds):
            if self._stop:
                break

            # start new epoch
            self.session_mgr.start_epoch()

            # ─────────────────────────────────────────────────────────────
            # Semantic Recall: fetch from both indexes
            open_hits = self.vector_mgr.semantic_search(
                self.topic, top_k=4, index="open"
            )
            concl_hits = self.vector_mgr.semantic_search(
                self.topic, top_k=4, index="concluded"
            )
            # build context strings
            open_ctx = "\n".join(
                f"• {self.vector_mgr.get_vector_text(id_)}"
                for id_, _ in open_hits
            )
            concl_ctx = "\n".join(
                f"• {self.vector_mgr.get_vector_text(id_)}"
                for id_, _ in concl_hits
            )
            semantic = (
                "=== Open‑ended Context ===\n"  f"{open_ctx}\n\n"
                "=== Concluded Facts ===\n"    f"{concl_ctx}"
            )
            # ─────────────────────────────────────────────────────────────

            # Blue (David) then Red (Zira)
            for side, persona in (('A', self.personaA), ('B', self.personaB)):
                if self._stop:
                    break

                # build prompt
                full_hist = self.graph_mgr.serialize_history()
                lines     = full_hist.splitlines()
                recent    = "\n".join(lines[-2 * ROLLING_PAIRS:])
                prompt = (
                    f"Debate Topic: {self.topic}\n\n"
                    f"Recent conversation:\n{recent}\n\n"
                    f"Additional context:\n{semantic}\n\n"
                    f"{persona.label}, please make your argument."
                )
                if self.use_web:
                    prompt += "\n\nYou may use the web to look up facts."

                text = persona.call_llm(prompt)

                # construct node, with dynamic intent/schema_drift support
                node = Node(
                    id=str(uuid.uuid4()),
                    author=persona.label,
                    role=side,
                    type="claim" if side=='A' else "rebuttal",
                    content=text,
                    tags=[f"topic:{self.topic}", f"epoch:{self.session_mgr.current_epoch:04d}"],
                    references=[],
                    status="open",
                    timestamp=datetime.utcnow().isoformat(),
                    intent=None,
                    schema_drift=False
                )

                # detect new intent marker
                if "[New Intent]:" in text:
                    new_intent = (
                        text.split("[New Intent]:",1)[1]
                            .splitlines()[0]
                            .strip()
                    )
                    node.intent       = new_intent
                    node.schema_drift = True

                # record, persist, and emit
                self.graph_mgr.add_node(node)
                self.session_mgr.save_node(node)
                self.vector_mgr.insert_node(
                    node, concluded=(node.status != "open")
                )
                self.new_node.emit(node)
                self.msleep(500)

        self.debate_finished.emit()


class DebateSimulatorApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate Simulator – Dockable Edition")
        self.resize(1400, 900)

        # Core managers
        self.graph_mgr   = GraphContextManager()
        self.session_mgr = SessionManager(SESSIONS_DIR)
        self.vector_mgr  = VectorMemoryManager()

        # Load personas & override labels
        self.personaA = PersonaEngine.load("blue", self.vector_mgr)
        self.personaA.label = "David"
        self.personaB = PersonaEngine.load("red", self.vector_mgr)
        self.personaB.label = "Zira"

        # TTS
        self.tts_manager = TTSManager()
        # monkey-patch legacy APIs
        if not hasattr(self.tts_manager, 'set_voice'):
            def _set_voice(this, speaker, voice_name):
                for v in this.engine.getProperty('voices'):
                    if voice_name.lower() in v.name.lower():
                        this.voice_map[speaker] = v.id
                        break
            self.tts_manager.set_voice = types.MethodType(
                _set_voice, self.tts_manager
            )
        if not hasattr(self.tts_manager, 'toggle_auto_read'):
            self.tts_manager.auto_read_enabled = True
            def _toggle(this, enabled):
                this.auto_read_enabled = enabled
            self.tts_manager.toggle_auto_read = types.MethodType(
                _toggle, self.tts_manager
            )

        self.worker              = None
        self.current_session_dir = None
        self.use_web             = False

        os.makedirs(SESSIONS_DIR, exist_ok=True)
        self._build_menu()
        self._build_docks()
        self._init_layout()

    def _build_menu(self):
        mb = self.menuBar()
        file = mb.addMenu("File")
        exp = QAction("Export Graph JSON…", self)
        exp.triggered.connect(self._export_graph)
        file.addAction(exp)
        file.addSeparator()
        file.addAction("Reset Settings", self._reset_config)
        file.addAction("Exit", self.close)

    def _build_docks(self):
        # Session Control
        sess = QWidget(); sl = QHBoxLayout(sess)
        sl.addWidget(QLabel("Session:"));    self.session_label = QLabel("—"); sl.addWidget(self.session_label)
        sl.addSpacing(20)
        sl.addWidget(QLabel("Epoch:"));      self.epoch_label   = QLabel("0");  sl.addWidget(self.epoch_label)
        sl.addStretch()
        dock_sess = QDockWidget("Session Control", self)
        dock_sess.setWidget(sess)
        dock_sess.setAllowedAreas(Qt.DockWidgetArea.TopDockWidgetArea)
        self.addDockWidget(Qt.DockWidgetArea.TopDockWidgetArea, dock_sess)

        # David (Blue) Panel
        blue = QWidget(); bl = QVBoxLayout(blue)
        blue.setStyleSheet("background:#E3F2FD;")
        bl.addWidget(QLabel("<b>David (Blue)</b>"))
        bl.addWidget(QLabel("Provider:"))
        self.providerA_cb = QComboBox(); self.providerA_cb.addItems(["OpenAI", "Ollama"]); bl.addWidget(self.providerA_cb)
        bl.addWidget(QLabel("Model:")); self.modelA_cb = QComboBox(); bl.addWidget(self.modelA_cb)
        bl.addWidget(QLabel("Topic:")); self.topic_le = QLineEdit(); bl.addWidget(self.topic_le)
        bl.addWidget(QLabel("Session Name (optional):")); self.session_name_le = QLineEdit(); bl.addWidget(self.session_name_le)
        bl.addWidget(QLabel("Rounds:")); self.rounds_sb = QSpinBox(); self.rounds_sb.setRange(1,50); self.rounds_sb.setValue(3); bl.addWidget(self.rounds_sb)
        bl.addWidget(QLabel("Use Web:")); self.web_cb = QCheckBox(); self.web_cb.stateChanged.connect(lambda s: setattr(self, 'use_web', s == Qt.CheckState.Checked)); bl.addWidget(self.web_cb)
        self.start_btn = QPushButton("Start Debate"); self.start_btn.clicked.connect(self.start_debate); bl.addWidget(self.start_btn)
        self.stop_btn = QPushButton("Stop Debate"); self.stop_btn.clicked.connect(self.stop_debate); self.stop_btn.setEnabled(False); bl.addWidget(self.stop_btn)

        # TTS Controls
        self.auto_read_cb = QCheckBox("Auto Read"); self.auto_read_cb.setChecked(True)
        self.auto_read_cb.stateChanged.connect(lambda s: self.tts_manager.toggle_auto_read(s == Qt.CheckState.Checked))
        bl.addWidget(self.auto_read_cb)
        self.read_btn = QPushButton("Read History"); self.read_btn.clicked.connect(self.tts_manager.manual_read); bl.addWidget(self.read_btn)
        self.stop_tts_btn = QPushButton("Stop TTS"); self.stop_tts_btn.clicked.connect(self.tts_manager.stop_monitoring); bl.addWidget(self.stop_tts_btn)

        # Voice selector for David
        bl.addWidget(QLabel("David Voice:"))
        self.voiceA_cb = QComboBox()
        voices = self.tts_manager.engine.getProperty('voices')
        for v in voices:
            self.voiceA_cb.addItem(v.name)
        default_david = next((v.name for v in voices if v.id == self.tts_manager.voice_map.get('David')), None)
        if default_david:
            self.voiceA_cb.setCurrentText(default_david)
            self.tts_manager.set_voice('David', default_david)
        self.voiceA_cb.currentTextChanged.connect(lambda name: self.tts_manager.set_voice('David', name))
        bl.addWidget(self.voiceA_cb)

        self.providerA_cb.currentTextChanged.connect(lambda p: self._update_models(p, self.modelA_cb, self.personaA))
        self._update_models(self.personaA.provider, self.modelA_cb, self.personaA)
        self.providerA_cb.setCurrentText(self.personaA.provider)
        self.modelA_cb.setCurrentText(self.personaA.model)

        dock_blue = QDockWidget("David Controls", self)
        dock_blue.setWidget(blue)
        dock_blue.setAllowedAreas(Qt.DockWidgetArea.LeftDockWidgetArea)
        self.addDockWidget(Qt.DockWidgetArea.LeftDockWidgetArea, dock_blue)

        # Zira (Red) Panel
        red = QWidget(); rl = QVBoxLayout(red)
        red.setStyleSheet("background:#FFEBEE;")
        rl.addWidget(QLabel("<b>Zira (Red)</b>"))
        rl.addWidget(QLabel("Provider:"))
        self.providerB_cb = QComboBox(); self.providerB_cb.addItems(["OpenAI", "Ollama"]); rl.addWidget(self.providerB_cb)
        rl.addWidget(QLabel("Model:")); self.modelB_cb = QComboBox(); rl.addWidget(self.modelB_cb)
        rl.addWidget(QLabel("Zira Voice:"))
        self.voiceB_cb = QComboBox()
        for v in voices:
            self.voiceB_cb.addItem(v.name)
        default_zira = next((v.name for v in voices if v.id == self.tts_manager.voice_map.get('Zira')), None)
        if default_zira:
            self.voiceB_cb.setCurrentText(default_zira)
            self.tts_manager.set_voice('Zira', default_zira)
        self.voiceB_cb.currentTextChanged.connect(lambda name: self.tts_manager.set_voice('Zira', name))
        rl.addWidget(self.voiceB_cb)

        self.providerB_cb.currentTextChanged.connect(lambda p: self._update_models(p, self.modelB_cb, self.personaB))
        self._update_models(self.personaB.provider, self.modelB_cb, self.personaB)
        self.providerB_cb.setCurrentText(self.personaB.provider)
        self.modelB_cb.setCurrentText(self.personaB.model)

        dock_red = QDockWidget("Zira Controls", self)
        dock_red.setWidget(red)
        dock_red.setAllowedAreas(Qt.DockWidgetArea.RightDockWidgetArea)
        self.addDockWidget(Qt.DockWidgetArea.RightDockWidgetArea, dock_red)

    def _init_layout(self):
        splitter = QSplitter(Qt.Orientation.Vertical)

        # Conversation pane
        conv = QWidget(); conv.setStyleSheet("background:#E1BEE7;")
        cl = QVBoxLayout(conv)
        self.chat_cont = QFrame(); self.chat_cont.setFrameShape(QFrame.Shape.NoFrame)
        self.chat_layout = QVBoxLayout(self.chat_cont)
        self.scroll = QScrollArea(); self.scroll.setWidgetResizable(True); self.scroll.setWidget(self.chat_cont)
        cl.addWidget(self.scroll)
        splitter.addWidget(conv)

        # Debate graph pane
        graph = QWidget(); gl = QVBoxLayout(graph)
        self.tree = QTreeWidget(); self.tree.setHeaderLabels(["ID","Author","Type","Status","Timestamp"])
        gl.addWidget(self.tree)
        splitter.addWidget(graph)

        self.setCentralWidget(splitter)

    def _update_models(self, provider, combo, persona):
        combo.clear()
        combo.addItems(persona.available_models())

    def start_debate(self):
        topic = self.topic_le.text().strip()
        if not topic:
            QMessageBox.warning(self, "Missing Topic", "Please enter a debate topic.")
            return

        sess_name = self.session_name_le.text().strip()
        if not sess_name:
            safe = "".join(c if c.isalnum() or c=='_' else '_' for c in topic)
            sess_name = f"{safe}_{int(time.time())}"
        else:
            sess_name = "".join(c if c.isalnum() or c=='_' else '_' for c in sess_name)

        self.session_mgr.start_session(sess_name)
        self.session_label.setText(sess_name)
        self.epoch_label.setText("0")
        self.current_session_dir = os.path.join(SESSIONS_DIR, sess_name)
        os.makedirs(self.current_session_dir, exist_ok=True)

        # Prepare and start TTS
        self.tts_manager.set_session_path(self.current_session_dir)
        hist_fp = os.path.join(self.current_session_dir, "history.txt")
        open(hist_fp, 'w', encoding='utf-8').close()
        self.tts_manager.start_monitoring()

        # Clear UI
        for i in reversed(range(self.chat_layout.count())):
            w = self.chat_layout.itemAt(i).widget()
            if w: w.setParent(None)
        self.tree.clear()

        # Apply persona settings
        self.personaA.provider = self.providerA_cb.currentText()
        self.personaA.model    = self.modelA_cb.currentText()
        self.personaB.provider = self.providerB_cb.currentText()
        self.personaB.model    = self.modelB_cb.currentText()

        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)

        self.worker = DebateWorker(
            topic, self.personaA, self.personaB,
            self.graph_mgr, self.session_mgr, self.vector_mgr,
            self.use_web, self.rounds_sb.value()
        )
        self.worker.new_node.connect(self._on_new_node)
        self.worker.debate_finished.connect(self._on_debate_finished)
        self.worker.start()

    def stop_debate(self):
        if self.worker:
            self.worker.stop()
        self.tts_manager.stop_monitoring()
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)

    def _on_new_node(self, node: Node):
        self.epoch_label.setText(f"{self.session_mgr.current_epoch:04d}")

        # Chat bubble
        w = QWidget(); hl = QHBoxLayout(w)
        bub = QWidget(); bl = QVBoxLayout(bub)
        color = "#E3F2FD" if node.role=='A' else "#FFEBEE"
        bub.setStyleSheet(f"background:{color}; padding:8px; border-radius:4px;")
        bl.addWidget(QLabel(f"<b>{node.author}</b>"))
        lbl = QLabel(node.content); lbl.setWordWrap(True)
        bl.addWidget(lbl)
        if node.role=='A':
            hl.addWidget(bub); hl.addStretch()
        else:
            hl.addStretch(); hl.addWidget(bub)
        self.chat_layout.addWidget(w)
        QCoreApplication.processEvents()
        self.scroll.verticalScrollBar().setValue(self.scroll.verticalScrollBar().maximum())

        # Append to history.txt
        if self.current_session_dir:
            hp = os.path.join(self.current_session_dir, "history.txt")
            with open(hp, 'a', encoding='utf-8') as hf:
                hf.write(f"[{node.author}]\n")
                for line in node.content.splitlines():
                    hf.write(line + "\n")
                hf.write("\n")

        # Update debate graph tree
        item = QTreeWidgetItem([node.id, node.author, node.type, node.status, node.timestamp])
        # highlight schema‑drift
        if getattr(node, "schema_drift", False):
            for c in range(self.tree.columnCount()):
                item.setBackground(c, Qt.GlobalColor.red)
        else:
            brush = Qt.GlobalColor.green if node.status=='closed' else Qt.GlobalColor.darkYellow
            for c in range(self.tree.columnCount()):
                item.setForeground(c, brush)
        self.tree.addTopLevelItem(item)

        # Save snapshot
        lg = os.path.join(SCRIPT_DIR, "last_graph.json")
        with open(lg, 'w', encoding='utf-8') as f:
            json.dump(self.graph_mgr.to_dict(), f, indent=2)

    def _on_debate_finished(self):
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)

    def _export_graph(self):
        path, _ = QFileDialog.getSaveFileName(self, "Save Graph JSON", SCRIPT_DIR, "JSON (*.json)")
        if path:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(self.graph_mgr.to_dict(), f, indent=2)

    def _reset_config(self):
        cfg = os.path.join(SCRIPT_DIR, "user_config.json")
        if os.path.exists(cfg):
            os.remove(cfg)
        QMessageBox.information(self, "Reset Settings", "Settings have been reset to defaults.")


if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = DebateSimulatorApp()
    window.show()
    sys.exit(app.exec())
```

Debate_AI.py – Debate Simulator with Dockable & Splitter UI
• Dockable panels for David (Blue), Zira (Red), Session control  
• Central vertical splitter: Conversation (top) & Debate Graph (bottom)  
• Color‑coded panels & bubbles  
• Provider/model selectors per agent  
• Session metadata display (session name + epoch)  
• Real‑time Node creation and vector insertion  
• Rolling window of last N pairs + semantic recall + schema‑drift  
• Full micro‑loop in DebateWorker  
• Automatic session start on “Start Debate”  
• TTS integration and styled conversation window  
• Voice‑selection dropdowns for each bot  
• Ensures David (Blue) speaks first, then Zira (Red)
**Classes:** DebateWorker, DebateSimulatorApp


## Module `Debate_AI_Old\debate 4\Debate_AI\Analyze_All.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get the script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")

        for root, dirs, files in os.walk(start_path):
            # Ensure only directories below the script's location are analyzed
            if not root.startswith(start_path):
                continue  # Skip any directory outside the scope

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and analyze.txt from files list
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file_path = os.path.join(root, f)
                file.write(f"{file_indent}|-- {f}\n")

                # Capture file content
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f_content:
                        file_data = f_content.read()
                        file.write(f"\n{file_indent}    --- File Content ---\n")
                        file.write(f"{file_data}\n")
                        file.write(f"{file_indent}    -------------------\n\n")
                except Exception as e:
                    file.write(f"{file_indent}    [Error reading file: {str(e)}]\n\n")

    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.abspath(os.path.dirname(__file__))  # Get the script's location
    analyze_folders(script_directory)  # Analyze only from script's directory downward
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\debate 4\Debate_AI\Analyze_folders.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")
        
        for root, dirs, files in os.walk(start_path):
            # Skip directories named 'venv'
            if "venv" in root.split(os.sep):
                continue

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and output file from the list of files
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file.write(f"{file_indent}|-- {f}\n")
    
    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.dirname(os.path.abspath(__file__))
    analyze_folders(script_directory)
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\debate 4\Debate_AI\bots.py`

```python
#!/usr/bin/env python3
# File: bots.py
# Defines DebateBot: encapsulates think/respond logic, provider selection, and cancellation.

import threading

class DebateBot:
    def __init__(self, name, persona, client, model, tts=None):
        """
        :param name:         "Bot A" or "Bot B"
        :param persona:      String describing the bot's persona
        :param client:       Instance of OpenAIClient or OllamaClient
        :param model:        Model name to use (e.g. "gpt-4o-mini")
        :param tts:          Optional TTSManager instance for spoken output
        """
        self.name = name
        self.persona = persona
        self.client = client
        self.model = model
        self.tts = tts

        # Internal state to allow cancellation
        self._lock = threading.Lock()
        self._current_call = None

    def think(self, context):
        """
        Generate internal monologue based on persona and context.
        Returns the monologue string.
        """
        prompt = (
            f"You are {self.name}, persona: {self.persona}.\n"
            "First, think to yourself (internal monologue) about the following context:\n"
            f"{context}\n\n"
            "Begin your internal reasoning:"
        )
        return self._call_llm(prompt, max_tokens=150, prefix="MONOLOGUE")

    def respond(self, context, monologue):
        """
        Generate the public response given context and the internal monologue.
        Returns the response string.
        """
        prompt = (
            f"You are {self.name}, persona: {self.persona}.\n"
            "You have thought:\n"
            f"{monologue}\n\n"
            "Now craft a concise, on-topic debate response to the context:\n"
            f"{context}\n\n"
            "Begin your public response:"
        )
        response = self._call_llm(prompt, max_tokens=100, prefix="RESPONSE")

        # Optionally speak the response
        if self.tts:
            threading.Thread(target=self.tts.speak_text, args=(response, self.name), daemon=True).start()
        return response

    def cancel_current(self):
        """
        Signal the current LLM call to cancel (if possible).
        """
        with self._lock:
            if self._current_call and hasattr(self._current_call, "cancel"):
                try:
                    self._current_call.cancel()
                except Exception:
                    pass

    def _call_llm(self, prompt, max_tokens, prefix):
        """
        Internal helper to call the LLM client with cancellation support.
        """
        # Prepare message depending on provider
        with self._lock:
            # Launch the call in a separate thread if needed
            call = self.client.chat(
                model=self.model,
                prompt=prompt,
                max_tokens=max_tokens
            )
            self._current_call = call

        try:
            result = call.get_response()  # .get_response() should block until done
        except Exception as e:
            result = f"[Error during {prefix}: {e}]"
        finally:
            with self._lock:
                self._current_call = None

        return result.strip()
```

**Classes:** DebateBot


## Module `Debate_AI_Old\debate 4\Debate_AI\Debate_AI.py`

```python
#!/usr/bin/env python3
# File: Debate_AI.py
# Description: Main entry point for the GUI‑based dual‑bot debate system with full session, schema, vector, and TTS support.

import os
import threading
import json
import tkinter as tk
from tkinter import ttk, filedialog, messagebox

# ---- Project modules ----
from session_manager import SessionManager
from schema_viewer import SchemaViewer
from bots import DebateBot
from providers import OpenAIClient, OllamaClient
from vector_utils import VectorStore
from tts import TTSManager

# ---- Constants ----
SESSIONS_DIR = os.path.join(os.path.dirname(__file__), "sessions")
API_DIR      = os.path.join(os.path.dirname(__file__), "api")
API_KEY_FILE = os.path.join(API_DIR, "api_key.txt")

# Ensure directories exist
os.makedirs(SESSIONS_DIR, exist_ok=True)
os.makedirs(API_DIR, exist_ok=True)

# ---- Load API key ----
try:
    with open(API_KEY_FILE, "r", encoding="utf-8") as f:
        OPENAI_API_KEY = f.read().strip()
except FileNotFoundError:
    OPENAI_API_KEY = ""
    

class DebateAIApp(tk.Tk):
    """Main application window for the debate engine."""
    def __init__(self):
        super().__init__()
        self.title("Debate AI Engine")
        self.geometry("1200x800")
        
        # Core components
        self.session_mgr = SessionManager(SESSIONS_DIR)
        self.schema_viewer = SchemaViewer(self)
        self.tts = TTSManager()
        
        # Active session context
        self.current_session = None
        self.schema = {}
        self.user_config = {}
        self.vector_store = None
        
        # LLM clients for each bot
        self.botA_client = None
        self.botB_client = None
        
        # Build GUI
        self._build_gui()
        self._populate_session_dropdown()
    
    def _build_gui(self):
        """Construct all GUI widgets."""
        # ----- Top control panel -----
        topfrm = ttk.Frame(self)
        topfrm.pack(side="top", fill="x", padx=5, pady=5)
        
        # Session controls
        ttk.Label(topfrm, text="Session:").pack(side="left")
        self.session_cb = ttk.Combobox(topfrm, state="readonly")
        self.session_cb.pack(side="left", padx=2)
        self.session_cb.bind("<<ComboboxSelected>>", lambda e: self.load_session())
        ttk.Button(topfrm, text="New Session", command=self.new_session).pack(side="left", padx=2)
        ttk.Button(topfrm, text="Save Session", command=self.save_session).pack(side="left", padx=2)
        
        # Schema editor
        ttk.Button(topfrm, text="View/Edit Schema", command=self._open_schema).pack(side="left", padx=10)
        
        # Debate parameters
        for label in ("Category", "Topic", "Persona A", "Persona B", "Max Turns"):
            ttk.Label(topfrm, text=label + ":").pack(side="left", padx=2)
        self.category_var   = tk.StringVar()
        self.topic_var      = tk.StringVar()
        self.personaA_var   = tk.StringVar()
        self.personaB_var   = tk.StringVar()
        self.maxturns_var   = tk.IntVar(value=10)
        ttk.Entry(topfrm, textvariable=self.category_var, width=15).pack(side="left", padx=2)
        ttk.Entry(topfrm, textvariable=self.topic_var,    width=20).pack(side="left", padx=2)
        ttk.Entry(topfrm, textvariable=self.personaA_var, width=15).pack(side="left", padx=2)
        ttk.Entry(topfrm, textvariable=self.personaB_var, width=15).pack(side="left", padx=2)
        ttk.Spinbox(topfrm, from_=1, to=50, textvariable=self.maxturns_var, width=5).pack(side="left", padx=2)
        
        # TTS toggle
        self.tts_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(topfrm, text="Use TTS", variable=self.tts_var).pack(side="left", padx=10)
        
        # Providers & models per bot
        ttk.Label(topfrm, text="Bot A Provider:").pack(side="left", padx=2)
        self.botA_provider = ttk.Combobox(topfrm, values=["OpenAI","Ollama"], state="readonly")
        self.botA_provider.current(0)
        self.botA_provider.pack(side="left", padx=2)
        ttk.Label(topfrm, text="Model A:").pack(side="left", padx=2)
        self.botA_model = ttk.Combobox(topfrm, values=OpenAIClient.MODELS, state="readonly")
        self.botA_model.set(OpenAIClient.MODELS[0])
        self.botA_model.pack(side="left", padx=2)
        
        ttk.Label(topfrm, text="Bot B Provider:").pack(side="left", padx=8)
        self.botB_provider = ttk.Combobox(topfrm, values=["OpenAI","Ollama"], state="readonly")
        self.botB_provider.current(0)
        self.botB_provider.pack(side="left", padx=2)
        ttk.Label(topfrm, text="Model B:").pack(side="left", padx=2)
        self.botB_model = ttk.Combobox(topfrm, values=OpenAIClient.MODELS, state="readonly")
        self.botB_model.set(OpenAIClient.MODELS[0])
        self.botB_model.pack(side="left", padx=2)
        
        # Bind provider/model changes
        self.botA_provider.bind("<<ComboboxSelected>>", lambda e: self._update_model_list(self.botA_provider, self.botA_model))
        self.botB_provider.bind("<<ComboboxSelected>>", lambda e: self._update_model_list(self.botB_provider, self.botB_model))
        
        # Debate control buttons
        ttk.Button(topfrm, text="Start Debate", command=self.start_debate).pack(side="right", padx=5)
        ttk.Button(topfrm, text="Skip Turn",    command=self.skip_turn).pack(side="right", padx=5)
        ttk.Button(topfrm, text="Stop Debate",  command=self.stop_debate).pack(side="right", padx=5)
        
        # ----- Main panes -----
        mainfrm = ttk.Frame(self)
        mainfrm.pack(fill="both", expand=True, padx=5, pady=5)
        
        # Bot A monologue (blue)
        leftfrm = ttk.Labelframe(mainfrm, text="Bot A Monologue", width=25)
        leftfrm.pack(side="left", fill="both", expand=True, padx=2)
        self.monoA_txt = tk.Text(leftfrm, bg="#E3F2FD")
        self.monoA_txt.pack(fill="both", expand=True)
        
        # Conversation window (purple)
        midfrm = ttk.Labelframe(mainfrm, text="Conversation", width=50)
        midfrm.pack(side="left", fill="both", expand=True, padx=2)
        self.conv_txt = tk.Text(midfrm, bg="#E0BBE4")
        self.conv_txt.pack(fill="both", expand=True)
        
        # Bot B monologue (red)
        rightfrm = ttk.Labelframe(mainfrm, text="Bot B Monologue", width=25)
        rightfrm.pack(side="left", fill="both", expand=True, padx=2)
        self.monoB_txt = tk.Text(rightfrm, bg="#FFEBEE")
        self.monoB_txt.pack(fill="both", expand=True)
        
        # ----- Bottom file log window -----
        bottomfrm = ttk.Labelframe(self, text="Session Files & Live Logs")
        bottomfrm.pack(side="bottom", fill="x", padx=5, pady=5)
        self.filelog_txt = tk.Text(bottomfrm, height=6)
        self.filelog_txt.pack(fill="x", expand=False)
    
    def _populate_session_dropdown(self):
        """Load existing session names into the dropdown."""
        names = self.session_mgr.list_sessions()
        self.session_cb['values'] = names
        if names:
            self.session_cb.current(0)
            self.load_session()
    
    def new_session(self):
        """Prompt for session name and create scaffolding."""
        name = tk.simpledialog.askstring("New Session", "Enter session name:")
        if not name:
            return
        try:
            self.session_mgr.create_session(name)
            self._populate_session_dropdown()
            self.session_cb.set(name)
            self.load_session()
        except Exception as e:
            messagebox.showerror("Error", f"Could not create session:\n{e}")
    
    def load_session(self):
        """Load schema, user_config, and logs from selected session."""
        name = self.session_cb.get()
        if not name:
            return
        self.current_session = name
        session_path = os.path.join(SESSIONS_DIR, name)
        # Load schema & config
        self.schema, self.user_config = self.session_mgr.load_session(session_path)
        # Apply to GUI fields
        self.category_var.set(self.schema.get("category",""))
        self.topic_var.set(self.schema.get("topic",""))
        self.personaA_var.set(self.schema.get("botA_persona",""))
        self.personaB_var.set(self.schema.get("botB_persona",""))
        self.maxturns_var.set(self.schema.get("max_turns",10))
        self.tts_var.set(self.schema.get("use_tts",True))
        # Providers & models
        self.botA_provider.set(self.schema.get("botA_provider","OpenAI"))
        self.botA_model.set(self.schema.get("botA_model",OpenAIClient.MODELS[0]))
        self.botB_provider.set(self.schema.get("botB_provider","OpenAI"))
        self.botB_model.set(self.schema.get("botB_model",OpenAIClient.MODELS[0]))
        # Load logs into text widgets
        self._load_text_file("conversation.txt", self.conv_txt)
        self._load_text_file("monologues_A.txt", self.monoA_txt)
        self._load_text_file("monologues_B.txt", self.monoB_txt)
        self._load_text_file("logs.json",      self.filelog_txt, wrap_json=True)
        # Prepare vector store
        self.vector_store = VectorStore(session_path)
    
    def save_session(self):
        """Persist current schema, config, and logs to disk."""
        if not self.current_session:
            messagebox.showwarning("No session", "Please create or load a session first.")
            return
        # Gather schema from GUI fields
        self.schema.update({
            "category":         self.category_var.get(),
            "topic":            self.topic_var.get(),
            "botA_persona":     self.personaA_var.get(),
            "botB_persona":     self.personaB_var.get(),
            "max_turns":        self.maxturns_var.get(),
            "use_tts":          self.tts_var.get(),
            "botA_provider":    self.botA_provider.get(),
            "botA_model":       self.botA_model.get(),
            "botB_provider":    self.botB_provider.get(),
            "botB_model":       self.botB_model.get(),
        })
        # Update user_config
        self.user_config = self.schema.copy()
        # Delegate to session manager
        session_path = os.path.join(SESSIONS_DIR, self.current_session)
        self.session_mgr.save_session(session_path, self.schema, self.user_config)
        self._append_filelog(f"Session '{self.current_session}' saved.")
    
    def _open_schema(self):
        """Open the raw JSON schema editor."""
        if not self.current_session:
            messagebox.showwarning("No session", "Please create or load a session first.")
            return
        session_path = os.path.join(SESSIONS_DIR, self.current_session)
        self.schema_viewer.open(session_path)
    
    def start_debate(self):
        """Kick off the debate loop in a background thread."""
        if not self.current_session:
            messagebox.showwarning("No session", "Please create or load a session first.")
            return
        # Initialize bots with selected providers/models
        self.botA_client = OpenAIClient(api_key=OPENAI_API_KEY) \
            if self.botA_provider.get()=="OpenAI" else OllamaClient()
        self.botB_client = OpenAIClient(api_key=OPENAI_API_KEY) \
            if self.botB_provider.get()=="OpenAI" else OllamaClient()
        # Create DebateBot instances
        self.botA = DebateBot(
            name="Bot A", persona=self.schema["botA_persona"],
            client=self.botA_client, model=self.schema["botA_model"],
            tts=self.tts if self.tts_var.get() else None
        )
        self.botB = DebateBot(
            name="Bot B", persona=self.schema["botB_persona"],
            client=self.botB_client, model=self.schema["botB_model"],
            tts=self.tts if self.tts_var.get() else None
        )
        # Start loop thread
        self._stop_flag = threading.Event()
        threading.Thread(target=self._debate_loop, daemon=True).start()
        self._append_filelog("Debate started.")
    
    def _debate_loop(self):
        """Main loop: alternate Bot A/B for max_turns rounds."""
        max_turns = self.schema.get("max_turns", 10)
        conversation_path = os.path.join(SESSIONS_DIR, self.current_session, "conversation.txt")
        logs_path         = os.path.join(SESSIONS_DIR, self.current_session, "logs.json")
        monoA_path        = os.path.join(SESSIONS_DIR, self.current_session, "monologues_A.txt")
        monoB_path        = os.path.join(SESSIONS_DIR, self.current_session, "monologues_B.txt")
        
        # Ensure files exist
        for p in (conversation_path, logs_path, monoA_path, monoB_path):
            open(p, "a").close()
        
        for turn in range(max_turns):
            if self._stop_flag.is_set():
                break
            
            # Bot A turn
            monologueA = self.botA.think(self._gather_context())
            responseA  = self.botA.respond(self._gather_context(), monologueA)
            self._log_turn(monologueA, responseA, monoA_path, conversation_path, logs_path, "A")
            self._update_gui(monologueA, responseA, target="A")
            
            if self._stop_flag.is_set():
                break
            
            # Bot B turn
            monologueB = self.botB.think(self._gather_context())
            responseB  = self.botB.respond(self._gather_context(), monologueB)
            self._log_turn(monologueB, responseB, monoB_path, conversation_path, logs_path, "B")
            self._update_gui(monologueB, responseB, target="B")
        
        self._append_filelog("Debate loop ended.")
    
    def skip_turn(self):
        """Signal to skip the current LLM call and move on."""
        self.botA.cancel_current()  # or botB depending on who's running
        self.botB.cancel_current()
        self._append_filelog("Turn skipped by user.")
    
    def stop_debate(self):
        """Signal the debate loop to terminate early."""
        self._stop_flag.set()
        self._append_filelog("Debate stopped by user.")
    
    # ---- Helper methods ----
    
    def _gather_context(self):
        """Read last 20 lines from conversation.txt for context."""
        path = os.path.join(SESSIONS_DIR, self.current_session, "conversation.txt")
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()[-20:]
        return "".join(lines)
    
    def _log_turn(self, monologue, response, mono_path, convo_path, logs_path, bot_label):
        """Append monologue, response to files and structured log."""
        # Monologue
        with open(mono_path, "a", encoding="utf-8") as f:
            f.write(monologue + "\n\n")
        # Conversation
        with open(convo_path, "a", encoding="utf-8") as f:
            f.write(f"Bot {bot_label}: {response}\n")
        # Structured logs
        entry = {
            "bot": bot_label,
            "monologue": monologue,
            "response":  response
        }
        logs = []
        with open(logs_path, "r+", encoding="utf-8") as f:
            try:
                logs = json.load(f)
            except json.JSONDecodeError:
                logs = []
            logs.append(entry)
            f.seek(0); f.truncate()
            json.dump(logs, f, indent=2)
    
    def _update_gui(self, monologue, response, target):
        """Thread‑safe GUI update for monologue & conversation panes."""
        def _inner():
            if target == "A":
                self.monoA_txt.insert("end", monologue + "\n\n")
            else:
                self.monoB_txt.insert("end", monologue + "\n\n")
            self.conv_txt.insert("end", f"{target}> {response}\n")
        self.after(0, _inner)
    
    def _append_filelog(self, text):
        """Append a line to the bottom file log window."""
        self.filelog_txt.insert("end", text + "\n")
        self.filelog_txt.see("end")
    
    def _load_text_file(self, filename, widget, wrap_json=False):
        """Load a text or JSON file into a Text widget."""
        path = os.path.join(SESSIONS_DIR, self.current_session, filename)
        try:
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
            if wrap_json:
                # pretty‑print JSON if possible
                try:
                    obj = json.loads(content)
                    content = json.dumps(obj, indent=2)
                except:
                    pass
            widget.delete("1.0", "end")
            widget.insert("1.0", content)
        except FileNotFoundError:
            pass
    
    def _update_model_list(self, provider_cb, model_cb):
        """When provider changes, refresh available model list."""
        provider = provider_cb.get()
        if provider == "OpenAI":
            model_cb['values'] = OpenAIClient.MODELS
        else:
            try:
                model_cb['values'] = OllamaClient.list_models()
            except Exception:
                model_cb['values'] = []
        model_cb.current(0)


if __name__ == "__main__":
    app = DebateAIApp()
    app.mainloop()
```

**Classes:** DebateAIApp


## Module `Debate_AI_Old\debate 4\Debate_AI\Quick_error.py`

```python
import subprocess
import time
import os
import sys

# Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DEBATE_SCRIPT = os.path.join(BASE_DIR, "Debate_AI.py")
LOG_FILE = os.path.join(BASE_DIR, "quick_error.log")

def monitor_debate():
    # Start log
    with open(LOG_FILE, 'w', encoding='utf-8') as log:
        log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Monitoring started for Debate_AI.py\n")

    # Launch Debate_AI.py with the same Python interpreter
    try:
        process = subprocess.Popen(
            [sys.executable, DEBATE_SCRIPT],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=BASE_DIR
        )

        # Read and log output/error
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            while True:
                out = process.stdout.readline()
                err = process.stderr.readline()

                if err:
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                    line = f"[{timestamp}] ERROR: {err.strip()}\n"
                    print(line, end='')
                    log.write(line)
                    log.flush()

                if out:
                    print(f"[OUTPUT] {out.strip()}")

                if process.poll() is not None:
                    break

            # Final exit log
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
            log.write(f"[{timestamp}] Debate_AI.py has exited with code {process.returncode}.\n")

    except Exception as exc:
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Exception: {exc}\n")

if __name__ == "__main__":
    print("Monitoring Debate_AI.py for errors...")
    monitor_debate()
    print("Debate_AI.py has stopped. Monitor exiting.")
```

**Functions:** monitor_debate()


## Module `Debate_AI_Old\debate 4\Debate_AI\schema_viewer.py`

```python
#!/usr/bin/env python3
# File: schema_viewer.py
# Provides a pop‑up window to view and edit a session’s raw schema.json

import os
import json
import tkinter as tk
from tkinter import ttk, messagebox

class SchemaViewer:
    def __init__(self, master):
        """
        :param master: the parent Tk instance
        """
        self.master = master
        self.window = None
        self.text = None
        self.current_schema_path = None

    def open(self, session_path):
        """
        Open (or raise) the schema editor window for the given session.
        :param session_path: path to the session folder
        """
        schema_path = os.path.join(session_path, "schema.json")
        self.current_schema_path = schema_path

        # Read existing JSON or start with empty dict
        try:
            with open(schema_path, "r", encoding="utf-8") as f:
                content = f.read()
        except FileNotFoundError:
            content = "{}"

        # If window already exists, just update text
        if self.window and tk.Toplevel.winfo_exists(self.window):
            self.text.delete("1.0", "end")
            self.text.insert("1.0", content)
            self.window.lift()
            return

        # Create new pop‑up
        self.window = tk.Toplevel(self.master)
        self.window.title("Edit schema.json")
        self.window.geometry("600x500")

        # Text area
        self.text = tk.Text(self.window, wrap="none")
        self.text.insert("1.0", content)
        self.text.pack(fill="both", expand=True, padx=5, pady=5)

        # Save / Cancel buttons
        btn_frame = ttk.Frame(self.window)
        btn_frame.pack(fill="x", pady=5)

        save_btn = ttk.Button(btn_frame, text="Save", command=self._save)
        save_btn.pack(side="right", padx=5)
        cancel_btn = ttk.Button(btn_frame, text="Cancel", command=self.window.destroy)
        cancel_btn.pack(side="right", padx=5)

    def _save(self):
        """Validate JSON and write back to schema.json."""
        raw = self.text.get("1.0", "end").strip()
        try:
            parsed = json.loads(raw)
        except json.JSONDecodeError as e:
            messagebox.showerror("Invalid JSON", f"Error parsing JSON:\n{e}")
            return

        try:
            with open(self.current_schema_path, "w", encoding="utf-8") as f:
                json.dump(parsed, f, indent=2)
            messagebox.showinfo("Saved", "schema.json has been updated.")
            self.window.destroy()
        except Exception as e:
            messagebox.showerror("Save Failed", f"Could not write file:\n{e}")
```

**Classes:** SchemaViewer


## Module `Debate_AI_Old\debate 4\Debate_AI\session_manager.py`

```python
#!/usr/bin/env python3
# File: session_manager.py
# Manages creation, listing, loading, and saving of debate sessions.

import os
import json
import shutil

DEFAULT_SCHEMA = {
    "category": "",
    "topic": "",
    "botA_persona": "",
    "botB_persona": "",
    "max_turns": 10,
    "use_tts": True,
    "botA_provider": "OpenAI",
    "botA_model": "gpt-4o-mini",
    "botB_provider": "OpenAI",
    "botB_model": "gpt-4o-mini"
}

DEFAULT_USER_CONFIG = {}

class SessionManager:
    def __init__(self, sessions_root):
        """
        :param sessions_root: path to the 'sessions/' directory
        """
        self.sessions_root = sessions_root
        os.makedirs(self.sessions_root, exist_ok=True)

    def list_sessions(self):
        """Return a sorted list of existing session folder names."""
        names = [
            name for name in os.listdir(self.sessions_root)
            if os.path.isdir(os.path.join(self.sessions_root, name))
        ]
        return sorted(names)

    def create_session(self, name):
        """
        Create a new session folder with default schema and config.
        Raises FileExistsError if the folder already exists.
        """
        session_path = os.path.join(self.sessions_root, name)
        if os.path.exists(session_path):
            raise FileExistsError(f"Session '{name}' already exists.")
        os.makedirs(session_path)

        # Write default schema.json
        schema_path = os.path.join(session_path, "schema.json")
        with open(schema_path, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_SCHEMA, f, indent=2)

        # Write default user_config.json
        config_path = os.path.join(session_path, "user_config.json")
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_USER_CONFIG, f, indent=2)

        # Create empty log files
        for fname in (
            "conversation.txt",
            "monologues_A.txt",
            "monologues_B.txt",
            "logs.json",
            "agreed_vector.json",
            "debated_vector.json"
        ):
            open(os.path.join(session_path, fname), "a").close()

    def load_session(self, session_path):
        """
        Load schema.json and user_config.json from the given session folder.
        Returns (schema_dict, user_config_dict).
        """
        schema_path = os.path.join(session_path, "schema.json")
        config_path = os.path.join(session_path, "user_config.json")

        # Load or fallback
        try:
            with open(schema_path, "r", encoding="utf-8") as f:
                schema = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            schema = DEFAULT_SCHEMA.copy()

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                user_config = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            user_config = DEFAULT_USER_CONFIG.copy()

        return schema, user_config

    def save_session(self, session_path, schema, user_config):
        """
        Save schema.json and user_config.json back to the session folder.
        Overwrites existing files.
        """
        schema_path = os.path.join(session_path, "schema.json")
        config_path = os.path.join(session_path, "user_config.json")

        with open(schema_path, "w", encoding="utf-8") as f:
            json.dump(schema, f, indent=2)

        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(user_config, f, indent=2)
```

**Classes:** SessionManager


## Module `Debate_AI_Old\debate 4\Debate_AI\tts.py`

```python
# tts.py

"""
Text-to-Speech Manager for Debate_AI
------------------------------------
Provides non-blocking TTS for multiple bots (e.g., "Red" and "Blue") using pyttsx3.
Supports:
 - Listing available voices
 - Assigning voices per bot
 - Adjusting rate and volume
 - Asynchronous speaking and stop control
"""

import threading
import time

try:
    import pyttsx3
except ImportError:
    raise ImportError(
        "pyttsx3 is required for TTS. Install with `pip install pyttsx3`."
    )


class TTSManager:
    def __init__(self):
        # Initialize engine
        self.engine = pyttsx3.init()
        # Available voices
        self.voices = self.engine.getProperty("voices")
        # Map bot names to voice IDs
        self.voice_map = {}
        # Lock for thread-safe engine access
        self._lock = threading.Lock()
        # Default rate and volume
        self.rate = self.engine.getProperty("rate")
        self.volume = self.engine.getProperty("volume")
        # Set default voices for bots if at least two voices exist
        self._initialize_default_voices()

    def _initialize_default_voices(self):
        """
        Assign the first voice to 'Red' and the second to 'Blue' by default.
        If fewer than 2 voices available, both use the first voice.
        """
        if len(self.voices) >= 2:
            self.voice_map["Red"] = self.voices[0].id
            self.voice_map["Blue"] = self.voices[1].id
        elif self.voices:
            self.voice_map["Red"] = self.voice_map["Blue"] = self.voices[0].id

    def list_voices(self):
        """
        Return a list of tuples (voice_id, voice_name) for all available voices.
        """
        return [(v.id, v.name) for v in self.voices]

    def set_bot_voice(self, bot_name: str, voice_id: str):
        """
        Assign a specific voice (by ID) to a bot.
        """
        if bot_name not in ("Red", "Blue"):
            raise ValueError(f"Unknown bot '{bot_name}'; valid options are 'Red' and 'Blue'.")
        if voice_id not in [v.id for v in self.voices]:
            raise ValueError(f"Voice ID '{voice_id}' not found among available voices.")
        self.voice_map[bot_name] = voice_id

    def set_rate(self, rate: int):
        """
        Set speech rate (words per minute). Applies to subsequent speaks.
        """
        with self._lock:
            self.rate = rate
            self.engine.setProperty("rate", rate)

    def set_volume(self, volume: float):
        """
        Set volume (0.0 to 1.0). Applies to subsequent speaks.
        """
        if not 0.0 <= volume <= 1.0:
            raise ValueError("Volume must be between 0.0 and 1.0.")
        with self._lock:
            self.volume = volume
            self.engine.setProperty("volume", volume)

    def speak(self, bot_name: str, text: str):
        """
        Speak the given text in a non-blocking thread, using the bot's assigned voice.
        """
        if bot_name not in self.voice_map:
            raise ValueError(f"No voice assigned for bot '{bot_name}'.")
        voice_id = self.voice_map[bot_name]

        def _speak():
            with self._lock:
                try:
                    self.engine.setProperty("voice", voice_id)
                    # rate and volume already set on engine
                    self.engine.say(text)
                    self.engine.runAndWait()
                except Exception as e:
                    # Swallow errors to avoid crashing the main app
                    print(f"[TTS Error] {e}")

        threading.Thread(target=_speak, daemon=True).start()

    def stop(self):
        """
        Stop any ongoing speech.
        """
        with self._lock:
            self.engine.stop()


# Example usage
if __name__ == "__main__":
    manager = TTSManager()

    print("Available voices:")
    for vid, name in manager.list_voices():
        print(f"  {vid}: {name}")

    # Set custom rate and volume if desired
    manager.set_rate(150)      # words per minute
    manager.set_volume(0.8)    # 80% volume

    # Assign a different voice to Blue if you like
    # e.g., manager.set_bot_voice("Blue", "<voice_id_from_list>")

    # Test speaking
    print("Red speaks:")
    manager.speak("Red", "Hello, I am the Red bot, advocating for GPU acceleration in art.")

    time.sleep(2)  # allow some overlap
    print("Blue speaks:")
    manager.speak("Blue", "Hello, I am the Blue bot, supporting efficient GPU use in gaming.")

    # Let them finish
    time.sleep(5)
```

Text-to-Speech Manager for Debate_AI
------------------------------------
Provides non-blocking TTS for multiple bots (e.g., "Red" and "Blue") using pyttsx3.
Supports:
 - Listing available voices
 - Assigning voices per bot
 - Adjusting rate and volume
 - Asynchronous speaking and stop control
**Classes:** TTSManager


## Module `Debate_AI_Old\Debate_AI\Analyze_folders.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")
        
        for root, dirs, files in os.walk(start_path):
            # Skip directories named 'venv'
            if "venv" in root.split(os.sep):
                continue

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and output file from the list of files
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file.write(f"{file_indent}|-- {f}\n")
    
    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.dirname(os.path.abspath(__file__))
    analyze_folders(script_directory)
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI\Debate_AI.py`

```python
#!/usr/bin/env python3
# File: Debate_AI.py
# Location: C:/Users/Art PC/Desktop/Debate_AI/

import os
import sys
import json
import threading
import requests
import speech_recognition as sr
from datetime import datetime

from PyQt5.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QTextEdit, QPushButton, QLabel, QComboBox,
    QStackedWidget, QLineEdit, QCheckBox, QFileDialog,
    QMessageBox, QStatusBar, QTreeWidget, QTreeWidgetItem,
    QSplitter, QSizePolicy
)
from PyQt5.QtCore import Qt, QTimer

import openai
from tts import (
    initialize_tts, set_tts_enabled,
    skip_tts, stop_tts, speak_text
)
from engines.debate_engine import DebateEngine
from engines.history_manager import append_shared, append_bot, tail
from engines.jarvis_injector import apply_queued_commands
from engines.jarvis_engine import summarize, interpret_user, load_structure

# ——— Load OpenAI key with fallback to environment ———
API_KEY_PATH = os.path.join(os.path.dirname(__file__), "api", "api_key.txt")
OPENAI_KEY = ""
if os.path.isfile(API_KEY_PATH):
    OPENAI_KEY = open(API_KEY_PATH, "r", encoding="utf-8").read().strip()
if not OPENAI_KEY:
    OPENAI_KEY = os.getenv("OPENAI_API_KEY", "").strip()

if not OPENAI_KEY:
    print("Warning: OpenAI API key not found. OpenAI provider disabled.")
    openai_client = None
else:
    openai_client = openai.OpenAI(api_key=OPENAI_KEY)

# ——— Initialize TTS manager ———
initialize_tts(voice_name="Zira", speed=1.0)

# ——— Ensure data dirs & files exist ———
BASE_DIR    = os.path.dirname(__file__)
STRUCT_PATH = os.path.join(BASE_DIR, "data", "structure.json")
HIST_PATH   = os.path.join(BASE_DIR, "data", "history.log")
SESS_DIR    = os.path.join(BASE_DIR, "data", "sessions")
BOTS_DIR    = os.path.join(BASE_DIR, "bots")

for p in (STRUCT_PATH, HIST_PATH):
    os.makedirs(os.path.dirname(p), exist_ok=True)
os.makedirs(SESS_DIR, exist_ok=True)
os.makedirs(BOTS_DIR, exist_ok=True)

if not os.path.exists(STRUCT_PATH) or os.path.getsize(STRUCT_PATH) == 0:
    json.dump(load_structure(), open(STRUCT_PATH, "w", encoding="utf-8"), indent=2)

class DebateAIApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate AI + Jarvis")
        self.setGeometry(30, 30, 1400, 900)

        self.mode = "debate"
        self.running = False
        self.engine = DebateEngine(load_structure())

        self._build_ui()
        QTimer.singleShot(120000, self._auto_summary)

    def _build_ui(self):
        main = QVBoxLayout(self)

        # Top controls
        top = QHBoxLayout()
        top.addWidget(QLabel("Provider:"))
        self.provider_cb = QComboBox(self)
        providers = ["Ollama"]
        if openai_client:
            providers.insert(0, "OpenAI")
        self.provider_cb.addItems(providers)
        self.provider_cb.currentTextChanged.connect(self._update_models)
        top.addWidget(self.provider_cb)

        top.addWidget(QLabel("Model:"))
        self.model_cb = QComboBox(self)
        # Double dropdown width so full model names are visible
        self.model_cb.setMinimumWidth(300)
        top.addWidget(self.model_cb)
        self._update_models(self.provider_cb.currentText())

        self.mode_btn = QPushButton("Switch to Jarvis", self)
        self.mode_btn.clicked.connect(self._toggle_mode)
        top.addWidget(self.mode_btn)

        top.addWidget(QLabel("Sessions:"))
        self.session_cb = QComboBox(self)
        self._refresh_sessions()
        top.addWidget(self.session_cb)

        load_btn = QPushButton("Load", self)
        load_btn.clicked.connect(self._load_session)
        save_btn = QPushButton("Save As…", self)
        save_btn.clicked.connect(self._save_session)
        top.addWidget(load_btn)
        top.addWidget(save_btn)

        top.addStretch()
        main.addLayout(top)

        # Splitter: left = logs, right = content
        splitter = QSplitter(Qt.Horizontal)

        # Left panel: David & Zira logs
        left = QWidget()
        left_l = QVBoxLayout(left)
        left_l.addWidget(QLabel("David (Blue) Log:"))
        self.david_log = QTextEdit()
        self.david_log.setReadOnly(True)
        self.david_log.setStyleSheet("background:#E3F2FD;")
        self.david_log.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        left_l.addWidget(self.david_log, 1)

        left_l.addWidget(QLabel("Zira (Red) Log:"))
        self.zira_log = QTextEdit()
        self.zira_log.setReadOnly(True)
        self.zira_log.setStyleSheet("background:#FFEBEE;")
        self.zira_log.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        left_l.addWidget(self.zira_log, 1)

        splitter.addWidget(left)
        splitter.setStretchFactor(0, 1)

        # Right panel: debate/jarvis stack
        self.stack = QStackedWidget(self)
        self.stack.addWidget(self._debate_page())
        self.stack.addWidget(self._jarvis_page())
        splitter.addWidget(self.stack)
        splitter.setStretchFactor(1, 3)

        main.addWidget(splitter, 1)

        # Status bar
        self.status = QStatusBar(self)
        main.addWidget(self.status)

    def _refresh_sessions(self):
        self.session_cb.clear()
        for fname in os.listdir(SESS_DIR):
            if fname.endswith(".json"):
                self.session_cb.addItem(fname)

    def _update_models(self, provider):
        self.model_cb.clear()
        if provider == "OpenAI" and openai_client:
            self.model_cb.addItems(["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4"])
        else:
            try:
                tags = requests.get("http://localhost:11434/api/tags", timeout=2).json()
                models = [m["name"] for m in tags.get("models", [])]
            except:
                models = ["ollama-base"]
            self.model_cb.addItems(models)

    def _debate_page(self):
        page = QWidget()
        v = QVBoxLayout(page)

        # Structure tree
        self.tree = QTreeWidget()
        self.tree.setHeaderLabels(["Structure"])
        self.tree.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self._populate_tree()
        self.tree.itemClicked.connect(self._on_tree_click)
        v.addWidget(self.tree, 2)

        # Context label
        self.ctx = QLabel("", self)
        v.addWidget(self.ctx)

        # Chat panes
        chat_h = QHBoxLayout()
        for lbl, attr, color in [
            ("David Chat:", "blue_chat", "#E0F7FA"),
            ("Zira Chat:",  "red_chat",  "#FFEBEE")
        ]:
            col = QVBoxLayout()
            col.addWidget(QLabel(lbl))
            pane = QTextEdit()
            pane.setReadOnly(True)
            pane.setStyleSheet(f"background:{color};")
            pane.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
            setattr(self, attr, pane)
            col.addWidget(pane)
            chat_h.addLayout(col, 1)
        v.addLayout(chat_h, 3)

        # Bottom controls
        ctl = QHBoxLayout()
        self.start_btn = QPushButton("Start Debate", self)
        self.start_btn.clicked.connect(self._toggle_run)
        ctl.addWidget(self.start_btn)

        self.skip_btn = QPushButton("Skip Turn", self)
        self.skip_btn.clicked.connect(self._skip)
        ctl.addWidget(self.skip_btn)

        self.stop_btn = QPushButton("Stop Voice", self)
        self.stop_btn.clicked.connect(stop_tts)
        ctl.addWidget(self.stop_btn)

        ctl.addWidget(QLabel("Blue voice:"))
        self.blue_voice = QComboBox(self)
        self.blue_voice.addItems(["David", "Zira"])
        ctl.addWidget(self.blue_voice)

        ctl.addWidget(QLabel("Red voice:"))
        self.red_voice = QComboBox(self)
        self.red_voice.addItems(["Zira", "David"])
        ctl.addWidget(self.red_voice)

        self.tts_cb = QCheckBox("Enable TTS", self)
        self.tts_cb.setChecked(True)
        self.tts_cb.stateChanged.connect(
            lambda st: set_tts_enabled(st == Qt.Checked)
        )
        ctl.addWidget(self.tts_cb)

        self.gen_btn = QPushButton("Auto-Generate Topics", self)
        self.gen_btn.clicked.connect(self._auto_generate)
        ctl.addWidget(self.gen_btn)

        ctl.addStretch()
        v.addLayout(ctl, 0)

        return page

    def _jarvis_page(self):
        page = QWidget()
        v = QVBoxLayout(page)

        v.addWidget(QLabel("Debate Summary:"))
        self.summary = QTextEdit()
        self.summary.setReadOnly(True)
        v.addWidget(self.summary, 1)

        v.addWidget(QLabel("Jarvis Conversation:"))
        self.chat_display = QTextEdit()
        self.chat_display.setReadOnly(True)
        v.addWidget(self.chat_display, 2)

        h = QHBoxLayout()
        self.user_in = QLineEdit(self)
        h.addWidget(self.user_in)

        btn_send = QPushButton("Send", self)
        btn_send.clicked.connect(self._handle_user)
        h.addWidget(btn_send)

        btn_mic = QPushButton("Speak", self)
        btn_mic.clicked.connect(self._voice_input)
        h.addWidget(btn_mic)

        h.addStretch()
        v.addLayout(h, 0)

        return page

    def _populate_tree(self):
        self.tree.clear()
        struct = load_structure()
        for tp in struct.get("talking_points", []):
            tp_item = QTreeWidgetItem(self.tree, [tp["name"]])
            for topic in tp.get("topics", []):
                t_item = QTreeWidgetItem(tp_item, [topic["name"]])
                for sub in topic.get("subtopics", []):
                    QTreeWidgetItem(t_item, [sub])
        self.tree.expandAll()

    def _on_tree_click(self, item, _):
        path = []
        cur = item
        while cur:
            path.insert(0, cur.text(0))
            cur = cur.parent()
        self.ctx.setText("<b>Selected:</b> " + " > ".join(path))

    def _toggle_mode(self):
        if self.mode == "debate":
            self.mode = "jarvis"
            self.stack.setCurrentIndex(1)
            self.mode_btn.setText("Switch to Debate")
            stop_tts()
            self._do_summary()
            self.running = False
            self.start_btn.setText("Start Debate")
        else:
            self.mode = "debate"
            self.stack.setCurrentIndex(0)
            self.mode_btn.setText("Switch to Jarvis")
        self._update_context()

    def _do_summary(self):
        text = summarize()
        self.summary.setPlainText(text)
        self.status.showMessage("Jarvis summary generated", 5000)
        append_bot("Jarvis", "SUMMARY", text)
        threading.Thread(
            target=lambda: speak_text(text, self.blue_voice.currentText(), 1.0),
            daemon=True
        ).start()

    def _auto_summary(self):
        if self.mode == "jarvis":
            self._do_summary()
        QTimer.singleShot(120000, self._auto_summary)

    def _handle_user(self):
        txt = self.user_in.text().strip()
        if not txt:
            return
        append_bot("Jarvis", "USER", txt)
        delta = interpret_user(txt)
        if delta:
            msg = f"Structure updated: {delta}"
            self.chat_display.append(f"<i>{msg}</i>")
            append_bot("Jarvis", "INFO", msg)
            self._populate_tree()
            self._do_summary()
        else:
            ans = self._answer_question(txt)
            self.chat_display.append(f"<b>Jarvis:</b> {ans}")
        self.user_in.clear()

    def _answer_question(self, question):
        struct = {}
        try:
            struct = json.load(open(STRUCT_PATH, "r", encoding="utf-8"))
        except:
            pass
        history = "".join(tail(50))
        prompt = (
            "You are Jarvis, an expert AI assistant summarizing a debate.\n\n"
            f"Structure:\n{json.dumps(struct, indent=2)}\n\n"
            f"History:\n{history}\n\n"
            f"User: {question}\n\n"
            "Answer using only the debate context."
        )
        if openai_client:
            try:
                resp = openai_client.chat.completions.create(
                    model=self.model_cb.currentText(),
                    messages=[{"role":"user","content":prompt}]
                )
                ans = resp.choices[0].message.content.strip()
            except Exception as e:
                ans = f"[Error: {e}]"
        else:
            ans = "[OpenAI unavailable]"
        append_bot("Jarvis", "RESPONSE", ans)
        threading.Thread(
            target=lambda: speak_text(ans, self.blue_voice.currentText(), 1.0),
            daemon=True
        ).start()
        return ans

    def _voice_input(self):
        r = sr.Recognizer()
        try:
            with sr.Microphone() as mic:
                audio = r.listen(mic, phrase_time_limit=5)
            txt = r.recognize_google(audio)
            self.user_in.setText(txt)
            self._handle_user()
        except:
            QMessageBox.warning(self, "Jarvis", "Speech not recognized.")

    def _toggle_run(self):
        self.running = not self.running
        self.start_btn.setText("Pause Debate" if self.running else "Start Debate")
        if self.running:
            self._run()

    def _run(self):
        if not self.running or self.mode != "debate":
            return
        prompt, speaker = self.engine.next_prompt()
        self._update_context()
        panel = self.david_log if speaker == "David" else self.zira_log
        panel.append(f"{speaker} PROMPT: {prompt}")
        threading.Thread(
            target=self._ask,
            args=(prompt, speaker),
            daemon=True
        ).start()

    def _ask(self, prompt, speaker):
        prov = self.provider_cb.currentText()
        model = self.model_cb.currentText()
        try:
            if prov == "Ollama":
                resp = requests.post(
                    "http://localhost:11434/api/generate",
                    json={"model":model,"prompt":prompt,"stream":False}
                ).json()
                text = resp.get("choices",[{"text":""}])[0]["text"].strip()
            elif openai_client:
                resp = openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role":"user","content":prompt}]
                )
                text = resp.choices[0].message.content.strip()
            else:
                text = "[No AI available]"
        except Exception as e:
            text = f"Error: {e}"

        append_shared(f"{speaker}: {text}")
        append_bot(speaker, "RESPONSE", text)
        panel = self.david_log if speaker == "David" else self.zira_log
        panel.append(f"{speaker} RESPONSE: {text}")

        if self.tts_cb.isChecked():
            voice = (
                self.blue_voice.currentText()
                if speaker == "David" else self.red_voice.currentText()
            )
            threading.Thread(
                target=lambda: speak_text(text, voice, 1.1),
                daemon=True
            ).start()

        self.engine.process_response(speaker, text)
        if self.running:
            QTimer.singleShot(1000, self._run)

    def _skip(self):
        skip_tts()
        if self.running:
            self._run()

    def _update_context(self):
        tp, topic, sub = self.engine.current_context()
        self.ctx.setText(
            f"<b>TP:</b> {tp} | <b>Topic:</b> {topic} | <b>Subtopic:</b> {sub}"
        )

    def _auto_generate(self):
        tp, topic, _ = self.engine.current_context()
        prompt = f"Suggest two new subtopics for '{topic}' under TP '{tp}'. Return JSON list."
        suggestions = []
        if openai_client:
            try:
                r = openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role":"user","content":prompt}]
                )
                suggestions = json.loads(r.choices[0].message.content)
            except:
                suggestions = []
        struct = load_structure()
        for tp_d in struct["talking_points"]:
            if tp_d["name"] == tp:
                for t_d in tp_d["topics"]:
                    if t_d["name"] == topic:
                        t_d["subtopics"].extend(suggestions)
        interpret_user("")  # force save
        self._populate_tree()
        QMessageBox.information(self, "Auto-Generate", f"Added: {suggestions}")
        self.status.showMessage("Auto-generated subtopics", 5000)

    def _save_session(self):
        fname, _ = QFileDialog.getSaveFileName(
            self, "Save Session", SESS_DIR, "JSON Files (*.json)"
        )
        if not fname:
            return
        session = {
            "structure": load_structure(),
            "history": "".join(tail(1000))
        }
        with open(fname, "w", encoding="utf-8") as f:
            json.dump(session, f, indent=2)
        self._refresh_sessions()
        self.status.showMessage("Session saved", 5000)

    def _load_session(self):
        choice = self.session_cb.currentText()
        if not choice:
            QMessageBox.warning(self, "Load Session", "No session selected")
            return
        path = os.path.join(SESS_DIR, choice)
        with open(path, "r", encoding="utf-8") as f:
            session = json.load(f)
        with open(STRUCT_PATH, "w", encoding="utf-8") as f:
            json.dump(session["structure"], f, indent=2)
        with open(HIST_PATH, "w", encoding="utf-8") as f:
            f.write(session["history"])
        self.engine = DebateEngine(load_structure())
        self._populate_tree()
        QMessageBox.information(self, "Load Session", f"Loaded {choice}")
        self.status.showMessage("Session loaded", 5000)

if __name__ == "__main__":
    app = QApplication(sys.argv)
    win = DebateAIApp()
    win.show()
    sys.exit(app.exec())
```

**Classes:** DebateAIApp


## Module `Debate_AI_Old\Debate_AI\Debate_AI_Full.py`

```python
#!/usr/bin/env python3
"""
Complete Debate AI System
A single-file implementation with GUI, dual-bot debate, Jarvis assistant, and topic generation
"""

import os
import sys
import json
import threading
import time
import subprocess
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import pyttsx3
import speech_recognition as sr
import openai
from pathlib import Path
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
    QTextEdit, QPushButton, QLabel, QComboBox, QStackedWidget,
    QLineEdit, QCheckBox, QFileDialog, QMessageBox, QStatusBar,
    QTreeWidget, QTreeWidgetItem, QDialog, QDialogButtonBox
)
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal
from PyQt5.QtGui import QFont

# ==============================================================================
# CONFIGURATION & CONSTANTS
# ==============================================================================

BASE_DIR = Path(__file__).parent
API_DIR = BASE_DIR / "api"
DATA_DIR = BASE_DIR / "data"
SESSIONS_DIR = DATA_DIR / "sessions"
BOTS_DIR = BASE_DIR / "bots"

# Ensure directories exist
for dir_path in [API_DIR, DATA_DIR, SESSIONS_DIR, BOTS_DIR]:
    dir_path.mkdir(exist_ok=True)

# File paths
API_KEY_FILE = API_DIR / "api_key.txt"
STRUCTURE_FILE = DATA_DIR / "structure.json"
HISTORY_FILE = DATA_DIR / "history.log"
DAVID_LOG = BOTS_DIR / "david_log.txt"
ZIRA_LOG = BOTS_DIR / "zira_log.txt"

# Default structure
DEFAULT_STRUCTURE = {
    "debate_style": "exploratory",
    "talking_points": [
        {
            "name": "Technology Impact",
            "topics": [
                {
                    "name": "AI in Society",
                    "subtopics": [
                        {"name": "Job displacement concerns", "status": "open"},
                        {"name": "Creative augmentation", "status": "open"},
                        {"name": "Ethical considerations", "status": "open"}
                    ]
                }
            ]
        }
    ]
}

# Load OpenAI client
try:
    with open(API_KEY_FILE, "r", encoding="utf-8") as f:
        OPENAI_KEY = f.read().strip()
    openai_client = openai.OpenAI(api_key=OPENAI_KEY)
except Exception as e:
    print(f"Error loading OpenAI API key: {e}")
    sys.exit(1)

# ==============================================================================
# TTS MANAGER
# ==============================================================================

class TTSManager:
    def __init__(self):
        self.queue = []
        self.is_speaking = False
        self._current_process = None
        self.lock = threading.Lock()

    def queue_speech(self, text: str, voice_name: str = "Zira", speed: float = 1.0):
        with self.lock:
            self.queue.append((text, voice_name, speed))
            if not self.is_speaking:
                self._process_queue()

    def _process_queue(self):
        with self.lock:
            if not self.queue or self.is_speaking:
                return
            self.is_speaking = True
            text, voice_name, speed = self.queue.pop(0)

        try:
            engine = pyttsx3.init()
            for voice in engine.getProperty("voices"):
                if voice_name.lower() in voice.name.lower():
                    engine.setProperty("voice", voice.id)
                    break
            rate = engine.getProperty("rate")
            engine.setProperty("rate", int(rate * speed))

            cmd_code = (
                "import pyttsx3\n"
                f"e = pyttsx3.init()\n"
                f"e.setProperty('voice', '{engine.getProperty('voice')}')\n"
                f"e.setProperty('rate', {int(rate * speed)})\n"
                f"e.say({repr(text)})\n"
                f"e.runAndWait()\n"
            )

            self._current_process = subprocess.Popen(
                [sys.executable, "-c", cmd_code],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )

            def monitor_process():
                self._current_process.wait()
                with self.lock:
                    self.is_speaking = False
                self._process_queue()

            threading.Thread(target=monitor_process, daemon=True).start()

        except Exception as e:
            print(f"TTS Error: {e}")
            with self.lock:
                self.is_speaking = False
            self._process_queue()

    def stop_speech(self):
        with self.lock:
            if self._current_process:
                try:
                    self._current_process.terminate()
                    self._current_process.wait(timeout=1)
                except:
                    pass
                self._current_process = None
            self.queue.clear()
            self.is_speaking = False

# ==============================================================================
# HISTORY MANAGER
# ==============================================================================

class HistoryManager:
    def __init__(self):
        self.lock = threading.Lock()

    def log_entry(self, entry: str, log_type: str = "main"):
        timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
        line = f"{timestamp} {entry}\n"
        file_path = {
            "main": HISTORY_FILE,
            "david": DAVID_LOG,
            "zira": ZIRA_LOG
        }.get(log_type, HISTORY_FILE)

        with self.lock:
            with open(file_path, "a", encoding="utf-8") as f:
                f.write(line)

    def get_log(self, log_type: str = "main", n: int = 100) -> List[str]:
        file_path = {
            "main": HISTORY_FILE,
            "david": DAVID_LOG,
            "zira": ZIRA_LOG
        }.get(log_type, HISTORY_FILE)
        if not file_path.exists():
            return []
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        return lines[-n:]

    def reset_history(self):
        with self.lock:
            for file_path in [HISTORY_FILE, DAVID_LOG, ZIRA_LOG]:
                if file_path.exists():
                    file_path.unlink()

    def archive_history(self, session_name: str):
        archive_path = SESSIONS_DIR / f"{session_name}_history.log"
        with self.lock:
            if HISTORY_FILE.exists():
                with open(HISTORY_FILE, "r", encoding="utf-8") as src, \
                     open(archive_path, "w", encoding="utf-8") as dst:
                    dst.write(src.read())
                HISTORY_FILE.unlink()

# ==============================================================================
# STRUCTURE MANAGER
# ==============================================================================

class StructureManager:
    def __init__(self):
        self.structure = self.load_structure()

    def load_structure(self) -> Dict:
        if not STRUCTURE_FILE.exists() or STRUCTURE_FILE.stat().st_size == 0:
            self.save_structure(DEFAULT_STRUCTURE)
            return DEFAULT_STRUCTURE.copy()
        try:
            with open(STRUCTURE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, Exception):
            self.save_structure(DEFAULT_STRUCTURE)
            return DEFAULT_STRUCTURE.copy()

    def save_structure(self, structure: Dict):
        with open(STRUCTURE_FILE, "w", encoding="utf-8") as f:
            json.dump(structure, f, indent=2)
        self.structure = structure

    def add_talking_point(self, name: str):
        self.structure["talking_points"].append({"name": name, "topics": []})
        self.save_structure(self.structure)

    def add_topic(self, tp_name: str, topic_name: str):
        for tp in self.structure["talking_points"]:
            if tp["name"] == tp_name:
                tp["topics"].append({"name": topic_name, "subtopics": []})
                break
        self.save_structure(self.structure)

    def add_subtopic(self, tp_name: str, topic_name: str, subtopic: str):
        for tp in self.structure["talking_points"]:
            if tp["name"] == tp_name:
                for topic in tp["topics"]:
                    if topic["name"] == topic_name:
                        topic["subtopics"].append({"name": subtopic, "status": "open"})
                        break
                break
        self.save_structure(self.structure)

    def update_subtopic_status(self, tp_name: str, topic_name: str, subtopic: str, status: str):
        for tp in self.structure["talking_points"]:
            if tp["name"] == tp_name:
                for topic in tp["topics"]:
                    if topic["name"] == topic_name:
                        for sub in topic["subtopics"]:
                            if sub["name"] == subtopic:
                                sub["status"] = status
                                break
                        break
                break
        self.save_structure(self.structure)

# ==============================================================================
# DEBATE ENGINE
# ==============================================================================

class DebateEngine:
    def __init__(self, structure_manager: StructureManager, history_manager: HistoryManager):
        self.structure_manager = structure_manager
        self.history_manager = history_manager
        self.current_tp = 0
        self.current_topic = 0
        self.current_subtopic = 0
        self.current_turn = 0  # 0 = David, 1 = Zira
        self.debate_style = structure_manager.structure.get("debate_style", "exploratory")

    def get_current_context(self) -> Tuple[str, str, str]:
        structure = self.structure_manager.structure
        if not structure["talking_points"]:
            return "No topics", "No topics", "No subtopics"
        tp = structure["talking_points"][self.current_tp]
        if not tp["topics"]:
            return tp["name"], "No topics", "No subtopics"
        topic = tp["topics"][self.current_topic]
        if not topic["subtopics"]:
            return tp["name"], topic["name"], "No subtopics"
        subtopic = topic["subtopics"][self.current_subtopic]["name"]
        return tp["name"], topic["name"], subtopic

    def next_prompt(self) -> Tuple[str, str]:
        tp_name, topic_name, subtopic = self.get_current_context()
        speaker = "David" if self.current_turn == 0 else "Zira"
        action = "pose a challenging question about" if self.current_turn == 0 else "provide a substantive response on"
        style_instruction = "Be aggressive and critical." if self.debate_style == "aggressive" else "Be thoughtful and exploratory."
        prompt = (
            f"You are {speaker}, a debate AI. {style_instruction} "
            f"{action} the subtopic '{subtopic}' in topic '{topic_name}' "
            f"of talking point '{tp_name}'. Stay strictly on this subtopic. "
            f"Do not diverge. Recent context:\n{self.get_recent_context()}"
        )
        return prompt, speaker

    def get_recent_context(self) -> str:
        return "".join(self.history_manager.get_log("main", 10))

    def advance_turn(self):
        self.current_turn = 1 - self.current_turn
        if self.current_turn == 0:
            self.check_conclusion()
            self.advance_subtopic()

    def advance_subtopic(self):
        structure = self.structure_manager.structure
        if not structure["talking_points"]:
            return
        tp = structure["talking_points"][self.current_tp]
        if not tp["topics"]:
            return
        topic = tp["topics"][self.current_topic]
        if not topic["subtopics"]:
            return
        self.current_subtopic += 1
        if self.current_subtopic >= len(topic["subtopics"]):
            self.history_manager.log_entry(f"Concluded topic: {topic['name']}")
            self.current_subtopic = 0
            self.current_topic += 1
            if self.current_topic >= len(tp["topics"]):
                self.current_topic = 0
                self.current_tp = (self.current_tp + 1) % len(structure["talking_points"])

    def check_conclusion(self):
        tp_name, topic_name, subtopic = self.get_current_context()
        recent = self.get_recent_context()
        prompt = (
            f"Based on recent dialogue:\n{recent}\n"
            f"Have David and Zira reached a conclusion or stalemate on subtopic '{subtopic}' "
            f"in topic '{topic_name}' of talking point '{tp_name}'? "
            f"Return JSON: {{'concluded': bool, 'summary': str}}"
        )
        try:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150
            )
            result = json.loads(response.choices[0].message.content)
            if result.get("concluded", False):
                self.structure_manager.update_subtopic_status(tp_name, topic_name, subtopic, "concluded")
                self.history_manager.log_entry(f"Concluded subtopic '{subtopic}': {result['summary']}")
        except Exception as e:
            self.history_manager.log_entry(f"Conclusion check error: {e}")

    def generate_response(self, prompt: str, speaker: str) -> str:
        try:
            self.history_manager.log_entry(f"Internal reasoning for {speaker}: {prompt}", speaker.lower())
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"You are {speaker}, a debate AI."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            reply = response.choices[0].message.content.strip()
            self.history_manager.log_entry(f"Response: {reply}", speaker.lower())
            return reply
        except Exception as e:
            return f"Error generating response: {str(e)}"

# ==============================================================================
# JARVIS ENGINE
# ==============================================================================

class JarvisEngine:
    def __init__(self, structure_manager: StructureManager, history_manager: HistoryManager):
        self.structure_manager = structure_manager
        self.history_manager = history_manager

    def generate_summary(self) -> str:
        recent = "".join(self.history_manager.get_log("main", 50))
        structure = json.dumps(self.structure_manager.structure, indent=2)
        prompt = (
            f"You are Jarvis, summarizing a debate between David and Zira.\n"
            f"Structure:\n{structure}\n\nRecent conversation:\n{recent}\n\n"
            f"Provide a concise summary of:\n1. Current debate topic and progress\n"
            f"2. Key arguments from both sides\n3. Conclusions reached\n4. Next steps"
        )
        try:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300
            )
            summary = response.choices[0].message.content.strip()
            self.history_manager.log_entry(f"Jarvis summary: {summary}")
            return summary
        except Exception as e:
            return f"Error generating summary: {str(e)}"

    def interpret_command(self, command: str) -> Dict:
        structure = json.dumps(self.structure_manager.structure, indent=2)
        prompt = (
            f"You are Jarvis. User command: \"{command}\"\n"
            f"Current structure:\n{structure}\n\n"
            f"Return JSON with:\n- action: 'add_tp', 'add_topic', 'add_subtopic', 'remove_tp', "
            f"'remove_topic', 'remove_subtopic', 'reorder_topic'\n"
            f"- talking_point: name (if applicable)\n- topic: name (if applicable)\n"
            f"- subtopic: name (if applicable)\n- message: explanation"
        )
        try:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200
            )
            result = json.loads(response.choices[0].message.content)
            self.execute_command(result)
            return result
        except Exception:
            return {"error": "Could not parse command"}

    def execute_command(self, command: Dict):
        action = command.get("action")
        if action == "add_tp":
            self.structure_manager.add_talking_point(command.get("talking_point", "New Topic"))
        elif action == "add_topic":
            self.structure_manager.add_topic(
                command.get("talking_point", ""),
                command.get("topic", "New Topic")
            )
        elif action == "add_subtopic":
            self.structure_manager.add_subtopic(
                command.get("talking_point", ""),
                command.get("topic", ""),
                command.get("subtopic", "New Subtopic")
            )
        elif action == "remove_tp":
            self.structure_manager.structure["talking_points"] = [
                tp for tp in self.structure_manager.structure["talking_points"]
                if tp["name"] != command.get("talking_point")
            ]
            self.structure_manager.save_structure(self.structure_manager.structure)
        elif action == "remove_topic":
            for tp in self.structure_manager.structure["talking_points"]:
                if tp["name"] == command.get("talking_point"):
                    tp["topics"] = [t for t in tp["topics"] if t["name"] != command.get("topic")]
                    break
            self.structure_manager.save_structure(self.structure_manager.structure)
        elif action == "remove_subtopic":
            for tp in self.structure_manager.structure["talking_points"]:
                if tp["name"] == command.get("talking_point"):
                    for topic in tp["topics"]:
                        if topic["name"] == command.get("topic"):
                            topic["subtopics"] = [
                                s for s in topic["subtopics"] if s["name"] != command.get("subtopic")
                            ]
                            break
                    break
            self.structure_manager.save_structure(self.structure_manager.structure)

# ==============================================================================
# TOPIC GENERATOR
# ==============================================================================

class TopicGenerator:
    def __init__(self, structure_manager: StructureManager, history_manager: HistoryManager):
        self.structure_manager = structure_manager
        self.history_manager = history_manager

    def generate_subtopics(self, tp_name: str, topic_name: str) -> List[str]:
        recent = "".join(self.history_manager.get_log("main", 30))
        prompt = (
            f"Based on the debate about '{topic_name}' in talking point '{tp_name}',\n"
            f"suggest 3 new subtopics. Recent context:\n{recent}\n"
            f"Return JSON: ['subtopic1', 'subtopic2', 'subtopic3']"
        )
        try:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            self.history_manager.log_entry(f"Topic generation error: {e}")
            return []

class TopicGenerationDialog(QDialog):
    def __init__(self, structure_manager: StructureManager, history_manager: HistoryManager, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Generate Subtopics")
        self.setGeometry(200, 200, 400, 300)
        self.structure_manager = structure_manager
        self.history_manager = history_manager
        self.topic_generator = TopicGenerator(structure_manager, history_manager)
        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout(self)
        self.tp_combo = QComboBox()
        self.topic_combo = QComboBox()
        self.suggestions = QTextEdit()
        self.suggestions.setReadOnly(True)
        generate_btn = QPushButton("Generate")
        generate_btn.clicked.connect(self.generate)
        apply_btn = QPushButton("Apply Suggestions")
        apply_btn.clicked.connect(self.apply_suggestions)
        buttons = QDialogButtonBox(QDialogButtonBox.Close)
        buttons.rejected.connect(self.reject)

        layout.addWidget(QLabel("Talking Point:"))
        layout.addWidget(self.tp_combo)
        layout.addWidget(QLabel("Topic:"))
        layout.addWidget(self.topic_combo)
        layout.addWidget(QLabel("Suggested Subtopics:"))
        layout.addWidget(self.suggestions)
        layout.addWidget(generate_btn)
        layout.addWidget(apply_btn)
        layout.addWidget(buttons)

        self.update_combos()

    def update_combos(self):
        self.tp_combo.clear()
        self.topic_combo.clear()
        structure = self.structure_manager.structure
        for tp in structure["talking_points"]:
            self.tp_combo.addItem(tp["name"])
        if structure["talking_points"]:
            for topic in structure["talking_points"][0]["topics"]:
                self.topic_combo.addItem(topic["name"])
        self.tp_combo.currentIndexChanged.connect(self.update_topics)

    def update_topics(self):
        self.topic_combo.clear()
        structure = self.structure_manager.structure
        tp_name = self.tp_combo.currentText()
        for tp in structure["talking_points"]:
            if tp["name"] == tp_name:
                for topic in tp["topics"]:
                    self.topic_combo.addItem(topic["name"])
                break

    def generate(self):
        tp_name = self.tp_combo.currentText()
        topic_name = self.topic_combo.currentText()
        suggestions = self.topic_generator.generate_subtopics(tp_name, topic_name)
        self.suggestions.setPlainText("\n".join(suggestions))

    def apply_suggestions(self):
        tp_name = self.tp_combo.currentText()
        topic_name = self.topic_combo.currentText()
        suggestions = self.suggestions.toPlainText().split("\n")
        for subtopic in suggestions:
            if subtopic.strip():
                self.structure_manager.add_subtopic(tp_name, topic_name, subtopic.strip())
        QMessageBox.information(self, "Success", "Subtopics applied")
        self.accept()

# ==============================================================================
# SESSION MANAGER
# ==============================================================================

class SessionManager:
    def __init__(self, structure_manager: StructureManager, history_manager: HistoryManager):
        self.structure_manager = structure_manager
        self.history_manager = history_manager

    def save_session(self, name: str) -> bool:
        try:
            session_data = {
                "name": name,
                "timestamp": datetime.now().isoformat(),
                "structure": self.structure_manager.structure,
                "history": self.history_manager.get_log("main", 1000),
                "david_log": self.history_manager.get_log("david", 1000),
                "zira_log": self.history_manager.get_log("zira", 1000)
            }
            session_file = SESSIONS_DIR / f"{name}.json"
            with open(session_file, "w", encoding="utf-8") as f:
                json.dump(session_data, f, indent=2)
            self.history_manager.archive_history(name)
            return True
        except Exception as e:
            print(f"Session save error: {e}")
            return False

    def load_session(self, name: str) -> bool:
        try:
            session_file = SESSIONS_DIR / f"{name}.json"
            if not session_file.exists():
                return False
            with open(session_file, "r", encoding="utf-8") as f:
                session_data = json.load(f)
            self.structure_manager.save_structure(session_data["structure"])
            self.history_manager.reset_history()
            with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                f.writelines(session_data["history"])
            with open(DAVID_LOG, "w", encoding="utf-8") as f:
                f.writelines(session_data["david_log"])
            with open(ZIRA_LOG, "w", encoding="utf-8") as f:
                f.writelines(session_data["zira_log"])
            return True
        except Exception as e:
            print(f"Session load error: {e}")
            return False

    def get_session_list(self) -> List[str]:
        try:
            return [f.stem for f in SESSIONS_DIR.glob("*.json")]
        except Exception:
            return []

# ==============================================================================
# VOICE RECOGNITION THREAD
# ==============================================================================

class VoiceRecognitionThread(QThread):
    text_recognized = pyqtSignal(str)
    error_occurred = pyqtSignal(str)

    def __init__(self):
        super().__init__()
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        self.is_listening = False

    def run(self):
        try:
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source)
            while self.is_listening:
                try:
                    with self.microphone as source:
                        audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)
                    text = self.recognizer.recognize_google(audio)
                    if text.strip():
                        self.text_recognized.emit(text)
                except sr.WaitTimeoutError:
                    continue
                except sr.UnknownValueError:
                    continue
                except sr.RequestError as e:
                    self.error_occurred.emit(f"Speech recognition error: {e}")
                    break
        except Exception as e:
            self.error_occurred.emit(f"Voice recognition thread error: {e}")

    def start_listening(self):
        self.is_listening = True
        self.start()

    def stop_listening(self):
        self.is_listening = False
        self.quit()
        self.wait()

# ==============================================================================
# MAIN GUI APPLICATION
# ==============================================================================

class DebateAIMainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate AI System v3.0")
        self.setGeometry(100, 100, 1400, 900)

        # Managers
        self.tts_manager = TTSManager()
        self.history_manager = HistoryManager()
        self.structure_manager = StructureManager()
        self.debate_engine = DebateEngine(self.structure_manager, self.history_manager)
        self.jarvis_engine = JarvisEngine(self.structure_manager, self.history_manager)
        self.session_manager = SessionManager(self.structure_manager, self.history_manager)

        # State
        self.current_mode = "debate"
        self.debate_running = False
        self.jarvis_autopilot = True
        self.auto_scroll = True
        self.voice_thread = VoiceRecognitionThread()
        self.voice_thread.text_recognized.connect(self.handle_voice_input)
        self.voice_thread.error_occurred.connect(self.show_error)

        # Initialize UI
        self.init_ui()

        # Auto-summary timer
        self.auto_summary_timer = QTimer()
        self.auto_summary_timer.timeout.connect(self.auto_summary)
        self.auto_summary_timer.start(120000)

        # Create API key file if needed
        if not API_KEY_FILE.exists():
            self.prompt_for_api_key()

    def init_ui(self):
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_layout = QVBoxLayout(central_widget)

        # Toolbar
        toolbar = self.create_toolbar()
        main_layout.addWidget(toolbar)

        # Main content (splitter for left/center/right)
        splitter = QHBoxLayout()
        main_layout.addLayout(splitter)

        # Left: Topic Tree and Detail Panel
        left_panel = QVBoxLayout()
        self.tree = QTreeWidget()
        self.tree.setHeaderLabels(["Talking Points / Topics / Subtopics", "Status"])
        self.tree.itemClicked.connect(self.show_topic_details)
        left_panel.addWidget(self.tree)

        self.detail_panel = QWidget()
        detail_layout = QVBoxLayout(self.detail_panel)
        self.detail_name = QLineEdit()
        self.detail_status = QComboBox()
        self.detail_status.addItems(["open", "concluded"])
        detail_layout.addWidget(QLabel("Name:"))
        detail_layout.addWidget(self.detail_name)
        detail_layout.addWidget(QLabel("Status:"))
        detail_layout.addWidget(self.detail_status)
        save_detail_btn = QPushButton("Save Changes")
        save_detail_btn.clicked.connect(self.save_topic_details)
        detail_layout.addWidget(save_detail_btn)
        left_panel.addWidget(self.detail_panel)
        splitter.addLayout(left_panel)

        # Center: Shared Chat and Prompt Debug
        center_panel = QVBoxLayout()
        self.context_label = QLabel()
        center_panel.addWidget(self.context_label)
        self.chat_display = QTextEdit()
        self.chat_display.setReadOnly(True)
        center_panel.addWidget(self.chat_display)
        self.prompt_debug = QTextEdit()
        self.prompt_debug.setPlaceholderText("Current prompt (editable)")
        center_panel.addWidget(self.prompt_debug)
        debug_btn = QPushButton("Send Manual Prompt")
        debug_btn.clicked.connect(self.send_manual_prompt)
        center_panel.addWidget(debug_btn)
        splitter.addLayout(center_panel)

        # Right: Bot Thought Logs
        right_panel = QVBoxLayout()
        self.david_log = QTextEdit()
        self.david_log.setReadOnly(True)
        self.zira_log = QTextEdit()
        self.zira_log.setReadOnly(True)
        right_panel.addWidget(QLabel("David's Thoughts"))
        right_panel.addWidget(self.david_log)
        right_panel.addWidget(QLabel("Zira's Thoughts"))
        right_panel.addWidget(self.zira_log)
        splitter.addLayout(right_panel)

        # Status Bar
        self.status_bar = QStatusBar()
        self.setStatusBar(self.status_bar)
        self.status_bar.showMessage("Ready")

        self.update_displays()

    def create_toolbar(self):
        toolbar = QWidget()
        layout = QHBoxLayout(toolbar)
        self.mode_button = QPushButton("Switch to Jarvis")
        self.mode_button.clicked.connect(self.toggle_mode)
        layout.addWidget(self.mode_button)

        layout.addWidget(QLabel("|"))
        layout.addWidget(QLabel("Session:"))
        self.session_combo = QComboBox()
        self.session_combo.setMinimumWidth(150)
        self.update_session_list()
        layout.addWidget(self.session_combo)
        save_session_btn = QPushButton("Save")
        save_session_btn.clicked.connect(self.save_session)
        layout.addWidget(save_session_btn)
        load_session_btn = QPushButton("Load")
        load_session_btn.clicked.connect(self.load_session)
        layout.addWidget(load_session_btn)

        layout.addWidget(QLabel("|"))
        self.start_debate_btn = QPushButton("Start Debate")
        self.start_debate_btn.clicked.connect(self.toggle_debate)
        layout.addWidget(self.start_debate_btn)
        self.skip_turn_btn = QPushButton("Skip Turn")
        self.skip_turn_btn.clicked.connect(self.skip_turn)
        layout.addWidget(self.skip_turn_btn)
        self.stop_voice_btn = QPushButton("Stop Voice")
        self.stop_voice_btn.clicked.connect(self.tts_manager.stop_speech)
        layout.addWidget(self.stop_voice_btn)

        layout.addWidget(QLabel("|"))
        self.tts_checkbox = QCheckBox("Enable TTS")
        self.tts_checkbox.setChecked(True)
        layout.addWidget(self.tts_checkbox)
        self.auto_scroll_checkbox = QCheckBox("Auto-Scroll")
        self.auto_scroll_checkbox.setChecked(True)
        layout.addWidget(self.auto_scroll_checkbox)

        layout.addWidget(QLabel("|"))
        layout.addWidget(QLabel("David Voice:"))
        self.david_voice_combo = QComboBox()
        self.david_voice_combo.addItems(["David", "Zira"])
        layout.addWidget(self.david_voice_combo)
        layout.addWidget(QLabel("Zira Voice:"))
        self.zira_voice_combo = QComboBox()
        self.zira_voice_combo.addItems(["Zira", "David"])
        layout.addWidget(self.zira_voice_combo)
        layout.addWidget(QLabel("Model:"))
        self.model_combo = QComboBox()
        self.model_combo.addItems(["gpt-4o-mini", "gpt-3.5-turbo"])
        layout.addWidget(self.model_combo)

        layout.addWidget(QLabel("|"))
        generate_btn = QPushButton("Generate Topics")
        generate_btn.clicked.connect(self.open_topic_generator)
        layout.addWidget(generate_btn)

        layout.addStretch()
        return toolbar

    def update_displays(self):
        self.update_tree()
        self.update_context()
        self.update_logs()

    def update_tree(self):
        self.tree.clear()
        structure = self.structure_manager.structure
        for tp in structure["talking_points"]:
            tp_item = QTreeWidgetItem(self.tree, [tp["name"], ""])
            for topic in tp["topics"]:
                topic_item = QTreeWidgetItem(tp_item, [topic["name"], ""])
                for subtopic in topic["subtopics"]:
                    status = subtopic.get("status", "open")
                    QTreeWidgetItem(topic_item, [subtopic["name"], status])
        self.tree.expandAll()

    def update_context(self):
        tp, topic, subtopic = self.debate_engine.get_current_context()
        status = "Running" if self.debate_running else "Paused"
        self.context_label.setText(
            f"<b>Mode:</b> {self.current_mode} | <b>Status:</b> {status} | "
            f"<b>TP:</b> {tp} | <b>Topic:</b> {topic} | <b>Subtopic:</b> {subtopic}"
        )

    def update_logs(self):
        self.david_log.setPlainText("".join(self.history_manager.get_log("david", 50)))
        self.zira_log.setPlainText("".join(self.history_manager.get_log("zira", 50)))
        if self.auto_scroll:
            self.david_log.verticalScrollBar().setValue(self.david_log.verticalScrollBar().maximum())
            self.zira_log.verticalScrollBar().setValue(self.zira_log.verticalScrollBar().maximum())

    def show_topic_details(self, item: QTreeWidgetItem, column: int):
        path = []
        current = item
        while current:
            path.append(current.text(0))
            current = current.parent()
        path.reverse()
        if len(path) == 1:  # Talking Point
            self.detail_name.setText(path[0])
            self.detail_status.setCurrentText("")
        elif len(path) == 2:  # Topic
            self.detail_name.setText(path[1])
            self.detail_status.setCurrentText("")
        elif len(path) == 3:  # Subtopic
            self.detail_name.setText(path[2])
            structure = self.structure_manager.structure
            for tp in structure["talking_points"]:
                if tp["name"] == path[0]:
                    for topic in tp["topics"]:
                        if topic["name"] == path[1]:
                            for sub in topic["subtopics"]:
                                if sub["name"] == path[2]:
                                    self.detail_status.setCurrentText(sub.get("status", "open"))
                                    break
                            break
                    break
        self.detail_panel.setProperty("path", path)

    def save_topic_details(self):
        path = self.detail_panel.property("path")
        if not path:
            return
        name = self.detail_name.text().strip()
        status = self.detail_status.currentText()
        structure = self.structure_manager.structure
        if len(path) == 3:  # Subtopic
            for tp in structure["talking_points"]:
                if tp["name"] == path[0]:
                    for topic in tp["topics"]:
                        if topic["name"] == path[1]:
                            for sub in topic["subtopics"]:
                                if sub["name"] == path[2]:
                                    sub["name"] = name
                                    sub["status"] = status
                                    break
                            break
                    break
            self.structure_manager.save_structure(structure)
        self.update_tree()

    def toggle_mode(self):
        try:
            self.structure_manager.load_structure()  # Ensure structure is loaded
            if self.current_mode == "debate":
                self.current_mode = "jarvis"
                self.mode_button.setText("Switch to Debate")
                self.stack.setCurrentIndex(1)
                self.debate_running = False
                self.start_debate_btn.setText("Start Debate")
                self.auto_summary()
            else:
                self.current_mode = "debate"
                self.mode_button.setText("Switch to Jarvis")
                self.stack.setCurrentIndex(0)
            self.update_context()
        except Exception as e:
            self.show_error(f"Mode toggle error: {e}")

    def toggle_debate(self):
        self.debate_running = not self.debate_running
        self.start_debate_btn.setText("Pause Debate" if self.debate_running else "Start Debate")
        if self.debate_running and self.current_mode == "debate":
            self.run_debate()

    def run_debate(self):
        if not self.debate_running or self.current_mode != "debate":
            return
        prompt, speaker = self.debate_engine.next_prompt()
        self.prompt_debug.setPlainText(prompt)
        self.update_context()
        self.chat_display.append(f"<i>{speaker} is thinking...</i>")
        threading.Thread(target=self.process_debate_turn, args=(prompt, speaker), daemon=True).start()

    def process_debate_turn(self, prompt: str, speaker: str):
        response = self.debate_engine.generate_response(prompt, speaker)
        self.display_response(speaker, response)
        self.debate_engine.advance_turn()
        if self.debate_running:
            QTimer.singleShot(1000, self.run_debate)

    def display_response(self, speaker: str, text: str):
        lines = self.chat_display.toPlainText().splitlines()[:-1]
        self.chat_display.setPlainText("\n".join(lines))
        self.chat_display.append(f"<b>{speaker}:</b> {text}")
        if self.auto_scroll:
            self.chat_display.verticalScrollBar().setValue(self.chat_display.verticalScrollBar().maximum())
        self.update_logs()
        if self.tts_checkbox.isChecked():
            voice = self.david_voice_combo.currentText() if speaker == "David" else self.zira_voice_combo.currentText()
            self.tts_manager.queue_speech(text, voice, 1.1)

    def skip_turn(self):
        self.tts_manager.stop_speech()
        if self.debate_running:
            self.run_debate()

    def auto_summary(self):
        if self.current_mode == "jarvis" and self.jarvis_autopilot:
            summary = self.jarvis_engine.generate_summary()
            self.summary_display.setPlainText(summary)
            self.status_bar.showMessage("Jarvis summary generated")
            if self.tts_checkbox.isChecked():
                self.tts_manager.queue_speech(summary, "David", 1.0)

    def handle_voice_input(self, text: str):
        self.user_input.setText(text)
        self.process_user_command()

    def process_user_command(self):
        text = self.user_input.text().strip()
        if not text:
            return
        result = self.jarvis_engine.interpret_command(text)
        if "error" in result:
            QMessageBox.warning(self, "Jarvis", result["error"])
        else:
            QMessageBox.information(self, "Jarvis", result.get("message", "Command applied"))
        self.user_input.clear()
        self.update_tree()

    def send_manual_prompt(self):
        prompt = self.prompt_debug.toPlainText().strip()
        if not prompt:
            return
        speaker = "David" if self.debate_engine.current_turn == 0 else "Zira"
        self.chat_display.append(f"<i>{speaker} is thinking...</i>")
        threading.Thread(target=self.process_debate_turn, args=(prompt, speaker), daemon=True).start()

    def update_session_list(self):
        self.session_combo.clear()
        sessions = self.session_manager.get_session_list()
        self.session_combo.addItems(sessions)

    def save_session(self):
        fname, _ = QFileDialog.getSaveFileName(self, "Save Session", "", "JSON Files (*.json)")
        if fname:
            name = Path(fname).stem
            if self.session_manager.save_session(name):
                self.update_session_list()
                self.status_bar.showMessage("Session saved")

    def load_session(self):
        name = self.session_combo.currentText()
        if name and self.session_manager.load_session(name):
            self.update_displays()
            self.status_bar.showMessage("Session loaded")
        else:
            QMessageBox.warning(self, "Error", "Failed to load session")

    def open_topic_generator(self):
        dialog = TopicGenerationDialog(self.structure_manager, self.history_manager, self)
        dialog.exec_()
        self.update_tree()

    def prompt_for_api_key(self):
        api_key, ok = QLineEdit.getText(self, "API Key", "Enter OpenAI API Key:")
        if ok and api_key:
            with open(API_KEY_FILE, "w", encoding="utf-8") as f:
                f.write(api_key)
        else:
            sys.exit(1)

    def show_error(self, message: str):
        QMessageBox.warning(self, "Error", message)

    def create_jarvis_page(self):
        page = QWidget()
        layout = QVBoxLayout(page)
        self.summary_display = QTextEdit()
        self.summary_display.setReadOnly(True)
        layout.addWidget(self.summary_display)
        input_layout = QHBoxLayout()
        self.user_input = QLineEdit()
        input_layout.addWidget(self.user_input)
        send_btn = QPushButton("Send")
        send_btn.clicked.connect(self.process_user_command)
        input_layout.addWidget(send_btn)
        voice_btn = QPushButton("Speak")
        voice_btn.clicked.connect(self.start_voice_input)
        input_layout.addWidget(voice_btn)
        layout.addLayout(input_layout)
        return page

    def start_voice_input(self):
        if not self.voice_thread.is_listening:
            self.voice_thread.start_listening()
            self.status_bar.showMessage("Listening for voice input...")
        else:
            self.voice_thread.stop_listening()
            self.status_bar.showMessage("Stopped voice input")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = DebateAIMainWindow()
    window.show()
    sys.exit(app.exec_())
```

Complete Debate AI System
A single-file implementation with GUI, dual-bot debate, Jarvis assistant, and topic generation
**Classes:** TTSManager, HistoryManager, StructureManager, DebateEngine, JarvisEngine, TopicGenerator, TopicGenerationDialog, SessionManager, VoiceRecognitionThread, DebateAIMainWindow


## Module `Debate_AI_Old\Debate_AI\Jarvis_Driver.py`

```python
#!/usr/bin/env python3
"""
File: Jarvis_Driver.py
Location: C:\Users\Art PC\Desktop\Debate_AI\

Standalone Jarvis interface:
  - Observes ongoing Debate_AI history
  - Periodically summarizes and speaks findings
  - Accepts user questions via text or voice
  - Applies structure changes or answers questions
  - Logs all interactions in bots/jarvis_log.txt
  - Does NOT hide or disable the main Debate_AI GUI; simply mutes its TTS
"""

import os
import sys
import json
import threading
import requests
import speech_recognition as sr
from datetime import datetime

from PyQt6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QLabel, QTextEdit, QLineEdit, QPushButton, QComboBox,
    QStatusBar
)
from PyQt6.QtCore import QTimer

import openai
from tts import speak_text, stop_tts
from engines.jarvis_engine import summarize as jarvis_summarize, interpret_user
from engines.history_manager import tail

# === Paths & Setup ===
BASE_DIR     = os.path.dirname(os.path.abspath(__file__))
API_KEY_PATH = os.path.join(BASE_DIR, "api", "api_key.txt")
STRUCT_PATH  = os.path.join(BASE_DIR, "data", "structure.json")
HIST_PATH    = os.path.join(BASE_DIR, "data", "history.log")
JARVIS_LOG   = os.path.join(BASE_DIR, "bots", "jarvis_log.txt")

# Ensure directories exist
os.makedirs(os.path.dirname(STRUCT_PATH), exist_ok=True)
os.makedirs(os.path.dirname(HIST_PATH), exist_ok=True)
os.makedirs(os.path.dirname(JARVIS_LOG), exist_ok=True)

# Load OpenAI key
with open(API_KEY_PATH, "r", encoding="utf-8") as f:
    OPENAI_KEY = f.read().strip()
openai_client = openai.OpenAI(api_key=OPENAI_KEY)

# Mute main Debate_AI TTS
stop_tts()

class JarvisDriver(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Jarvis – Debate Assistant")
        self.setGeometry(300, 200, 600, 700)

        # Build UI
        self._build_ui()

        # Initial summary
        self._do_summary()

        # Periodic auto-summary every 2 minutes
        QTimer.singleShot(120000, self._auto_summary)

    def _build_ui(self):
        layout = QVBoxLayout(self)

        # Voice selector for Jarvis TTS
        hl = QHBoxLayout()
        hl.addWidget(QLabel("Jarvis Voice:"))
        self.voice_cb = QComboBox(self)
        self.voice_cb.addItems(["Zira", "David"])
        hl.addWidget(self.voice_cb)
        hl.addStretch()
        layout.addLayout(hl)

        # Summary display
        layout.addWidget(QLabel("Debate Summary:"))
        self.summary_display = QTextEdit(self)
        self.summary_display.setReadOnly(True)
        layout.addWidget(self.summary_display)

        # Q&A display
        layout.addWidget(QLabel("Jarvis Conversation:"))
        self.chat_display = QTextEdit(self)
        self.chat_display.setReadOnly(True)
        layout.addWidget(self.chat_display)

        # User input & controls
        inp_layout = QHBoxLayout()
        self.user_input = QLineEdit(self)
        self.user_input.setPlaceholderText("Ask Jarvis or give structure command…")
        inp_layout.addWidget(self.user_input)

        send_btn = QPushButton("Send", self)
        send_btn.clicked.connect(self._handle_user)
        inp_layout.addWidget(send_btn)

        mic_btn = QPushButton("Speak", self)
        mic_btn.clicked.connect(self._voice_input)
        inp_layout.addWidget(mic_btn)

        layout.addLayout(inp_layout)

        # Status bar
        self.status = QStatusBar(self)
        layout.addWidget(self.status)

    def _log_jarvis(self, role, text):
        """Append timestamped entry to jarvis_log.txt."""
        ts = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
        with open(JARVIS_LOG, "a", encoding="utf-8") as f:
            f.write(f"{ts} {role}: {text}\n\n")

    def _do_summary(self):
        """Generate and display/speak a debate summary."""
        summary = jarvis_summarize()
        # Display
        self.summary_display.setPlainText(summary)
        self.status.showMessage("Jarvis summary generated", 5000)
        # Log
        self._log_jarvis("JARVIS_SUMMARY", summary)
        # Speak
        voice = self.voice_cb.currentText()
        threading.Thread(target=speak_text, args=(summary, voice, 1.0), daemon=True).start()

    def _auto_summary(self):
        """Timer callback for auto-summary."""
        self._do_summary()
        QTimer.singleShot(120000, self._auto_summary)

    def _handle_user(self):
        """Process user input: structure command or question."""
        text = self.user_input.text().strip()
        if not text:
            return

        # Log user query
        self._log_jarvis("USER", text)

        # First attempt to interpret as structure command
        delta = interpret_user(text)
        if delta:
            # Structure updated
            msg = f"Structure updated: {delta}"
            self.chat_display.append(f"<i>{msg}</i>")
            self.status.showMessage("Structure modified", 5000)
            self._log_jarvis("JARVIS", msg)
            # Immediately refresh summary
            self._do_summary()
        else:
            # Otherwise, treat as question about debate
            answer = self._answer_question(text)
            self.chat_display.append(f"<b>Jarvis:</b> {answer}")
            self._log_jarvis("JARVIS", answer)

        self.user_input.clear()

    def _answer_question(self, question):
        """Generate an answer using debate context + history + user question."""
        # Load structure
        try:
            struct = json.load(open(STRUCT_PATH, "r", encoding="utf-8"))
        except Exception:
            struct = {}

        # Get recent history
        history = "".join(tail(50))

        # Build prompt
        prompt = (
            "You are Jarvis, an expert AI assistant summarizing and explaining\n"
            "an ongoing debate between two agents (David & Zira).\n\n"
            f"Structure:\n{json.dumps(struct, indent=2)}\n\n"
            f"Recent dialogue:\n{history}\n\n"
            f"User asks: {question}\n\n"
            "Provide a clear, concise answer based strictly on the debate context."
        )

        # Query OpenAI
        try:
            resp = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            answer = resp.choices[0].message.content.strip()
        except Exception as e:
            answer = f"[Error generating response: {e}]"

        # Speak answer
        voice = self.voice_cb.currentText()
        threading.Thread(target=speak_text, args=(answer, voice, 1.0), daemon=True).start()

        return answer

    def _voice_input(self):
        """Capture microphone input and treat it as user input."""
        recognizer = sr.Recognizer()
        try:
            with sr.Microphone() as mic:
                audio = recognizer.listen(mic, phrase_time_limit=5)
            text = recognizer.recognize_google(audio)
            self.user_input.setText(text)
            self._handle_user()
        except Exception:
            QMessageBox.warning(self, "Jarvis", "Speech not recognized.")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    win = JarvisDriver()
    win.show()
    sys.exit(app.exec())
```



## Module `Debate_AI_Old\Debate_AI\monitor_debate.py`

```python
import subprocess
import time
import os
import sys

# Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DEBATE_SCRIPT = os.path.join(BASE_DIR, "Debate_AI.py")
LOG_FILE = os.path.join(BASE_DIR, "quick_error.log")

def monitor_debate():
    # Start log
    with open(LOG_FILE, 'w', encoding='utf-8') as log:
        log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Monitoring started for Debate_AI.py\n")

    # Launch Debate_AI.py with the same Python interpreter
    try:
        process = subprocess.Popen(
            [sys.executable, DEBATE_SCRIPT],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=BASE_DIR
        )

        # Read and log output/error
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            while True:
                out = process.stdout.readline()
                err = process.stderr.readline()

                if err:
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                    line = f"[{timestamp}] ERROR: {err.strip()}\n"
                    print(line, end='')
                    log.write(line)
                    log.flush()

                if out:
                    print(f"[OUTPUT] {out.strip()}")

                if process.poll() is not None:
                    break

            # Final exit log
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
            log.write(f"[{timestamp}] Debate_AI.py has exited with code {process.returncode}.\n")

    except Exception as exc:
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Exception: {exc}\n")

if __name__ == "__main__":
    print("Monitoring Debate_AI.py for errors...")
    monitor_debate()
    print("Debate_AI.py has stopped. Monitor exiting.")
```

**Functions:** monitor_debate()


## Module `Debate_AI_Old\Debate_AI\tts.py`

```python
#!/usr/bin/env python3
"""
File: tts.py

Advanced TTS Manager for Debate AI and Jarvis modes:

  - Monitors multiple bot logs (David, Zira, Jarvis) in real time.
  - Queues every new line for speech; no responses skipped.
  - Provides skip and stop controls.
  - Supports enable/disable via a checkbox in the GUI.
  - Uses pyttsx3 for offline TTS.
  - Exposes speak_text() for on-demand utterances.
"""

import os
import threading
import time
import queue
import pyttsx3

# === Configuration ===
BASE_DIR    = os.path.dirname(os.path.abspath(__file__))
BOTS_DIR    = os.path.join(BASE_DIR, "bots")
LOG_FILES   = [
    os.path.join(BOTS_DIR, "david_log.txt"),
    os.path.join(BOTS_DIR, "zira_log.txt"),
    os.path.join(BOTS_DIR, "jarvis_log.txt")
]
POLL_INTERVAL = 1.0  # seconds between file checks

class TTSManager:
    def __init__(self, log_paths, voice_name="Zira", speed=1.0):
        """
        :param log_paths: List of file paths to monitor for new lines.
        :param voice_name: Default voice for pyttsx3.
        :param speed: Rate multiplier (1.0 = normal speed).
        """
        self.log_paths = log_paths
        self.voice_name = voice_name
        self.speed = speed

        # File offsets to track read position
        self.offsets = {}
        for path in self.log_paths:
            try:
                with open(path, "rb") as f:
                    f.seek(0, os.SEEK_END)
                    self.offsets[path] = f.tell()
            except FileNotFoundError:
                self.offsets[path] = 0

        # Thread-safe queue of utterances
        self.queue = queue.Queue()

        # TTS engine setup
        self.engine = pyttsx3.init()
        self._configure_engine()

        # Control flags
        self.enabled = True
        self._running = True

        # Start background threads
        self._file_thread = threading.Thread(target=self._file_watcher, daemon=True)
        self._speech_thread = threading.Thread(target=self._speech_loop, daemon=True)
        self._file_thread.start()
        self._speech_thread.start()

    def _configure_engine(self):
        """Configure voice and rate on the pyttsx3 engine."""
        base_rate = self.engine.getProperty("rate")
        self.engine.setProperty("rate", int(base_rate * self.speed))
        for v in self.engine.getProperty("voices"):
            if self.voice_name.lower() in v.name.lower():
                self.engine.setProperty("voice", v.id)
                break

    def set_voice(self, voice_name: str):
        """Change the TTS voice dynamically."""
        self.voice_name = voice_name
        self._configure_engine()

    def set_speed(self, speed: float):
        """Change speech speed multiplier."""
        self.speed = speed
        self._configure_engine()

    def set_enabled(self, enabled: bool):
        """Enable or disable speaking (checkbox in UI)."""
        self.enabled = enabled

    def skip(self):
        """Stop current utterance and proceed to next in queue."""
        self.engine.stop()

    def stop(self):
        """Stop TTS manager threads and engine."""
        self._running = False
        self.engine.stop()

    def _file_watcher(self):
        """Background thread: poll log files for new lines and queue them."""
        while self._running:
            for path in self.log_paths:
                try:
                    with open(path, "r", encoding="utf-8", errors="ignore") as f:
                        f.seek(self.offsets[path])
                        lines = f.readlines()
                        self.offsets[path] = f.tell()
                except FileNotFoundError:
                    lines = []
                for line in lines:
                    text = line.strip()
                    if text:
                        self.queue.put(text)
            time.sleep(POLL_INTERVAL)

    def _speech_loop(self):
        """Background thread: read queued utterances one by one."""
        while self._running:
            try:
                utterance = self.queue.get(timeout=0.5)
            except queue.Empty:
                continue

            if not self.enabled:
                continue

            self.engine.say(utterance)
            self.engine.runAndWait()

# === Module-level manager ===
_manager: TTSManager = None

def initialize_tts(voice_name="Zira", speed=1.0, log_paths=None):
    """
    Create the global TTS manager. Call once from main GUI.
    :param voice_name: Default voice.
    :param speed: Speech rate multiplier.
    :param log_paths: Optional list of log file paths to monitor.
    """
    global _manager
    if _manager is None:
        paths = log_paths if log_paths is not None else LOG_FILES
        _manager = TTSManager(paths, voice_name, speed)

def set_tts_enabled(enabled: bool):
    """Toggle TTS on/off from GUI checkbox."""
    if _manager:
        _manager.set_enabled(enabled)

def skip_tts():
    """Skip current utterance and move to the next."""
    if _manager:
        _manager.skip()

def stop_tts():
    """Stop all TTS activity and background threads."""
    if _manager:
        _manager.stop()

def speak_text(text: str, voice_name: str = None, speed: float = None):
    """
    On-demand TTS: optionally update voice/speed, then enqueue `text`.
    :param text: The text to speak.
    :param voice_name: If provided, switch manager to this voice.
    :param speed: If provided, switch manager to this speed.
    """
    if _manager:
        if voice_name:
            _manager.set_voice(voice_name)
        if speed:
            _manager.set_speed(speed)
        _manager.queue.put(text)
```

File: tts.py

Advanced TTS Manager for Debate AI and Jarvis modes:

  - Monitors multiple bot logs (David, Zira, Jarvis) in real time.
  - Queues every new line for speech; no responses skipped.
  - Provides skip and stop controls.
  - Supports enable/disable via a checkbox in the GUI.
  - Uses pyttsx3 for offline TTS.
  - Exposes speak_text() for on-demand utterances.
**Classes:** TTSManager
**Functions:** initialize_tts(voice_name, speed, log_paths), set_tts_enabled(enabled), skip_tts(), stop_tts(), speak_text(text, voice_name, speed)


## Module `Debate_AI_Old\Debate_AI\engines\Analyze_folders.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")
        
        for root, dirs, files in os.walk(start_path):
            # Skip directories named 'venv'
            if "venv" in root.split(os.sep):
                continue

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and output file from the list of files
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file.write(f"{file_indent}|-- {f}\n")
    
    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.dirname(os.path.abspath(__file__))
    analyze_folders(script_directory)
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI\engines\config_panel_ui.py`

```python
#!/usr/bin/env python3
"""
config_panel_ui.py

Debate Parameters Panel:

  - Max Tokens (QSpinBox)
  - Temperature (QDoubleSpinBox)
  - Turn Timeout (seconds) (QSpinBox)
  - Auto-Summary Interval (minutes) (QSpinBox)
  - Debate Rule Set (QComboBox)
  - Apply Settings button

Emits `settings_changed(dict)` when Apply is clicked.
"""

import os
from PyQt5.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout,
    QLabel, QSpinBox, QDoubleSpinBox,
    QComboBox, QPushButton, QSizePolicy
)
from PyQt5.QtCore import pyqtSignal


class ConfigPanel(QWidget):
    # Emitted with a dict of all settings when the user clicks "Apply Settings"
    settings_changed = pyqtSignal(dict)

    def __init__(self, parent=None):
        super().__init__(parent)
        self._init_ui()

    def _init_ui(self):
        layout = QVBoxLayout(self)
        layout.setContentsMargins(8, 8, 8, 8)
        layout.setSpacing(10)

        # Max Tokens
        max_layout = QHBoxLayout()
        max_layout.addWidget(QLabel("Max Tokens:", self))
        self.max_tokens_spin = QSpinBox(self)
        self.max_tokens_spin.setRange(1, 100000)
        self.max_tokens_spin.setValue(2048)
        self.max_tokens_spin.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Fixed)
        max_layout.addWidget(self.max_tokens_spin)
        layout.addLayout(max_layout)

        # Temperature
        temp_layout = QHBoxLayout()
        temp_layout.addWidget(QLabel("Temperature:", self))
        self.temp_spin = QDoubleSpinBox(self)
        self.temp_spin.setRange(0.0, 2.0)
        self.temp_spin.setSingleStep(0.05)
        self.temp_spin.setValue(0.7)
        self.temp_spin.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Fixed)
        temp_layout.addWidget(self.temp_spin)
        layout.addLayout(temp_layout)

        # Turn Timeout
        timeout_layout = QHBoxLayout()
        timeout_layout.addWidget(QLabel("Turn Timeout (s):", self))
        self.timeout_spin = QSpinBox(self)
        self.timeout_spin.setRange(1, 300)
        self.timeout_spin.setValue(60)
        self.timeout_spin.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Fixed)
        timeout_layout.addWidget(self.timeout_spin)
        layout.addLayout(timeout_layout)

        # Auto-summary Interval
        summary_layout = QHBoxLayout()
        summary_layout.addWidget(QLabel("Auto-Summary Interval (min):", self))
        self.summary_spin = QSpinBox(self)
        self.summary_spin.setRange(1, 1440)
        self.summary_spin.setValue(2)
        self.summary_spin.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Fixed)
        summary_layout.addWidget(self.summary_spin)
        layout.addLayout(summary_layout)

        # Debate Rule Set
        rules_layout = QHBoxLayout()
        rules_layout.addWidget(QLabel("Debate Rule Set:", self))
        self.rules_combo = QComboBox(self)
        self.rules_combo.addItems(["Default", "Strict", "Open"])
        self.rules_combo.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)
        rules_layout.addWidget(self.rules_combo)
        layout.addLayout(rules_layout)

        # Apply Button
        self.apply_btn = QPushButton("Apply Settings", self)
        self.apply_btn.clicked.connect(self._on_apply)
        layout.addWidget(self.apply_btn)

        layout.addStretch()

    def _on_apply(self):
        """
        Gather all settings into a dict and emit `settings_changed`.
        """
        config = {
            "max_tokens": self.max_tokens_spin.value(),
            "temperature": self.temp_spin.value(),
            "turn_timeout": self.timeout_spin.value(),
            "auto_summary_interval": self.summary_spin.value(),
            "rule_set": self.rules_combo.currentText()
        }
        self.settings_changed.emit(config)
```

config_panel_ui.py

Debate Parameters Panel:

  - Max Tokens (QSpinBox)
  - Temperature (QDoubleSpinBox)
  - Turn Timeout (seconds) (QSpinBox)
  - Auto-Summary Interval (minutes) (QSpinBox)
  - Debate Rule Set (QComboBox)
  - Apply Settings button

Emits `settings_changed(dict)` when Apply is clicked.
**Classes:** ConfigPanel


## Module `Debate_AI_Old\Debate_AI\engines\debate_engine.py`

```python
#!/usr/bin/env python3
"""
File: engines/debate_engine.py

Manages the turn‐based debate between two AI agents (David & Zira), integrating:
  - Structure navigation (Talking Point → Topic → Subtopic)
  - Prompt composition via PromptRouter
  - Rule enforcement via DebateRules
  - Jarvis command injection
  - Logging to shared history and per‐bot internal logs
"""

import os
import json
from datetime import datetime
from typing import Tuple, List

import openai

from .history_manager import append_shared, append_bot, tail
from .prompt_router import PromptRouter
from .debate_rules import DebateRules
from .jarvis_injector import apply_queued_commands

# === Load API key for direct calls if needed ===
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
API_KEY_PATH = os.path.join(BASE_DIR, "api", "api_key.txt")
with open(API_KEY_PATH, "r", encoding="utf-8") as f:
    OPENAI_KEY = f.read().strip()
openai_client = openai.OpenAI(api_key=OPENAI_KEY)

# === DebateEngine ===

class DebateEngine:
    """
    Controls debate state, composes prompts, applies rules, and logs activity.
    """

    def __init__(self, structure: dict):
        """
        :param structure: dict loaded from data/structure.json
        """
        self.structure = structure
        # Indices into structure lists
        self.tp_idx = 0
        self.topic_idx = 0
        self.sub_idx = 0
        self.turn = 0  # 0 → David asks, 1 → Zira responds

        # Helpers
        self.prompt_router = PromptRouter()
        self.rules = DebateRules()

        # Immediately apply any queued Jarvis commands
        apply_queued_commands(self)

    def current_context(self) -> Tuple[str, str, str]:
        """
        Returns the names of the current (talking_point, topic, subtopic).
        """
        tp = self.structure["talking_points"][self.tp_idx]
        topic = tp["topics"][self.topic_idx]
        sub = topic["subtopics"][self.sub_idx]
        return tp["name"], topic["name"], sub

    def next_prompt(self) -> Tuple[str, str]:
        """
        Advance debate through these steps:
          1. Apply any pending Jarvis commands
          2. Determine current speaker ("David" or "Zira")
          3. Compose prompt via PromptRouter, including history snippet & rules
          4. Log internal monologue to bot-specific log
          5. Return (prompt_text, speaker_label)
        """
        # 1. Inject Jarvis commands if any
        apply_queued_commands(self)

        # 2. Determine speaker and action
        speaker = "David" if self.turn == 0 else "Zira"

        # 3. Compose the raw prompt
        tp_name, topic_name, sub_name = self.current_context()
        # Extract recent shared history for context
        recent = tail(10)
        # Fetch rule instructions for this depth
        rule_info = self.rules.get_rule_for_depth("subtopic")
        prompt = self.prompt_router.compose_prompt(
            bot_name=speaker,
            talking_point=tp_name,
            topic=topic_name,
            subtopic=sub_name,
            history=recent,
            style=rule_info["mode"],
            rules={"depth": "subtopic", "behavior": rule_info["mode"]}
        )

        # 4. Log bot internal monologue (prompt)
        self.log_bot(speaker, "PROMPT", prompt)

        # 5. Flip turn so next call is the other bot
        self.turn ^= 1
        return prompt, speaker

    def process_response(self, speaker: str, response: str):
        """
        After receiving `response` from the LLM, do:
          1. Log to shared history
          2. Log to bot internal log
          3. Enforce rules (may advance indices)
          4. Advance indices if both have spoken in this pair
        """
        # 1. Shared history
        shared_entry = f"{speaker}: {response}"
        append_shared(shared_entry)

        # 2. Bot internal log
        self.log_bot(speaker, "RESPONSE", response)

        # 3. Apply rules to decide if we conclude subtopic/topic
        history = tail(20)
        self.rules.apply_rules(self, history)

        # 4. After Zira responds (turn flipped back to David), advance subtopic
        if self.turn == 0:
            self._advance_indices()

    def _advance_indices(self):
        """
        Advance to next subtopic/topic/talking point when a debate
        pair completes. Wraps around and logs conclusions.
        """
        tp = self.structure["talking_points"][self.tp_idx]
        topics = tp["topics"]
        subtopics = topics[self.topic_idx]["subtopics"]

        self.sub_idx += 1
        # If out of subtopics, wrap to next topic
        if self.sub_idx >= len(subtopics):
            append_shared(f"🔹 Completed all subtopics under topic '{topics[self.topic_idx]['name']}'")
            self.sub_idx = 0
            self.topic_idx += 1

            # If out of topics, wrap to next talking point
            if self.topic_idx >= len(topics):
                append_shared(f"🔷 Completed talking point '{tp['name']}'")
                self.topic_idx = 0
                self.tp_idx = (self.tp_idx + 1) % len(self.structure["talking_points"])

    def advance_topic(self):
        """
        Force-advance at the topic level, resetting subtopic.
        """
        tp = self.structure["talking_points"][self.tp_idx]
        topics = tp["topics"]
        self.topic_idx = (self.topic_idx + 1) % len(topics)
        self.sub_idx = 0
        append_shared(f"⏭️ Skipped to next topic: '{topics[self.topic_idx]['name']}'")

    def advance_talking_point(self):
        """
        Force-advance at the talking-point level.
        """
        self.tp_idx = (self.tp_idx + 1) % len(self.structure["talking_points"])
        self.topic_idx = 0
        self.sub_idx = 0
        tp = self.structure["talking_points"][self.tp_idx]["name"]
        append_shared(f"⏭️ Jumped to talking point: '{tp}'")

    def log_bot(self, bot_name: str, role: str, text: str):
        """
        Log internal monologue or response to the per-bot file.
        """
        append_bot(bot_name, role, text)

    def log_shared(self, message: str):
        """
        Convenience for appending to global shared history.
        """
        append_shared(message)
```

File: engines/debate_engine.py

Manages the turn‐based debate between two AI agents (David & Zira), integrating:
  - Structure navigation (Talking Point → Topic → Subtopic)
  - Prompt composition via PromptRouter
  - Rule enforcement via DebateRules
  - Jarvis command injection
  - Logging to shared history and per‐bot internal logs
**Classes:** DebateEngine


## Module `Debate_AI_Old\Debate_AI\engines\debate_rules.py`

```python
#!/usr/bin/env python3
"""
File: engines/debate_rules.py

Implements the rules engine that governs how the two debate agents
(David and Zira) behave at each level of the debate hierarchy:
  - Subtopic: seek consensus or conclude after a fixed number of turns
  - Topic: argue/debate with higher tolerance
  - Talking Point: explore ideas broadly

This module provides:
  * DebateRules class with configurable thresholds
  * Functions to determine current “mode” and whether a subtopic or topic should conclude
  * Consensus detection based on simple keyword matching or turn counts
"""

import re
from typing import List

class DebateRules:
    """
    Governs debate behavior and conclusion logic based on debate depth.
    """

    def __init__(self, config: dict = None):
        """
        Initialize thresholds and behavior settings.

        Config keys (all optional):
          - subtopic_max_turns: int (default 6)
          - topic_max_turns: int (default 10)
          - talking_point_max_turns: int (default 5)
          - consensus_keywords: List[str] (default ["agree", "consensus", "concur"])
          - consensus_min_hits: int (default 2)
        """
        cfg = config or {}
        self.subtopic_max_turns      = cfg.get("subtopic_max_turns", 6)
        self.topic_max_turns         = cfg.get("topic_max_turns", 10)
        self.talking_point_max_turns = cfg.get("talking_point_max_turns", 5)
        self.consensus_keywords      = cfg.get("consensus_keywords", ["agree", "consensus", "concur"])
        self.consensus_min_hits      = cfg.get("consensus_min_hits", 2)

    def get_rule_for_depth(self, depth: str) -> dict:
        """
        Return behavior parameters for a given depth:
          - depth: one of "subtopic", "topic", "talking_point"
        Returns a dict:
          { "mode": str,            # "consensus", "debate", or "explore"
            "max_turns": int        # Maximum turns before forced conclusion
          }
        """
        if depth == "subtopic":
            return {"mode": "consensus", "max_turns": self.subtopic_max_turns}
        elif depth == "topic":
            return {"mode": "debate", "max_turns": self.topic_max_turns}
        else:  # talking_point
            return {"mode": "explore", "max_turns": self.talking_point_max_turns}

    def should_conclude_subtopic(
        self,
        history: List[str],
        subtopic_identifier: str
    ) -> bool:
        """
        Determine if the subtopic should conclude, based on:
          - Whether consensus keywords appear in recent responses
          - Or whether the turn count for this subtopic exceeds threshold

        history: a list of strings representing the shared debate history
                 (e.g., ["Blue Bot: text", "Red Bot: text", ...])
        subtopic_identifier: the name of the current subtopic

        Returns True if the subtopic should end.
        """
        # Count only responses about this subtopic
        # We assume each pair of lines corresponds to one turn by each bot
        turns = [line for line in history if subtopic_identifier in line]
        turn_count = len(turns)

        # 1) Turn-based conclusion
        if turn_count >= self.subtopic_max_turns:
            return True

        # 2) Consensus-based conclusion
        keyword_hits = 0
        pattern = re.compile("|".join([re.escape(k) for k in self.consensus_keywords]), re.IGNORECASE)
        for line in turns[-(self.consensus_min_hits * 2):]:
            if pattern.search(line):
                keyword_hits += 1
            if keyword_hits >= self.consensus_min_hits:
                return True

        return False

    def should_conclude_topic(
        self,
        history: List[str],
        topic_identifier: str
    ) -> bool:
        """
        Similar logic for topic-level conclusion.
        """
        turns = [line for line in history if topic_identifier in line]
        return len(turns) >= self.topic_max_turns

    def should_conclude_talking_point(
        self,
        history: List[str],
        tp_identifier: str
    ) -> bool:
        """
        Determine if the entire talking point should conclude.
        """
        turns = [line for line in history if tp_identifier in line]
        return len(turns) >= self.talking_point_max_turns

    def apply_rules(
        self,
        engine,
        history: List[str]
    ) -> None:
        """
        Hook to be called after each response. Inspects the engine's current
        context (depth and identifiers) and if a conclusion condition is met,
        advances the engine to the next level.

        engine: instance of DebateEngine
        history: full shared history list (recent N lines)
        """
        tp, topic, subtopic = engine.current_context()
        # Subtopic level
        if self.should_conclude_subtopic(history, subtopic):
            # Advance to next subtopic/topic/tp
            engine._advance_indices()
            append_msg = f"📌 Subtopic '{subtopic}' concluded by rules."
            engine._log_shared(append_msg)
            return

        # Topic level
        if self.should_conclude_topic(history, topic):
            # Reset subtopic index, advance topic
            engine.advance_topic()
            append_msg = f"📌 Topic '{topic}' concluded by rules."
            engine._log_shared(append_msg)
            return

        # Talking Point level
        if self.should_conclude_talking_point(history, tp):
            engine.advance_talking_point()
            append_msg = f"📌 Talking Point '{tp}' concluded by rules."
            engine._log_shared(append_msg)
            return
```

File: engines/debate_rules.py

Implements the rules engine that governs how the two debate agents
(David and Zira) behave at each level of the debate hierarchy:
  - Subtopic: seek consensus or conclude after a fixed number of turns
  - Topic: argue/debate with higher tolerance
  - Talking Point: explore ideas broadly

This module provides:
  * DebateRules class with configurable thresholds
  * Functions to determine current “mode” and whether a subtopic or topic should conclude
  * Consensus detection based on simple keyword matching or turn counts
**Classes:** DebateRules


## Module `Debate_AI_Old\Debate_AI\engines\history_manager.py`

```python
#!/usr/bin/env python3
"""
File: engines/history_manager.py

Enhanced history and memory management for Debate AI:

  - Global shared history log (history.log)
  - Per-bot internal monologue logs (bots/{bot}_log.txt)
  - Pair-based context retrieval (prompt/response pairs)
  - Soft cap on memory context size
  - Automatic log rotation when files exceed size thresholds
  - Utilities for tailing logs and retrieving paired contexts
"""

import os
import threading
from datetime import datetime
from typing import List, Tuple

# === Configuration ===

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
BOTS_DIR = os.path.join(BASE_DIR, "bots")

HISTORY_PATH = os.path.join(DATA_DIR, "history.log")
BOT_LOGS = {
    "David": os.path.join(BOTS_DIR, "david_log.txt"),
    "Zira": os.path.join(BOTS_DIR, "zira_log.txt"),
    "Jarvis": os.path.join(BOTS_DIR, "jarvis_log.txt")
}

# Soft limits
HISTORY_SOFT_CAP_BYTES = 5 * 1024 * 1024   # 5 MB
BOT_LOG_SOFT_CAP_BYTES = 2 * 1024 * 1024   # 2 MB

# Memory context
PAIRS_CONTEXT_LIMIT = 20  # last N prompt/response pairs per AI call

_lock = threading.Lock()

def _ensure_dirs():
    """Ensure that required directories exist."""
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(BOTS_DIR, exist_ok=True)

def _rotate_if_oversize(path: str, max_bytes: int):
    """
    If `path` exceeds `max_bytes`, rotate it by renaming with timestamp.
    Creates a new empty file at `path`.
    """
    try:
        if os.path.exists(path) and os.path.getsize(path) > max_bytes:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            archive = f"{path}.{timestamp}.bak"
            os.rename(path, archive)
            # New empty file
            open(path, "w", encoding="utf-8").close()
    except Exception:
        # Failure to rotate should not crash the system
        pass

def append_shared(entry: str):
    """
    Append a timestamped line to the global shared history log.
    Also rotates the log if it grows too large.
    """
    _ensure_dirs()
    timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    line = f"{timestamp} {entry}\n"
    with _lock:
        _rotate_if_oversize(HISTORY_PATH, HISTORY_SOFT_CAP_BYTES)
        with open(HISTORY_PATH, "a", encoding="utf-8") as f:
            f.write(line)

def append_bot(bot_name: str, role: str, text: str):
    """
    Append a timestamped entry to a specific bot's internal log.
    - bot_name: one of "David", "Zira", or "Jarvis"
    - role: typically "PROMPT" or "RESPONSE"
    - text: the content to log
    Rotates bot log if oversize.
    """
    _ensure_dirs()
    path = BOT_LOGS.get(bot_name, None)
    if not path:
        return
    timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    entry = f"{timestamp} {role}: {text}\n"
    with _lock:
        _rotate_if_oversize(path, BOT_LOG_SOFT_CAP_BYTES)
        with open(path, "a", encoding="utf-8") as f:
            f.write(entry)

def tail(n: int = 100) -> List[str]:
    """
    Return the last `n` lines from the shared history log.
    If the file is shorter, returns all lines.
    """
    if not os.path.exists(HISTORY_PATH):
        return []
    with open(HISTORY_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()
    return lines[-n:]

def get_recent_pairs(limit: int = PAIRS_CONTEXT_LIMIT) -> List[Tuple[str, str]]:
    """
    Retrieve the last `limit` prompt/response pairs from the shared history.
    Assumes that history.log is formatted as alternating lines:
      [timestamp] Bot: message
      [timestamp] OtherBot: response
    Returns a list of (prompt, response) tuples, oldest first.
    """
    lines = tail(limit * 2)
    pairs = []
    buffer = []
    for line in lines:
        # Strip timestamp
        try:
            content = line.split("]", 1)[1].strip()
        except IndexError:
            continue
        buffer.append(content)
        if len(buffer) == 2:
            pairs.append((buffer[0], buffer[1]))
            buffer = []
    return pairs

def clear_history():
    """
    Completely clear shared history log and all bot logs.
    WARNING: This deletes all existing logs.
    """
    _ensure_dirs()
    with _lock:
        open(HISTORY_PATH, "w", encoding="utf-8").close()
        for path in BOT_LOGS.values():
            open(path, "w", encoding="utf-8").close()

# Alias for backward compatibility
append_entry = append_shared
```

File: engines/history_manager.py

Enhanced history and memory management for Debate AI:

  - Global shared history log (history.log)
  - Per-bot internal monologue logs (bots/{bot}_log.txt)
  - Pair-based context retrieval (prompt/response pairs)
  - Soft cap on memory context size
  - Automatic log rotation when files exceed size thresholds
  - Utilities for tailing logs and retrieving paired contexts
**Functions:** _ensure_dirs(), _rotate_if_oversize(path, max_bytes), append_shared(entry), append_bot(bot_name, role, text), tail(n), get_recent_pairs(limit), clear_history()


## Module `Debate_AI_Old\Debate_AI\engines\jarvis_engine.py`

```python
# File: engines/jarvis_engine.py

import os
import json
from json import JSONDecodeError
import openai
from .history_manager import tail, append_entry

# Paths
BASE_DIR = os.path.dirname(__file__)
API_KEY_PATH = os.path.join(BASE_DIR, "..", "api", "api_key.txt")
STRUCT_PATH = os.path.join(BASE_DIR, "..", "data", "structure.json")

# Load OpenAI key
with open(API_KEY_PATH, "r", encoding="utf-8") as f:
    OPENAI_KEY = f.read().strip()
openai_client = openai.OpenAI(api_key=OPENAI_KEY)

# Default structure to bootstrap if file is missing or invalid
DEFAULT_STRUCT = {
    "talking_points": [
        {
            "name": "Sample Talking Point",
            "topics": [
                {
                    "name": "Sample Topic",
                    "subtopics": ["Example Subtopic"]
                }
            ]
        }
    ]
}

def load_structure():
    """
    Load the debate structure from disk. If missing, empty, or invalid,
    initialize with DEFAULT_STRUCT and return a copy.
    """
    os.makedirs(os.path.dirname(STRUCT_PATH), exist_ok=True)

    # If file doesn't exist or is empty, write default
    if not os.path.exists(STRUCT_PATH) or os.path.getsize(STRUCT_PATH) == 0:
        with open(STRUCT_PATH, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_STRUCT, f, indent=2)
        return DEFAULT_STRUCT.copy()

    # Try to load existing JSON
    try:
        with open(STRUCT_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except (JSONDecodeError, ValueError):
        # Overwrite corrupted file with default
        with open(STRUCT_PATH, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_STRUCT, f, indent=2)
        return DEFAULT_STRUCT.copy()

def save_structure(structure: dict):
    """
    Persist the debate structure back to disk.
    """
    os.makedirs(os.path.dirname(STRUCT_PATH), exist_ok=True)
    with open(STRUCT_PATH, "w", encoding="utf-8") as f:
        json.dump(structure, f, indent=2)

def summarize():
    """
    Summarize the current state of the debate by feeding structure + recent history
    into OpenAI and returning a concise summary string.
    """
    recent = "".join(tail(100))
    struct = load_structure()
    prompt = (
        "You are Jarvis, an AI assistant that summarizes the state of an expert debate.\n\n"
        f"Current structure:\n{json.dumps(struct, indent=2)}\n\n"
        f"Recent transcript:\n{recent}\n\n"
        "Please provide a concise summary of the debate's current status, "
        "highlighting key conclusions and pending questions."
    )
    resp = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    summary = resp.choices[0].message.content.strip()
    append_entry(f"Jarvis summary: {summary}")
    return summary

def interpret_user(text: str):
    """
    Parse the user's instruction text to generate a JSON delta for the structure.
    Returns the delta and applies it to structure.json.
    """
    struct = load_structure()
    prompt = (
        "You are Jarvis, an AI that applies user instructions to update a debate structure.\n\n"
        f"Existing structure:\n{json.dumps(struct, indent=2)}\n\n"
        f"User instruction: \"{text}\"\n\n"
        "Extract a JSON object with keys:\n"
        "  action: one of 'add_tp', 'add_topic', 'add_subtopic'\n"
        "  talking_point: <name>\n"
        "  topic: <name> (if applicable)\n"
        "  subtopic: <name> (if applicable)\n"
        "If instruction doesn't map to a valid change, return {}."
    )
    resp = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    try:
        delta = json.loads(resp.choices[0].message.content.strip())
    except Exception:
        delta = {}

    # Apply the delta if valid
    action = delta.get("action")
    if action == "add_tp":
        struct.setdefault("talking_points", []).append({
            "name": delta["talking_point"],
            "topics": []
        })
    elif action == "add_topic":
        for tp in struct.get("talking_points", []):
            if tp["name"] == delta.get("talking_point"):
                tp.setdefault("topics", []).append({
                    "name": delta.get("topic"),
                    "subtopics": []
                })
    elif action == "add_subtopic":
        for tp in struct.get("talking_points", []):
            if tp["name"] == delta.get("talking_point"):
                for topic in tp.get("topics", []):
                    if topic["name"] == delta.get("topic"):
                        topic.setdefault("subtopics", []).append(delta.get("subtopic"))

    # Save and log the change
    save_structure(struct)
    append_entry(f"Jarvis applied delta: {delta}")
    return delta
```

**Functions:** load_structure(), save_structure(structure), summarize(), interpret_user(text)


## Module `Debate_AI_Old\Debate_AI\engines\jarvis_injector.py`

```python
#!/usr/bin/env python3
"""
File: engines/jarvis_injector.py

Bridges Jarvis Driver commands and the main Debate AI engine by:
  - Queuing user instructions from Jarvis_Driver into a JSONL file.
  - Periodically reading and clearing that file.
  - Applying each instruction via the `interpret_user` function
    (which updates the debate structure).
  - Ensuring thread-safety and durability.

Usage in Debate_AI main loop:
    from engines.jarvis_injector import apply_queued_commands
    apply_queued_commands(self.engine)
"""

import os
import json
import threading
from datetime import datetime
from .jarvis_engine import interpret_user, load_structure, save_structure

# === Paths & Setup ===
BASE_DIR       = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR       = os.path.join(BASE_DIR, "data")
COMMANDS_PATH  = os.path.join(DATA_DIR, "jarvis_commands.jsonl")

_lock = threading.Lock()

def _ensure_data_dir():
    """Ensure that the data directory exists."""
    os.makedirs(DATA_DIR, exist_ok=True)

def queue_command(command_text: str):
    """
    Append a user instruction to the Jarvis commands queue.
    Each line in COMMANDS_PATH is a JSON object:
       {"timestamp": "...", "command": "..."}
    """
    _ensure_data_dir()
    entry = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "command": command_text
    }
    line = json.dumps(entry, ensure_ascii=False)
    with _lock, open(COMMANDS_PATH, "a", encoding="utf-8") as f:
        f.write(line + "\n")

def get_queued_commands() -> list:
    """
    Read all queued commands from the file.
    Returns a list of command strings (in chronological order).
    """
    _ensure_data_dir()
    commands = []
    if not os.path.exists(COMMANDS_PATH):
        return commands
    with _lock, open(COMMANDS_PATH, "r", encoding="utf-8") as f:
        for raw in f:
            raw = raw.strip()
            if not raw:
                continue
            try:
                obj = json.loads(raw)
                cmd = obj.get("command")
                if cmd:
                    commands.append(cmd)
            except json.JSONDecodeError:
                # Skip malformed lines
                continue
    return commands

def clear_queued_commands():
    """
    Remove all queued commands by truncating the file.
    """
    _ensure_data_dir()
    with _lock, open(COMMANDS_PATH, "w", encoding="utf-8"):
        pass  # Opening in write mode empties the file

def apply_queued_commands(engine):
    """
    Load and apply every queued Jarvis command to the DebateEngine:
      - interpret_user(...) is called for each command text
      - structure.json is updated accordingly
      - the engine is re-initialized with the new structure
    
    Returns:
        List of commands that were applied.
    """
    commands = get_queued_commands()
    if not commands:
        return []

    # Apply each command via interpret_user (which updates structure.json)
    applied = []
    for cmd in commands:
        delta = interpret_user(cmd)
        if delta:
            applied.append({"command": cmd, "delta": delta})

    # If any changes, reload structure into engine
    if applied:
        new_struct = load_structure()
        engine.structure = new_struct

    # Clear the queue so we don't reapply
    clear_queued_commands()
    return applied
```

File: engines/jarvis_injector.py

Bridges Jarvis Driver commands and the main Debate AI engine by:
  - Queuing user instructions from Jarvis_Driver into a JSONL file.
  - Periodically reading and clearing that file.
  - Applying each instruction via the `interpret_user` function
    (which updates the debate structure).
  - Ensuring thread-safety and durability.

Usage in Debate_AI main loop:
    from engines.jarvis_injector import apply_queued_commands
    apply_queued_commands(self.engine)
**Functions:** _ensure_data_dir(), queue_command(command_text), get_queued_commands(), clear_queued_commands(), apply_queued_commands(engine)


## Module `Debate_AI_Old\Debate_AI\engines\prompt_router.py`

```python
#!/usr/bin/env python3
"""
File: engines/prompt_router.py

This module is responsible for constructing API prompts for each bot
(David or Zira) based on the current debate context (Talking Point → Topic → Subtopic),
the selected prompt template, and any additional parameters (e.g., debate rules,
history, or style modifiers).

It loads a catalog of prompt templates from prompt_templates.py, fills in the
placeholders, and returns a ready-to-send string.  This ensures that prompts
are consistent, schema-aware, and easily adjustable via templates.

Usage:
    from engines.prompt_router import PromptRouter

    router = PromptRouter()
    prompt = router.compose_prompt(
        bot_name="David Bot",
        talking_point="Ethics of AI",
        topic="Transparency",
        subtopic="Explainability",
        history=last_20_messages,
        style="analytical"
    )
    # Send `prompt` to the AI provider
"""

import os
import json
from typing import List, Dict

# Import the prompt template functions
from .prompt_templates import load_templates, get_template_for

class PromptRouter:
    """
    PromptRouter builds fully-formed prompts for AI agents based on:
      - Bot identity (e.g., "David Bot" or "Zira Bot")
      - Debate structure context (talking point, topic, subtopic)
      - Predefined templates (loaded from prompt_templates)
      - Recent history (for context if needed)
      - Optional style or rule modifiers

    The main entrypoint is `compose_prompt(...)`.
    """

    def __init__(self, templates_dir: str = None):
        """
        Initialize the router by loading templates.

        :param templates_dir: Optional path to a directory of JSON templates.
                              If None, uses the default directory defined
                              within prompt_templates.py.
        """
        # Load all templates into a nested dict:
        # { (topic, subtopic) -> template_dict, ... }
        self.templates = load_templates(templates_dir)
        # Example of a template entry:
        # {
        #   "template": "As {bot}, debate the ramifications of '{subtopic}' in the context of '{topic}'.",
        #   "style": "analytical",
        #   "depth": "subtopic"
        # }
        # style/depth can be used to select which template fits best

    def compose_prompt(
        self,
        bot_name: str,
        talking_point: str,
        topic: str,
        subtopic: str,
        history: List[str] = None,
        style: str = None,
        rules: Dict[str, str] = None
    ) -> str:
        """
        Build and return the prompt string for a given bot at the specified
        point in the debate.

        :param bot_name: Name of the speaking bot ("David Bot" or "Zira Bot").
        :param talking_point: High-level debate category.
        :param topic: Mid-level topic under the talking point.
        :param subtopic: Specific subtopic under the topic.
        :param history: Optional list of recent debate messages for extra context.
        :param style: Optional style override (e.g., "aggressive", "concise").
        :param rules: Optional dict of rule parameters (e.g., {"level": "subtopic"}).
        :return: Fully formatted prompt string ready for the LLM API.
        """

        # 1. Select the best template for this (topic, subtopic) pair.
        #    If no exact match, fall back to topic-level or global default.
        template_entry = get_template_for(
            topic=topic,
            subtopic=subtopic,
            style=style,
            templates=self.templates
        )
        template_str = template_entry.get("template", "")
        if not template_str:
            # Hard fallback if template is missing
            template_str = (
                "You are {bot}. Please discuss the subtopic '{subtopic}' "
                "within the topic '{topic}' and talking point '{talking_point}'."
            )

        # 2. Prepare the placeholder mapping
        placeholder_map = {
            "bot": bot_name,
            "talking_point": talking_point,
            "topic": topic,
            "subtopic": subtopic
        }

        # 3. Fill in the template placeholders
        try:
            prompt_core = template_str.format(**placeholder_map)
        except KeyError as e:
            # If a placeholder is missing, fall back to a simpler format
            prompt_core = (
                f"{bot_name}, please address subtopic '{subtopic}' "
                f"of topic '{topic}' in talking point '{talking_point}'."
            )

        # 4. Append rule-specific instructions if provided
        #    e.g., enforce consensus-seeking at subtopic level
        rule_instructions = ""
        if rules:
            # Example: rules may contain {"depth": "subtopic", "behavior": "consensus"}
            depth = rules.get("depth")
            behavior = rules.get("behavior")
            if depth and behavior:
                rule_instructions = (
                    f" This engagement is at the {depth} level, "
                    f"so please adopt a {behavior} style."
                )

        # 5. Optionally include a snippet of recent history for context
        history_instructions = ""
        if history:
            # Join the last few entries into a single text block
            snippet = "\n".join(history[-5:])
            history_instructions = (
                f"\n\nRecent debate excerpts:\n{snippet}\n\n"
                "Continue from here."
            )

        # 6. Combine all parts into the final prompt
        prompt = prompt_core + rule_instructions + history_instructions

        return prompt


# Optional convenience function for quick use
def route_prompt(
    bot_name: str,
    talking_point: str,
    topic: str,
    subtopic: str,
    history: List[str] = None,
    style: str = None,
    rules: Dict[str, str] = None
) -> str:
    """
    Quick helper to build a prompt without explicitly instantiating PromptRouter.
    """
    router = PromptRouter()
    return router.compose_prompt(
        bot_name=bot_name,
        talking_point=talking_point,
        topic=topic,
        subtopic=subtopic,
        history=history,
        style=style,
        rules=rules
    )
```

File: engines/prompt_router.py

This module is responsible for constructing API prompts for each bot
(David or Zira) based on the current debate context (Talking Point → Topic → Subtopic),
the selected prompt template, and any additional parameters (e.g., debate rules,
history, or style modifiers).

It loads a catalog of prompt templates from prompt_templates.py, fills in the
placeholders, and returns a ready-to-send string.  This ensures that prompts
are consistent, schema-aware, and easily adjustable via templates.

Usage:
    from engines.prompt_router import PromptRouter

    router = PromptRouter()
    prompt = router.compose_prompt(
        bot_name="David Bot",
        talking_point="Ethics of AI",
        topic="Transparency",
        subtopic="Explainability",
        history=last_20_messages,
        style="analytical"
    )
    # Send `prompt` to the AI provider
**Classes:** PromptRouter
**Functions:** route_prompt(bot_name, talking_point, topic, subtopic, history, style, rules)


## Module `Debate_AI_Old\Debate_AI\engines\prompt_templates.py`

```python
#!/usr/bin/env python3
"""
File: engines/prompt_templates.py

Provides loading and retrieval of MadLib‐style prompt templates for the
Debate AI system. Templates can be defined per topic, per subtopic, and
optionally by style. If no external templates directory is provided, a
built-in set of defaults is used.

Templates are simple JSON objects with fields:
  - "topic":       Name of the topic (string) or None for wildcard
  - "subtopic":    Name of the subtopic (string) or None for wildcard
  - "style":       Style key (e.g., "aggressive", "analytical") or None
  - "template":    The prompt string, using Python .format() placeholders:
                   {bot}, {talking_point}, {topic}, {subtopic}
  - "description": (optional) Human‐readable description of the template

Functions:
  - load_templates(templates_dir=None): returns a list of template dicts
  - get_template_for(topic, subtopic, style, templates): picks best match
"""

import os
import json
from typing import List, Dict, Optional

# Built-in default templates
_DEFAULT_TEMPLATES = [
    {
        "topic": None,
        "subtopic": None,
        "style": None,
        "template": (
            "{bot}, please discuss the subtopic '{subtopic}' "
            "within the topic '{topic}' under the talking point '{talking_point}'."
        ),
        "description": "Global fallback template"
    },
    {
        "topic": None,
        "subtopic": None,
        "style": "analytical",
        "template": (
            "{bot}, adopt an analytical tone as you examine "
            "the subtopic '{subtopic}' under '{topic}'."
        ),
        "description": "Generic analytical style"
    },
    {
        "topic": None,
        "subtopic": None,
        "style": "aggressive",
        "template": (
            "{bot}, argue forcefully about '{subtopic}' "
            "in the context of '{topic}'. Be bold and challenge assumptions."
        ),
        "description": "Generic aggressive style"
    }
]

def load_templates(templates_dir: Optional[str] = None) -> List[Dict]:
    """
    Load prompt templates from JSON files in the given directory.
    Each file should contain either a single JSON object or a list of objects
    matching the template schema. If templates_dir is None or empty, returns
    the built-in defaults.

    :param templates_dir: Path to directory containing .json template files.
    :return: List of template dictionaries.
    """
    templates = []

    # If a directory is provided and exists, load .json files
    if templates_dir and os.path.isdir(templates_dir):
        for fname in os.listdir(templates_dir):
            if fname.lower().endswith(".json"):
                path = os.path.join(templates_dir, fname)
                try:
                    data = json.load(open(path, "r", encoding="utf-8"))
                    # If single object, wrap in list
                    if isinstance(data, dict):
                        templates.append(data)
                    elif isinstance(data, list):
                        templates.extend(data)
                except Exception as e:
                    print(f"Warning: failed to load templates from {path}: {e}")

    # Always include built-in defaults at the end, so they have lowest priority
    templates.extend(_DEFAULT_TEMPLATES)
    return templates

def get_template_for(
    topic: str,
    subtopic: str,
    style: Optional[str],
    templates: List[Dict]
) -> Dict:
    """
    Select the most specific matching template given the debate context.

    Matching priority (highest to lowest):
      1. Exact match on (topic, subtopic, style)
      2. Match on (topic, subtopic, None style)
      3. Match on (topic, None, style)
      4. Match on (None, None, style)
      5. Match on (topic, None, None)
      6. Global default (None, None, None)—always present

    :param topic:      Current topic name
    :param subtopic:   Current subtopic name
    :param style:      Desired style key or None
    :param templates:  List of template dicts loaded via load_templates
    :return: A single template dict
    """
    # Prepare list of candidate match functions in order
    def match_exact(tpl):
        return (tpl.get("topic") == topic and
                tpl.get("subtopic") == subtopic and
                tpl.get("style") == style)

    def match_ts(tpl):
        return (tpl.get("topic") == topic and
                tpl.get("subtopic") == subtopic and
                tpl.get("style") is None)

    def match_t_s(tpl):
        return (tpl.get("topic") == topic and
                tpl.get("subtopic") is None and
                tpl.get("style") == style)

    def match__s(tpl):
        return (tpl.get("topic") is None and
                tpl.get("subtopic") is None and
                tpl.get("style") == style)

    def match_t__(tpl):
        return (tpl.get("topic") == topic and
                tpl.get("subtopic") is None and
                tpl.get("style") is None)

    def match_default(tpl):
        return tpl.get("topic") is None and tpl.get("subtopic") is None

    # List of (matcher_fn, description) in priority order
    matchers = [
        (match_exact,       "topic+subtopic+style"),
        (match_ts,          "topic+subtopic"),
        (match_t_s,         "topic+style"),
        (match__s,          "style only"),
        (match_t__,         "topic only"),
        (match_default,     "default")
    ]

    # Iterate matchers in order
    for matcher, desc in matchers:
        for tpl in templates:
            if matcher(tpl):
                return tpl

    # If somehow nothing matched, return the first default
    return _DEFAULT_TEMPLATES[0]
```

File: engines/prompt_templates.py

Provides loading and retrieval of MadLib‐style prompt templates for the
Debate AI system. Templates can be defined per topic, per subtopic, and
optionally by style. If no external templates directory is provided, a
built-in set of defaults is used.

Templates are simple JSON objects with fields:
  - "topic":       Name of the topic (string) or None for wildcard
  - "subtopic":    Name of the subtopic (string) or None for wildcard
  - "style":       Style key (e.g., "aggressive", "analytical") or None
  - "template":    The prompt string, using Python .format() placeholders:
                   {bot}, {talking_point}, {topic}, {subtopic}
  - "description": (optional) Human‐readable description of the template

Functions:
  - load_templates(templates_dir=None): returns a list of template dicts
  - get_template_for(topic, subtopic, style, templates): picks best match
**Functions:** load_templates(templates_dir), get_template_for(topic, subtopic, style, templates)


## Module `Debate_AI_Old\Debate_AI\engines\session_manager.py`

```python
#!/usr/bin/env python3
"""
session_manager.py

Manages Debate AI sessions: creation, listing, saving, loading, and resetting.

Folder layout assumed:
  <project_root>/
    data/
      structure.json
      history.log
      sessions/
        <session_name>/
          structure.json
          history.log
          david_log.txt
          zira_log.txt
          jarvis_log.txt
    bots/
      david_log.txt
      zira_log.txt
      jarvis_log.txt
"""

import os
import json
import shutil
from typing import List

# ----- Constants & Paths -----

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
DATA_DIR     = os.path.join(PROJECT_ROOT, "data")
SESSIONS_DIR = os.path.join(DATA_DIR, "sessions")

STRUCT_FILE  = os.path.join(DATA_DIR, "structure.json")
HISTORY_FILE = os.path.join(DATA_DIR, "history.log")

BOTS_DIR     = os.path.join(PROJECT_ROOT, "bots")
BOT_LOG_FILES = {
    "David": os.path.join(BOTS_DIR, "david_log.txt"),
    "Zira":  os.path.join(BOTS_DIR, "zira_log.txt"),
    "Jarvis":os.path.join(BOTS_DIR, "jarvis_log.txt"),
}


# ----- Helpers -----

def _ensure_dirs():
    """Make sure data/ and sessions/ folders exist."""
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(SESSIONS_DIR, exist_ok=True)
    os.makedirs(BOTS_DIR, exist_ok=True)


def _copy(src: str, dst: str):
    """Copy a file, overwriting if needed."""
    os.makedirs(os.path.dirname(dst), exist_ok=True)
    shutil.copy2(src, dst)


# ----- Session API -----

def get_all_sessions() -> List[str]:
    """
    Return sorted list of existing session names (subdirectories).
    """
    _ensure_dirs()
    names = []
    for name in os.listdir(SESSIONS_DIR):
        full = os.path.join(SESSIONS_DIR, name)
        if os.path.isdir(full):
            names.append(name)
    return sorted(names)


def new_session():
    """
    Wipe out current data to start fresh:
      • Overwrite structure.json with an empty template
      • Truncate history.log
      • Truncate each bot’s log
    """
    _ensure_dirs()

    # 1) Reset structure to an empty template
    empty_struct = {"talking_points": []}
    with open(STRUCT_FILE, "w", encoding="utf-8") as f:
        json.dump(empty_struct, f, indent=2)

    # 2) Clear history
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        f.write("")

    # 3) Clear bot logs
    for path in BOT_LOG_FILES.values():
        with open(path, "w", encoding="utf-8") as f:
            f.write("")


def save_session(name: str):
    """
    Save the *current* session under a new or existing name.
    Creates folder data/sessions/{name}/ and copies:
      - structure.json
      - history.log
      - each bot log file
    """
    _ensure_dirs()

    if not name or any(c in name for c in r'\/:*?"<>|'):
        raise ValueError(f"Invalid session name: '{name}'")

    dest_dir = os.path.join(SESSIONS_DIR, name)
    os.makedirs(dest_dir, exist_ok=True)

    # Copy core files
    _copy(STRUCT_FILE,  os.path.join(dest_dir, "structure.json"))
    _copy(HISTORY_FILE, os.path.join(dest_dir, "history.log"))

    # Copy each bot log
    for bot, path in BOT_LOG_FILES.items():
        if os.path.exists(path):
            _copy(path, os.path.join(dest_dir, f"{bot.lower()}_log.txt"))
        else:
            # create empty if missing
            open(os.path.join(dest_dir, f"{bot.lower()}_log.txt"), "w").close()


def load_session(name: str):
    """
    Load the session named `name`:
      - Prompts to `new_session()` should be done by caller if dirty.
      - Copies files from data/sessions/{name}/ → live locations.
    """
    _ensure_dirs()

    src_dir = os.path.join(SESSIONS_DIR, name)
    if not os.path.isdir(src_dir):
        raise FileNotFoundError(f"Session '{name}' does not exist.")

    # Copy back into live data
    _copy(os.path.join(src_dir, "structure.json"), STRUCT_FILE)
    _copy(os.path.join(src_dir, "history.log"),  HISTORY_FILE)

    for bot, live_path in BOT_LOG_FILES.items():
        session_path = os.path.join(src_dir, f"{bot.lower()}_log.txt")
        # If missing in session, just clear live
        if os.path.exists(session_path):
            _copy(session_path, live_path)
        else:
            open(live_path, "w").close()
```

session_manager.py

Manages Debate AI sessions: creation, listing, saving, loading, and resetting.

Folder layout assumed:
  <project_root>/
    data/
      structure.json
      history.log
      sessions/
        <session_name>/
          structure.json
          history.log
          david_log.txt
          zira_log.txt
          jarvis_log.txt
    bots/
      david_log.txt
      zira_log.txt
      jarvis_log.txt
**Functions:** _ensure_dirs(), _copy(src, dst), get_all_sessions(), new_session(), save_session(name), load_session(name)


## Module `Debate_AI_Old\Debate_AI\engines\structure_controller.py`

```python
#!/usr/bin/env python3
"""
structure_controller.py

Manages the debate “structure” hierarchy:
  Talking Points → Topics → Subtopics

All changes are kept in memory and persisted to data/structure.json.
"""

import os
import json
import uuid
from typing import List, Optional, Tuple, Dict, Any

# ——— Paths ———
BASE_DIR    = os.path.dirname(os.path.abspath(__file__))
DATA_DIR    = os.path.join(BASE_DIR, "data")
STRUCT_PATH = os.path.join(DATA_DIR, "structure.json")


# ——— Helpers ———

def _ensure_data_dir():
    """Ensure that the data directory exists."""
    os.makedirs(DATA_DIR, exist_ok=True)


def _load_file() -> Dict[str, Any]:
    """Read the JSON structure file, creating default if missing."""
    _ensure_data_dir()
    if not os.path.exists(STRUCT_PATH) or os.path.getsize(STRUCT_PATH) == 0:
        default = {"talking_points": []}
        _save_file(default)
        return default
    with open(STRUCT_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def _save_file(struct: Dict[str, Any]):
    """Write the in-memory structure back to disk."""
    _ensure_data_dir()
    with open(STRUCT_PATH, "w", encoding="utf-8") as f:
        json.dump(struct, f, indent=2)


# ——— Core API ———

def load_structure() -> Dict[str, Any]:
    """
    Return the current debate structure as a dict.
    Schema:
      { "talking_points": [
           { "name": str, "desc": str, "details": str,
             "topics": [
                { "name": str, "desc": str, "details": str,
                  "subtopics": [
                     { "name": str, "desc": str, "details": str }
                  ]
                }
             ]
           }
         ]
      }
    """
    return _load_file()


def save_structure(struct: Dict[str, Any]):
    """
    Persist the given structure dict to data/structure.json.
    """
    _save_file(struct)


def _find_node_and_parent(
    path: List[str],
    struct: Optional[Dict[str, Any]] = None
) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
    """
    Locate a node by path of names and return (node, parent_list).
    path examples:
      ["TP Name"]
      ["TP Name", "Topic Name"]
      ["TP Name", "Topic Name", "Subtopic Name"]
    Raises KeyError if not found.
    """
    struct = struct or load_structure()
    # Top-level talking points
    if len(path) == 1:
        name = path[0]
        for tp in struct["talking_points"]:
            if tp["name"] == name:
                return tp, struct["talking_points"]
        raise KeyError(f"Talking Point '{name}' not found")

    # Topic level
    if len(path) == 2:
        tp_name, topic_name = path
        for tp in struct["talking_points"]:
            if tp["name"] == tp_name:
                for topic in tp["topics"]:
                    if topic["name"] == topic_name:
                        return topic, tp["topics"]
        raise KeyError(f"Topic '{topic_name}' under TP '{tp_name}' not found")

    # Subtopic level
    if len(path) == 3:
        tp_name, topic_name, sub_name = path
        for tp in struct["talking_points"]:
            if tp["name"] == tp_name:
                for topic in tp["topics"]:
                    if topic["name"] == topic_name:
                        for sub in topic["subtopics"]:
                            if sub["name"] == sub_name:
                                return sub, topic["subtopics"]
        raise KeyError(f"Subtopic '{sub_name}' under Topic '{topic_name}' not found")

    raise KeyError(f"Invalid path length: {path}")


def create_node(
    node_type: str,
    name: str,
    desc: str = "",
    details: str = "",
    parent_path: Optional[List[str]] = None
):
    """
    Create a new node in the structure.
    :param node_type: "talking_point" | "topic" | "subtopic"
    :param name: Name of the new node
    :param desc: Short description
    :param details: Full detail text
    :param parent_path:
       - None for top-level talking_point
       - ["TP Name"] for topic
       - ["TP Name","Topic Name"] for subtopic
    """
    struct = load_structure()
    node = { "name": name, "desc": desc, "details": details }
    if node_type == "talking_point":
        node["topics"] = []
        struct["talking_points"].append(node)

    elif node_type == "topic":
        if not parent_path or len(parent_path) != 1:
            raise ValueError("parent_path must be [TP Name] for topic")
        tp, tp_list = _find_node_and_parent(parent_path, struct)
        node["subtopics"] = []
        tp["topics"].append(node)

    elif node_type == "subtopic":
        if not parent_path or len(parent_path) != 2:
            raise ValueError("parent_path must be [TP Name,Topic Name] for subtopic")
        topic, topic_list = _find_node_and_parent(parent_path, struct)
        topic["subtopics"].append(node)

    else:
        raise ValueError(f"Unknown node_type '{node_type}'")

    save_structure(struct)


def update_node(
    path: List[str],
    name: Optional[str] = None,
    desc: Optional[str] = None,
    details: Optional[str] = None
):
    """
    Update fields of an existing node.
    :param path: Path to node by names.
    :param name: New name (if changing)
    :param desc: New description
    :param details: New full details
    """
    struct = load_structure()
    node, parent_list = _find_node_and_parent(path, struct)

    if name is not None:
        node["name"] = name
    if desc is not None:
        node["desc"] = desc
    if details is not None:
        node["details"] = details

    save_structure(struct)


def delete_node(path: List[str]):
    """
    Delete a node (and its subtree) from the structure.
    :param path: Path to node by names.
    """
    struct = load_structure()
    node, parent_list = _find_node_and_parent(path, struct)
    parent_list[:] = [n for n in parent_list if n is not node]
    save_structure(struct)


def get_node(path: List[str]) -> Dict[str, Any]:
    """
    Return the node dict at the given path.
    """
    struct = load_structure()
    node, _ = _find_node_and_parent(path, struct)
    return node.copy()
```

structure_controller.py

Manages the debate “structure” hierarchy:
  Talking Points → Topics → Subtopics

All changes are kept in memory and persisted to data/structure.json.
**Functions:** _ensure_data_dir(), _load_file(), _save_file(struct), load_structure(), save_structure(struct), _find_node_and_parent(path, struct), create_node(node_type, name, desc, details, parent_path), update_node(path, name, desc, details), delete_node(path), get_node(path)


## Module `Debate_AI_Old\Debate_AI\engines\structure_panel_ui.py`

```python
#!/usr/bin/env python3
"""
structure_panel_ui.py

Far-right panel for viewing and editing a selected structure node:
  - Name, Description, Details fields
  - Add Child, Update, Delete buttons
  - Emits signals when operations complete, so main UI can refresh
"""

import os
from PyQt5.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout,
    QLabel, QLineEdit, QTextEdit,
    QPushButton, QMessageBox, QSizePolicy, QInputDialog
)
from PyQt5.QtCore import Qt, pyqtSignal

import structure_controller as sc


class StructurePanel(QWidget):
    # Emitted after any change: new node, update, or delete
    structure_changed = pyqtSignal()

    def __init__(self, parent=None):
        super().__init__(parent)
        self._current_path = None  # List[str] path to selected node
        self._init_ui()

    def _init_ui(self):
        layout = QVBoxLayout(self)
        layout.setContentsMargins(10, 10, 10, 10)
        layout.setSpacing(8)

        # Node Type Display
        self.type_label = QLabel("Node Type: —", self)
        self.type_label.setStyleSheet("font-weight: bold;")
        layout.addWidget(self.type_label)

        # Name
        layout.addWidget(QLabel("Name:", self))
        self.name_edit = QLineEdit(self)
        layout.addWidget(self.name_edit)

        # Description
        layout.addWidget(QLabel("Description:", self))
        self.desc_edit = QLineEdit(self)
        layout.addWidget(self.desc_edit)

        # Details
        layout.addWidget(QLabel("Details:", self))
        self.details_edit = QTextEdit(self)
        self.details_edit.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        layout.addWidget(self.details_edit)

        # Buttons: Add Child, Update, Delete
        btn_layout = QHBoxLayout()
        self.add_btn = QPushButton("Add Child", self)
        self.add_btn.clicked.connect(self._on_add)
        btn_layout.addWidget(self.add_btn)

        self.update_btn = QPushButton("Update", self)
        self.update_btn.clicked.connect(self._on_update)
        btn_layout.addWidget(self.update_btn)

        self.delete_btn = QPushButton("Delete", self)
        self.delete_btn.clicked.connect(self._on_delete)
        btn_layout.addWidget(self.delete_btn)

        btn_layout.addStretch()
        layout.addLayout(btn_layout)

        # Spacer at bottom
        layout.addStretch()

    def load_node(self, path: list):
        """
        Populate panel with the node at `path`.
        :param path: list of names, e.g. ["Talking Point"] or ["TP","Topic"] etc.
        """
        try:
            node = sc.get_node(path)
        except KeyError:
            QMessageBox.warning(self, "Structure Panel", f"Node {' > '.join(path)} not found.")
            return

        self._current_path = path.copy()
        depth = len(path)
        tmap = {1: "Talking Point", 2: "Topic", 3: "Subtopic"}
        self.type_label.setText(f"Node Type: {tmap.get(depth,'?')}")

        self.name_edit.setText(node.get("name", ""))
        self.desc_edit.setText(node.get("desc", ""))
        self.details_edit.setPlainText(node.get("details", ""))

        # Enable/disable Add Child based on depth
        self.add_btn.setEnabled(depth < 3)

    def _on_add(self):
        """
        Add a child node under the current node.
        Prompts user for new node type & name via simple dialog.
        """
        if not self._current_path:
            # Add top-level Talking Point
            parent = None
            node_type = "talking_point"
        else:
            depth = len(self._current_path)
            if depth == 1:
                node_type = "topic"
            elif depth == 2:
                node_type = "subtopic"
            else:
                return
            parent = self._current_path

        # Prompt for name
        name, ok = QInputDialog.getText(self, f"New {node_type.title()}", "Enter name:")
        if not ok or not name.strip():
            return
        # Prompt for description
        desc, _ = QInputDialog.getText(self, "Description", "Enter short description:", text="")
        # Prompt for details
        details, _ = QInputDialog.getMultiLineText(self, "Details", "Enter detailed info:", "")

        try:
            sc.create_node(node_type, name.strip(), desc.strip(), details, parent_path=parent)
        except Exception as e:
            QMessageBox.critical(self, "Add Child", f"Error creating node:\n{e}")
            return

        self.structure_changed.emit()

    def _on_update(self):
        """
        Update the current node’s name/desc/details in the structure.
        """
        if not self._current_path:
            QMessageBox.warning(self, "Update Node", "No node selected.")
            return

        new_name = self.name_edit.text().strip()
        new_desc = self.desc_edit.text().strip()
        new_details = self.details_edit.toPlainText()

        try:
            sc.update_node(self._current_path, name=new_name, desc=new_desc, details=new_details)
        except Exception as e:
            QMessageBox.critical(self, "Update Node", f"Error updating node:\n{e}")
            return

        self.structure_changed.emit()

    def _on_delete(self):
        """
        Delete the current node and its sub-tree after confirmation.
        """
        if not self._current_path:
            QMessageBox.warning(self, "Delete Node", "No node selected.")
            return

        reply = QMessageBox.question(
            self, "Delete Node",
            f"Delete '{self._current_path[-1]}' and all its children?",
            QMessageBox.Yes | QMessageBox.No
        )
        if reply != QMessageBox.Yes:
            return

        try:
            sc.delete_node(self._current_path)
        except Exception as e:
            QMessageBox.critical(self, "Delete Node", f"Error deleting node:\n{e}")
            return

        # Clear panel
        self._current_path = None
        self.type_label.setText("Node Type: —")
        self.name_edit.clear()
        self.desc_edit.clear()
        self.details_edit.clear()
        self.add_btn.setEnabled(False)

        self.structure_changed.emit()
```

structure_panel_ui.py

Far-right panel for viewing and editing a selected structure node:
  - Name, Description, Details fields
  - Add Child, Update, Delete buttons
  - Emits signals when operations complete, so main UI can refresh
**Classes:** StructurePanel


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\Analyze_All.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get the script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")

        for root, dirs, files in os.walk(start_path):
            # Ensure only directories below the script's location are analyzed
            if not root.startswith(start_path):
                continue  # Skip any directory outside the scope

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and analyze.txt from files list
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file_path = os.path.join(root, f)
                file.write(f"{file_indent}|-- {f}\n")

                # Capture file content
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f_content:
                        file_data = f_content.read()
                        file.write(f"\n{file_indent}    --- File Content ---\n")
                        file.write(f"{file_data}\n")
                        file.write(f"{file_indent}    -------------------\n\n")
                except Exception as e:
                    file.write(f"{file_indent}    [Error reading file: {str(e)}]\n\n")

    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.abspath(os.path.dirname(__file__))  # Get the script's location
    analyze_folders(script_directory)  # Analyze only from script's directory downward
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\Analyze_folders.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")
        
        for root, dirs, files in os.walk(start_path):
            # Skip directories named 'venv'
            if "venv" in root.split(os.sep):
                continue

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and output file from the list of files
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file.write(f"{file_indent}|-- {f}\n")
    
    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.dirname(os.path.abspath(__file__))
    analyze_folders(script_directory)
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\Debate_AI.py`

```python
#!/usr/bin/env python3

import sys
from datetime import datetime

# Correct imports from synced modules
from utils import fetch_openai_models, fetch_ollama_models
from debate_core import DebateWorker, safe_gui_call, cosine_sim, parse_mad_lib

# GUI setup
from PyQt5.QtWidgets import QApplication
from ui_components import DebateAI

if __name__ == "__main__":
    app = QApplication(sys.argv)

    try:
        # Launch GUI
        window = DebateAI()
        window.show()

        # Clean exit hook
        app.aboutToQuit.connect(
            lambda: print(f"[{datetime.now().strftime('%H:%M:%S')}] GUI closed.")
        )

        # Start event loop
        sys.exit(app.exec_())

    except Exception as e:
        # Log GUI launch errors
        with open("error.log", "a", encoding="utf-8") as f:
            f.write(
                f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] "
                f"Error launching GUI: {str(e)}\n"
            )
        sys.exit(1)
```



## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\debate_core.py`

```python
#!/usr/bin/env python3

import os
import json
import time
import asyncio
import shutil
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

import numpy as np
from PyQt5.QtCore import (
    QThread, pyqtSignal, QTimer,
    QStateMachine, QState, QFinalState
)


def cosine_sim(a, b):
    """Compute cosine similarity between two vectors."""
    a, b = np.array(a), np.array(b)
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-8
    return float(np.dot(a, b) / denom)


def parse_mad_lib(text, required_slots=None):
    """
    Extracts slots from a 'Mad Lib'–style internal monologue.
    Returns (valid, slots_dict).
    """
    if required_slots is None:
        required_slots = ["Stance", "reason", "opponent's point", "argument"]
    slots = {slot: None for slot in required_slots}
    for slot in required_slots:
        start_tag = f"[{slot}]"
        start = text.find(start_tag)
        if start != -1:
            content_start = start + len(start_tag)
            # capture until next bracket or end of text
            end = text.find("[", content_start)
            if end == -1:
                end = len(text)
            slots[slot] = text[content_start:end].strip()
    valid = all(slots[slot] is not None for slot in required_slots)
    return valid, slots


class DebateWorker(QThread):
    message_ready    = pyqtSignal(str, str, str)
    status_update    = pyqtSignal(str)
    epoch_complete   = pyqtSignal(int)
    error_occurred   = pyqtSignal(str)
    topic_updated    = pyqtSignal(str, str)
    progress_update  = pyqtSignal(int)

    def __init__(self, parent):
        super().__init__(parent)
        self.parent            = parent
        self.running           = False
        self.executor          = ThreadPoolExecutor(max_workers=4)
        self.loop              = asyncio.new_event_loop()
        self.current_bot       = "Red"
        self.talking_points    = []
        self.current_point_idx = 0
        self.rebuttal_count    = 0
        self.max_rebuttals     = 0
        self.state_machine     = None
        self.deadlock_count    = 0
        self.MAX_DEADLOCKS     = 2

    def start_debate(self):
        self.running = True
        self._setup_state_machine()
        self.start()

    def stop_debate(self):
        self.running = False
        self.executor.shutdown(wait=False)
        if self.loop.is_running():
            self.loop.call_soon_threadsafe(self.loop.stop)
        if self.state_machine and self.state_machine.isRunning():
            self.state_machine.stop()

    def _setup_state_machine(self):
        sm = QStateMachine(self)
        init    = QState(sm)
        rebut   = QState(sm)
        resolve = QState(sm)
        final   = QFinalState(sm)

        init.addTransition(self._init_complete, rebut)
        rebut.addTransition(self._rebuttal_complete, resolve if self.current_point_idx >= len(self.talking_points) - 1 else rebut)
        resolve.addTransition(self._resolution_complete, final)

        init.entered.connect(self._enter_init)
        rebut.entered.connect(self._enter_rebuttal)
        resolve.entered.connect(self._enter_resolution)
        final.entered.connect(self._enter_final)

        sm.setInitialState(init)
        sm.start()
        self.state_machine = sm

    def _init_complete(self):
        return self.current_point_idx > 0

    def _rebuttal_complete(self):
        return self.rebuttal_count >= self.max_rebuttals or self._detect_deadlock()

    def _resolution_complete(self):
        return True

    def _enter_init(self):
        asyncio.run_coroutine_threadsafe(self._initialize_debate(), self.loop)

    def _enter_rebuttal(self):
        asyncio.run_coroutine_threadsafe(self._execute_rebuttal(), self.loop)

    def _enter_resolution(self):
        asyncio.run_coroutine_threadsafe(self._execute_resolution(), self.loop)

    def _enter_final(self):
        asyncio.run_coroutine_threadsafe(self._finalize_epoch(), self.loop)
        self.running = False

    def run(self):
        asyncio.set_event_loop(self.loop)
        try:
            self.loop.run_until_complete(self._debate_flow())
        except Exception as e:
            self.error_occurred.emit(str(e))
        finally:
            self.loop.close()

    async def _debate_flow(self):
        if not await self._validate_preconditions():
            self.error_occurred.emit("Preconditions failed")
            return

        # Prepare epoch directory
        self.parent.epoch += 1
        eid  = f"{self.parent.epoch:03d}"
        edir = os.path.join(self.parent.session_dir, f"epoch_{eid}")
        os.makedirs(edir, exist_ok=True)

        self.status_update.emit(f"Starting Epoch {self.parent.epoch}")
        self.progress_update.emit(0)
        if self.parent.log_path:
            open(self.parent.log_path, "w", encoding="utf-8").close()

        # Arbiter plan
        if self.parent.chk_enable_arbiter.isChecked():
            self.status_update.emit("Arbiter generating talking points...")
            plan = await self._call_arbiter_plan()
            stages = plan.get("stages", [])
            if not stages:
                self.error_occurred.emit("Failed to generate talking points")
                return
            self.talking_points = stages
            self.max_rebuttals  = self.parent.rounds_spin.value()
            with open(os.path.join(edir, "talking_points.json"), "w", encoding="utf-8") as f:
                json.dump({"talking_points": stages, "epoch": self.parent.epoch}, f, indent=2)
            self.message_ready.emit("Arbiter", "Plan", json.dumps(stages))
        # The state machine is already started in _setup_state_machine

        # Wait until debate completes
        while self.state_machine.isRunning():
            await asyncio.sleep(0.1)

        self.epoch_complete.emit(self.parent.epoch)
        self.status_update.emit(f"Epoch {self.parent.epoch} completed")
        self.progress_update.emit(100)

    async def _validate_preconditions(self):
        return bool(self.parent.topic_edit.text().strip()
                    and self.parent.persona_red.toPlainText().strip()
                    and self.parent.persona_blue.toPlainText().strip())

    async def _initialize_debate(self):
        if not self.talking_points:
            self.error_occurred.emit("No talking points available")
            return
        mp = self.talking_points[self.current_point_idx]
        await self._execute_turn(self.current_bot, mp["name"], mp["questions"][0])
        self.current_bot = "Blue" if self.current_bot == "Red" else "Red"
        self.current_point_idx += 1
        self.progress_update.emit(10)

    async def _execute_rebuttal(self):
        if self.current_point_idx >= len(self.talking_points):
            self.error_occurred.emit("No more stages")
            return

        mp = self.talking_points[self.current_point_idx]
        self.status_update.emit(f"Stage: {mp['name']}")
        self.message_ready.emit("System", "Stage", f"--- {mp['name']} ---")

        await self._execute_turn(self.current_bot, mp["name"], mp["questions"][0])
        self.rebuttal_count += 1

        if self.rebuttal_count >= self.max_rebuttals or self._detect_deadlock():
            await self._arbiter_interjection()
            self.rebuttal_count = 0
            self.current_bot = "Red"
            self.current_point_idx += 1
        else:
            self.current_bot = "Blue" if self.current_bot == "Red" else "Red"

        pct = min(50, int((self.current_point_idx / len(self.talking_points)) * 50))
        self.progress_update.emit(pct)

    async def _execute_resolution(self):
        if self.current_point_idx >= len(self.talking_points):
            self.error_occurred.emit("No resolution stage")
            return

        mp = self.talking_points[self.current_point_idx]
        await self._execute_resolution_turn(self.current_bot, mp["name"])
        self.current_bot = "Blue" if self.current_bot == "Red" else "Red"
        self.current_point_idx += 1
        self.progress_update.emit(75)

    async def _execute_turn(self, bot, stage, question):
        try:
            history     = await asyncio.get_event_loop().run_in_executor(self.executor, self.parent._get_history_slice)
            rag_context = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._get_semantic_context(bot))
            persona     = self.parent._get_persona(bot)

            # Internal monologue
            mono_prompt = self._build_monologue_prompt(bot, stage, persona, history, rag_context, question)
            monologue   = await self._call_model(mono_prompt)
            if monologue:
                valid, slots = parse_mad_lib(monologue)
                if not valid:
                    correction = self._correct_mad_lib(monologue, slots)
                    monologue = correction or monologue
                self.message_ready.emit(bot, "Monologue", monologue)
            else:
                raise ValueError("Monologue generation failed")

            # Public response
            pub_prompt = self._build_public_prompt(bot, stage, persona, monologue, history)
            public_msg = await self._call_model(pub_prompt)
            if public_msg:
                await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._log_message(bot, public_msg))
                self.message_ready.emit(bot, "Public", public_msg)
            else:
                raise ValueError("Public response generation failed")

            # TTS
            if ((bot == "Red" and self.parent.chk_enable_tts_red.isChecked())
                or (bot == "Blue" and self.parent.chk_enable_tts_blue.isChecked())):
                await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent.tts.speak(bot, public_msg))

            # Semantic save & contradiction check
            await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._semantic_save(bot, monologue, public_msg, stage))
            await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._check_contradiction(bot, public_msg))

            # ScoreBot
            await self._scorebot_analysis(bot, stage)

        except Exception as e:
            self.error_occurred.emit(f"Turn execution error: {e}")

    async def _execute_resolution_turn(self, bot, stage):
        try:
            history     = await asyncio.get_event_loop().run_in_executor(self.executor, self.parent._get_history_slice)
            rag_context = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._get_semantic_context(bot))
            persona     = self.parent._get_persona(bot)

            res_prompt = self._build_resolution_prompt(bot, stage, persona, history, rag_context)
            monologue  = await self._call_model(res_prompt)
            if monologue:
                valid, slots = parse_mad_lib(monologue, ["Resolution type", "reason", "evidence"])
                if not valid:
                    correction = self._correct_mad_lib(monologue, slots, is_resolution=True)
                    monologue = correction or monologue
                self.message_ready.emit(bot, "Monologue", monologue)
            else:
                raise ValueError("Resolution monologue generation failed")

            pub_prompt = self._build_public_prompt(bot, stage, persona, monologue, history)
            public_msg = await self._call_model(pub_prompt)
            if public_msg:
                quality = await self._evaluate_resolution_quality(bot, public_msg)
                await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    lambda: self.parent._log_message(bot, f"{public_msg} (Quality: {quality:.2f})")
                )
                self.message_ready.emit(bot, "Public", public_msg)
            else:
                raise ValueError("Resolution public response generation failed")

            if ((bot == "Red" and self.parent.chk_enable_tts_red.isChecked())
                or (bot == "Blue" and self.parent.chk_enable_tts_blue.isChecked())):
                await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent.tts.speak(bot, public_msg))

            await asyncio.get_event_loop().run_in_executor(
                self.executor,
                lambda: self.parent._semantic_save(bot, monologue, public_msg, stage, is_resolution=True)
            )

        except Exception as e:
            self.error_occurred.emit(f"Resolution turn error: {e}")

    async def _arbiter_interjection(self):
        # if coherence too low or deadlock, insert clarifications
        cr = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._calculate_coherence("Red"))
        cb = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._calculate_coherence("Blue"))

        if cr < 0.3 or cb < 0.3 or self.deadlock_count >= self.MAX_DEADLOCKS:
            self._adjust_talking_points(cr, cb)
            self.deadlock_count = 0

        if self.current_point_idx < len(self.talking_points):
            nxt = self.talking_points[self.current_point_idx]
            q   = nxt["questions"][0]
            self.message_ready.emit("Arbiter", "Interjection", f"Next question: {q} (Adjusted for coherence)")
            self.current_bot = "Red"

    def _detect_deadlock(self):
        cr = self.parent._calculate_coherence("Red")
        cb = self.parent._calculate_coherence("Blue")
        if cr < 0.2 or cb < 0.2:
            self.deadlock_count += 1
            return True
        return False

    def _adjust_talking_points(self, cr, cb):
        idx = self.current_point_idx + 1
        if cr < 0.3:
            self.talking_points.insert(idx, {
                "name": f"Clarify Red ({self.rebuttal_count+1})",
                "questions": [f"Red, clarify your stance (coherence {cr:.2f})"],
                "outcomes": "Red clarifies",
                "max_duration": 300
            })
        if cb < 0.3:
            self.talking_points.insert(idx, {
                "name": f"Clarify Blue ({self.rebuttal_count+1})",
                "questions": [f"Blue, clarify your stance (coherence {cb:.2f})"],
                "outcomes": "Blue clarifies",
                "max_duration": 300
            })

    def _build_monologue_prompt(self, bot, stage, persona, history, rag, question):
        opp = "Blue" if bot == "Red" else "Red"
        opp_hist = "\n".join([m for m in history.split("\n") if f"[{opp}]" in m])
        return (
            f"[INTERNAL MONOLOGUE - {bot}]\n"
            f"Stage: {stage}\nPersona: {persona}\n"
            f"Arbiter Question: {question}\n\n"
            f"Recent History:\n{history}\n\n"
            f"Opponent History:\n{opp_hist}\n\n"
            f"Relevant Context (RAG):\n{rag}\n\n"
            "Generate your reasoning in format: "
            "[Stance] because [reason], countering [opponent's point] with [argument]."
        )

    def _build_public_prompt(self, bot, stage, persona, monologue, history):
        return (
            f"[PUBLIC RESPONSE - {bot}]\n"
            f"Stage: {stage}\nPersona: {persona}\n\n"
            "Internal Analysis:\n" f"{monologue}\n\n"
            "Recent History:\n" f"{history}\n\n"
            "Present a clear, persuasive argument engaging the opponent."
        )

    def _build_resolution_prompt(self, bot, stage, persona, history, rag):
        return (
            f"[INTERNAL MONOLOGUE - {bot} - RESOLUTION]\n"
            f"Stage: {stage}\nPersona: {persona}\n\n"
            "Recent History:\n" f"{history}\n\n"
            f"Relevant Context (RAG):\n{rag}\n\n"
            "Propose a resolution: [Resolution type] because [reason], supported by [evidence]."
        )

    def _correct_mad_lib(self, text, slots, is_resolution=False):
        req = ["Resolution type", "reason", "evidence"] if is_resolution else ["Stance", "reason", "opponent's point", "argument"]
        missing = [s for s in req if not slots.get(s)]
        if missing:
            prompt = (
                f"Monologue is missing slots {missing}. Correct to fit format.\n"
                f"Text: {text}\nProvide corrected version with all slots."
            )
            return self.parent._call_model(prompt)
        return None

    async def _call_arbiter_plan(self):
        topic = self.parent.topic_edit.text().strip()
        prev = ""
        if self.parent.epoch > 1:
            prev_file = os.path.join(
                self.parent.session_dir,
                f"epoch_{self.parent.epoch-1:03d}",
                f"summary_{self.parent.epoch-1:03d}.txt"
            )
            if os.path.exists(prev_file):
                with open(prev_file, "r", encoding="utf-8") as f:
                    prev = f.read()

        prompt = (
            f"As Arbiter AI, create a debate plan for Epoch {self.parent.epoch}.\n"
            f"Topic: {topic}\nPrior Summary: {prev}\n\n"
            "Return JSON: stages with name, questions[], outcomes, max_duration."
        )
        text = await self._call_model(prompt)
        if not text.strip():
            self.error_occurred.emit("Arbiter plan failed")
            return {"stages": []}
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            self.error_occurred.emit("Invalid JSON from Arbiter plan")
            return {"stages": []}

    async def _call_model(self, prompt):
        try:
            res = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._call_model(prompt)
            )
            return res.strip() if res and res.strip() else self._generate_fallback_response(prompt)
        except Exception as e:
            self.error_occurred.emit(f"Model call error: {e}")
            return self._generate_fallback_response(prompt)

    def _generate_fallback_response(self, prompt):
        return f"[Fallback] Unable to process prompt: {prompt[:50]}..."

    async def _scorebot_analysis(self, bot, stage):
        try:
            cr = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._calculate_coherence(bot))
            ct = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._count_contradictions(bot))
            rl = await asyncio.get_event_loop().run_in_executor(self.executor, lambda: self.parent._calculate_relevance(bot))
            weight = 2.0 if "Closing" in stage else 1.0
            score = (cr*0.4 + (1 - ct/5)*0.3 + rl*0.3) * weight
            entry = {"bot": bot, "stage": stage, "coherence": cr, "contradictions": ct,
                     "relevance": rl, "score": score, "timestamp": time.time()}
            self.parent.score_updates.append(entry)

            color = "#FF4444" if bot == "Red" else "#4444FF"
            text = (
                f"<span style='color:{color}'>{stage}/{bot}</span> → "
                f"Score: {score:.2f}, Coherence: {cr:.2f}, Contradictions: {ct}, Relevance: {rl:.2f}"
            )
            safe_gui_call(self.parent._update_scorebot_display, text)
        except Exception as e:
            self.error_occurred.emit(f"ScoreBot error: {e}")

    async def _evaluate_resolution_quality(self, bot, text):
        prompt = (
            "Evaluate this resolution on 0–1 scale (clarity, evidence, negotiation):\n" f"{text}"
        )
        val = await self._call_model(prompt)
        try:
            return float(val)
        except (ValueError, TypeError):
            return 0.5

    async def _finalize_epoch(self):
        """Save transcripts, embeddings, scores, and summaries at epoch end."""
        epoch_id  = f"{self.parent.epoch:03d}"
        epoch_dir = os.path.join(self.parent.session_dir, f"epoch_{epoch_id}")

        # Transcript
        if self.parent.log_path and os.path.exists(self.parent.log_path):
            shutil.copy(self.parent.log_path, os.path.join(epoch_dir, f"transcript_{epoch_id}.txt"))

        # Embeddings
        if getattr(self.parent, "sem_model", None) and self.parent.log_path:
            try:
                with open(self.parent.log_path, "r", encoding="utf-8") as f:
                    txt = f.read()
                emb = self.parent.sem_model.encode(txt).tolist()
                with open(os.path.join(epoch_dir, f"vectors_{epoch_id}.json"), "w", encoding="utf-8") as f:
                    json.dump(emb, f)
            except Exception as e:
                self.status_update.emit(f"Vector save error: {e}")

        # Scores
        if self.parent.score_updates:
            with open(os.path.join(epoch_dir, f"scores_{epoch_id}.json"), "w", encoding="utf-8") as f:
                json.dump(self.parent.score_updates, f, indent=2)

        # Arbiter summary
        if self.parent.chk_enable_arbiter.isChecked():
            summary = await self._generate_arbiter_summary(self.parent.epoch, self.talking_points)
            with open(os.path.join(epoch_dir, f"summary_{epoch_id}.txt"), "w", encoding="utf-8") as f:
                f.write(summary)
            self.message_ready.emit("Arbiter", "Summary", summary)

        # Diagnostics
        with open(os.path.join(epoch_dir, "diagnostics.log"), "w", encoding="utf-8") as f:
            f.write(f"Epoch {epoch_id} completed at {datetime.now()}\n")
            f.write(f"Talking Points: {json.dumps(self.talking_points, indent=2)}\n")
            last_scores = self.parent.score_updates[-2:] if len(self.parent.score_updates) >= 2 else self.parent.score_updates
            f.write(f"Final Scores: {json.dumps(last_scores, indent=2)}\n")

    async def _generate_arbiter_summary(self, epoch_id, stages):
        try:
            transcript = ""
            if self.parent.log_path and os.path.exists(self.parent.log_path):
                with open(self.parent.log_path, "r", encoding="utf-8") as f:
                    transcript = f.read()
            prompt = (
                f"As Arbiter AI, summarize Epoch {epoch_id}.\n"
                f"Transcript (last 2000 chars):\n{transcript[-2000:]}\n\n"
                "Provide key arguments, agreements, contradictions, and recommendations in markdown."
            )
            return self.parent._call_model(prompt)
        except Exception as e:
            return f"Summary generation error: {e}"


def safe_gui_call(func, *args, **kwargs):
    """Schedule a GUI update from any thread."""
    QTimer.singleShot(0, lambda: func(*args, **kwargs))
```

**Classes:** DebateWorker
**Functions:** cosine_sim(a, b), parse_mad_lib(text, required_slots), safe_gui_call(func)


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\monitor_debate.py`

```python
# monitor_debate.py

import time
from PyQt5.QtCore import QThread, pyqtSignal

class DebateMonitor(QThread):
    """
    A no-op monitor so that ui_components.py can import DebateMonitor
    without error.  You can expand this later with real monitoring logic.
    """
    warning = pyqtSignal(str)

    def __init__(self, parent=None):
        super().__init__(parent)
        self._running = True

    def run(self):
        # Example placeholder loop
        while self._running:
            # you could inspect parent.semantic_store or other state here
            time.sleep(1)

    def stop(self):
        self._running = False
```

**Classes:** DebateMonitor


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\Quick_error.py`

```python
import subprocess
import time
import os
import sys

# Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DEBATE_SCRIPT = os.path.join(BASE_DIR, "Debate_AI.py")
LOG_FILE = os.path.join(BASE_DIR, "quick_error.log")

def monitor_debate():
    # Start log
    with open(LOG_FILE, 'w', encoding='utf-8') as log:
        log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Monitoring started for Debate_AI.py\n")

    # Launch Debate_AI.py with the same Python interpreter
    try:
        process = subprocess.Popen(
            [sys.executable, DEBATE_SCRIPT],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=BASE_DIR
        )

        # Read and log output/error
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            while True:
                out = process.stdout.readline()
                err = process.stderr.readline()

                if err:
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                    line = f"[{timestamp}] ERROR: {err.strip()}\n"
                    print(line, end='')
                    log.write(line)
                    log.flush()

                if out:
                    print(f"[OUTPUT] {out.strip()}")

                if process.poll() is not None:
                    break

            # Final exit log
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
            log.write(f"[{timestamp}] Debate_AI.py has exited with code {process.returncode}.\n")

    except Exception as exc:
        with open(LOG_FILE, 'a', encoding='utf-8') as log:
            log.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Exception: {exc}\n")

if __name__ == "__main__":
    print("Monitoring Debate_AI.py for errors...")
    monitor_debate()
    print("Debate_AI.py has stopped. Monitor exiting.")
```

**Functions:** monitor_debate()


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\ui_components.py`

```python
#!/usr/bin/env python3
import os
import sys
import json
import shutil
import time
from datetime import datetime
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget,
    QVBoxLayout, QHBoxLayout, QLabel,
    QLineEdit, QTextEdit, QPushButton,
    QComboBox, QSlider, QDockWidget,
    QCheckBox, QListWidget, QMessageBox,
    QSpinBox, QStatusBar, QFileDialog,
    QProgressBar
)
from PyQt5.QtGui import QColor, QPalette, QTextCharFormat, QTextCursor
from PyQt5.QtCore import Qt, QTimer

# Import TTS and Monitor (assumed available in environment)
try:
    from tts import TTSManager
    from monitor_debate import DebateMonitor
except ImportError as e:
    QMessageBox.critical(None, "Error", f"Import error: {e}")
    sys.exit(1)

# Import core debate logic (to be moved to debate_core.py in full modularization)
from debate_core import DebateWorker, safe_gui_call, cosine_sim, parse_mad_lib
import utils

class DebateAI(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate AI - Enhanced System")
        self.resize(1600, 1000)
        
        # State variables
        self.session = None
        self.session_dir = None
        self.epoch = 0
        self.log_path = None
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        self.running = False
        
        # Initialize semantic model (delayed to avoid early UI issues)
        self.sem_model = None
        self.tts = None
        self.monitor = None
        
        # Load debate formats
        self.formats = self._load_debate_formats()
        
        # Text formats
        self.red_format = QTextCharFormat()
        self.red_format.setForeground(QColor("#FF4444"))
        self.blue_format = QTextCharFormat()
        self.blue_format.setForeground(QColor("#4444FF"))
        self.arbiter_format = QTextCharFormat()
        self.arbiter_format.setForeground(QColor("#800080"))
        self.system_format = QTextCharFormat()
        self.system_format.setForeground(QColor("#666666"))
        self.resolution_format = QTextCharFormat()
        self.resolution_format.setForeground(QColor("#006400"))
        
        # Build UI
        self._build_ui()
        self._apply_theme()
        self._connect_signals()
        self._initialize_tools()  # Delayed tool initialization

    def _load_debate_formats(self):
        formats = []
        formats_dir = "formats"
        os.makedirs(formats_dir, exist_ok=True)
        
        self_diag_file = os.path.join(formats_dir, "self_diagnostic_debate.json")
        if not os.path.exists(self_diag_file):
            default_format = {
                "name": "Self-Diagnostic Debate",
                "description": "Debate about improving the Debate_AI system itself",
                "stages": [
                    {"bot": "Red", "type": "Architectural Analysis", "max_duration": 300},
                    {"bot": "Blue", "type": "Feature Evaluation", "max_duration": 300},
                    {"bot": "Red", "type": "Failure Mode Review", "max_duration": 300},
                    {"bot": "Blue", "type": "Innovation Proposals", "max_duration": 300},
                    {"bot": "Red", "type": "Implementation Strategy", "max_duration": 300},
                    {"bot": "Blue", "type": "Summary & Recommendations", "max_duration": 300}
                ]
            }
            with open(self_diag_file, "w", encoding="utf-8") as f:
                json.dump(default_format, f, indent=2)
        
        for filename in os.listdir(formats_dir):
            if filename.endswith(".json"):
                try:
                    with open(os.path.join(formats_dir, filename), "r", encoding="utf-8") as f:
                        format_data = json.load(f)
                        formats.append(format_data)
                except Exception as e:
                    print(f"Error loading format {filename}: {e}")
        
        return formats
    
    def _build_ui(self):
        self.chat = QTextEdit()
        self.chat.setReadOnly(True)
        self.chat.setStyleSheet("background-color: #E6C8E6; color: #4A0E4E; font-size: 14px;")
        self.setCentralWidget(self.chat)
        
        self._create_monologue_docks()
        self._create_persona_docks()
        self._create_control_docks()
        self._create_analysis_docks()
        self._create_session_docks()
        
        self.status_bar = QStatusBar()
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        self.progress_bar.setMaximumWidth(200)
        self.resolution_label = QLabel("Resolution Status: Pending")
        self.resolution_label.setStyleSheet("color: #006400;")
        self.status_bar.addWidget(self.resolution_label)
        self.status_bar.addPermanentWidget(self.progress_bar)
        self.setStatusBar(self.status_bar)
    
    def _create_monologue_docks(self):
        dock_red_mono = QDockWidget("Red Monologue", self)
        self.red_mono = QTextEdit()
        self.red_mono.setReadOnly(True)
        self.red_mono.setStyleSheet("background-color: #FFE6E6; color: #8B0000; font-size: 14px;")
        dock_red_mono.setWidget(self.red_mono)
        self.addDockWidget(Qt.LeftDockWidgetArea, dock_red_mono)
        
        dock_blue_mono = QDockWidget("Blue Monologue", self)
        self.blue_mono = QTextEdit()
        self.blue_mono.setReadOnly(True)
        self.blue_mono.setStyleSheet("background-color: #E6F3FF; color: #000080; font-size: 14px;")
        dock_blue_mono.setWidget(self.blue_mono)
        self.addDockWidget(Qt.RightDockWidgetArea, dock_blue_mono)
    
    def _create_persona_docks(self):
        dock_red_persona = QDockWidget("Red Persona", self)
        widget_red = QWidget()
        layout_red = QVBoxLayout()
        self.persona_red = QTextEdit()
        self.persona_red.setMaximumHeight(100)
        self.persona_red.setStyleSheet("background-color: #FFF0F0; color: #8B0000; font-size: 14px;")
        layout_red.addWidget(QLabel("Red Bot Persona:"))
        layout_red.addWidget(self.persona_red)
        widget_red.setLayout(layout_red)
        dock_red_persona.setWidget(widget_red)
        self.addDockWidget(Qt.LeftDockWidgetArea, dock_red_persona)
        
        dock_blue_persona = QDockWidget("Blue Persona", self)
        widget_blue = QWidget()
        layout_blue = QVBoxLayout()
        self.persona_blue = QTextEdit()
        self.persona_blue.setMaximumHeight(100)
        self.persona_blue.setStyleSheet("background-color: #E6FFFF; color: #000080; font-size: 14px;")
        layout_blue.addWidget(QLabel("Blue Bot Persona:"))
        layout_blue.addWidget(self.persona_blue)
        widget_blue.setLayout(layout_blue)
        dock_blue_persona.setWidget(widget_blue)
        self.addDockWidget(Qt.RightDockWidgetArea, dock_blue_persona)
    
    def _create_control_docks(self):
        dock_struct = QDockWidget("Debate Structure", self)
        widget_struct = QWidget()
        layout_struct = QVBoxLayout()
        self.topic_edit = QLineEdit()
        self.topic_edit.setPlaceholderText("Enter debate topic or resolution...")
        layout_struct.addWidget(QLabel("Topic / Resolution"))
        layout_struct.addWidget(self.topic_edit)
        self.format_combo = QComboBox()
        self.format_combo.addItem("Classic Debate", None)
        for fmt in self.formats:
            self.format_combo.addItem(fmt["name"], fmt)
        layout_struct.addWidget(QLabel("Debate Format"))
        layout_struct.addWidget(self.format_combo)
        self.rounds_spin = QSpinBox()
        self.rounds_spin.setRange(0, 10)
        self.rounds_spin.setValue(2)
        layout_struct.addWidget(QLabel("Rebuttal Rounds"))
        layout_struct.addWidget(self.rounds_spin)
        self.chk_enable_arbiter = QCheckBox("Enable Arbiter AI")
        self.chk_enable_scorebot = QCheckBox("Enable ScoreBot")
        self.chk_enable_selfdiag = QCheckBox("Enable Self-Diagnostic Mode")
        layout_struct.addWidget(self.chk_enable_arbiter)
        layout_struct.addWidget(self.chk_enable_scorebot)
        layout_struct.addWidget(self.chk_enable_selfdiag)
        widget_struct.setLayout(layout_struct)
        dock_struct.setWidget(widget_struct)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_struct)
        
        dock_models = QDockWidget("Providers & Models", self)
        widget_models = QWidget()
        layout_models = QVBoxLayout()
        self.provider_combo = QComboBox()
        self.provider_combo.addItems(["OpenAI", "Ollama"])
        layout_models.addWidget(QLabel("Provider:"))
        layout_models.addWidget(self.provider_combo)
        self.model_combo = QComboBox()
        layout_models.addWidget(QLabel("Model:"))
        layout_models.addWidget(self.model_combo)
        btn_refresh = QPushButton("Refresh Models")
        layout_models.addWidget(btn_refresh)
        widget_models.setLayout(layout_models)
        dock_models.setWidget(widget_models)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_models)
        
        dock_settings = QDockWidget("Model Settings", self)
        widget_settings = QWidget()
        layout_settings = QVBoxLayout()
        self.temp_slider = QSlider(Qt.Horizontal)
        self.temp_slider.setRange(0, 100)
        self.temp_slider.setValue(70)
        self.temp_label = QLabel("Temperature: 0.70")
        layout_settings.addWidget(self.temp_label)
        layout_settings.addWidget(self.temp_slider)
        self.token_spin = QSpinBox()
        self.token_spin.setRange(64, 8192)
        self.token_spin.setValue(512)
        layout_settings.addWidget(QLabel("Max Tokens:"))
        layout_settings.addWidget(self.token_spin)
        widget_settings.setLayout(layout_settings)
        dock_settings.setWidget(widget_settings)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_settings)
        
        dock_controls = QDockWidget("Controls & TTS", self)
        widget_controls = QWidget()
        layout_controls = QVBoxLayout()
        layout_buttons = QHBoxLayout()
        self.btn_start = QPushButton("Start Debate")
        self.btn_pause = QPushButton("Pause Debate")
        self.btn_reset = QPushButton("Reset Session")
        self.btn_start.setStyleSheet("background-color: #90EE90; color: #000000;")
        self.btn_pause.setStyleSheet("background-color: #FFD700; color: #000000;")
        self.btn_reset.setStyleSheet("background-color: #FF4040; color: #FFFFFF;")
        layout_buttons.addWidget(self.btn_start)
        layout_buttons.addWidget(self.btn_pause)
        layout_buttons.addWidget(self.btn_reset)
        layout_tts = QVBoxLayout()
        tts_row1 = QHBoxLayout()
        self.chk_enable_tts_red = QCheckBox("Enable TTS Red")
        self.chk_enable_tts_blue = QCheckBox("Enable TTS Blue")
        self.voice_red = QComboBox()
        self.voice_blue = QComboBox()
        if self.tts:  # Check if TTS is initialized
            for vid, name in self.tts.list_voices():
                self.voice_red.addItem(name, vid)
                self.voice_blue.addItem(name, vid)
        self.btn_test_red = QPushButton("▶ Test Red Voice")
        self.btn_test_blue = QPushButton("▶ Test Blue Voice")
        tts_row1.addWidget(self.chk_enable_tts_red)
        tts_row1.addWidget(self.voice_red)
        tts_row1.addWidget(self.btn_test_red)
        tts_row1.addWidget(self.chk_enable_tts_blue)
        tts_row1.addWidget(self.voice_blue)
        tts_row1.addWidget(self.btn_test_blue)
        tts_row2 = QHBoxLayout()
        self.rate_slider = QSlider(Qt.Horizontal)
        self.rate_slider.setRange(100, 300)
        self.rate_slider.setValue(150)
        self.rate_label = QLabel("TTS Rate: 150")
        tts_row2.addWidget(self.rate_label)
        tts_row2.addWidget(self.rate_slider)
        self.volume_slider = QSlider(Qt.Horizontal)
        self.volume_slider.setRange(0, 100)
        self.volume_slider.setValue(80)
        self.volume_label = QLabel("TTS Volume: 0.80")
        tts_row2.addWidget(self.volume_label)
        tts_row2.addWidget(self.volume_slider)
        layout_tts.addLayout(tts_row1)
        layout_tts.addLayout(tts_row2)
        layout_controls.addLayout(layout_buttons)
        layout_controls.addLayout(layout_tts)
        widget_controls.setLayout(layout_controls)
        dock_controls.setWidget(widget_controls)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_controls)
        
        self.tabifyDockWidget(dock_models, dock_settings)
        self.tabifyDockWidget(dock_models, dock_controls)
        dock_models.raise_()
    
    def _create_analysis_docks(self):
        dock_topics = QDockWidget("Topics & Subtopics", self)
        self.idx_list = QListWidget()
        self.idx_list.setStyleSheet("background-color: #F0F0F0; color: #000000;")
        dock_topics.setWidget(self.idx_list)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_topics)
        
        dock_score = QDockWidget("ScoreBot Analysis", self)
        self.score_list = QListWidget()
        self.score_list.setStyleSheet("background-color: #F0F0F0; color: #000000;")
        dock_score.setWidget(self.score_list)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_score)
        
        dock_summary = QDockWidget("Arbiter Summary", self)
        self.summary_panel = QTextEdit()
        self.summary_panel.setReadOnly(True)
        self.summary_panel.setStyleSheet("background-color: #F0E6FF; color: #4A0E4E; font-size: 14px;")
        dock_summary.setWidget(self.summary_panel)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_summary)
    
    def _create_session_docks(self):
        dock_session = QDockWidget("Session Workflows", self)
        widget_session = QWidget()
        layout_session = QHBoxLayout()
        self.btn_new = QPushButton("New Session")
        self.btn_save = QPushButton("Save Session")
        self.btn_load = QPushButton("Load Session")
        self.btn_export = QPushButton("Export Transcript")
        for btn in (self.btn_new, self.btn_save, self.btn_load, self.btn_export):
            btn.setStyleSheet("background-color: #D3D3D3; color: #000000;")
            layout_session.addWidget(btn)
        widget_session.setLayout(layout_session)
        dock_session.setWidget(widget_session)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_session)
    
    def _apply_theme(self):
        palette = self.palette()
        palette.setColor(QPalette.Window, QColor("#F0F0F0"))
        self.setPalette(palette)
    
    def _connect_signals(self):
        self.format_combo.currentIndexChanged.connect(self._on_format_change)
        self.chk_enable_selfdiag.stateChanged.connect(self._apply_selfdiag_defaults)
        self.provider_combo.currentTextChanged.connect(self._refresh_model_list)
        self.temp_slider.valueChanged.connect(
            lambda v: self.temp_label.setText(f"Temperature: {v/100:.2f}")
        )
        self.btn_new.clicked.connect(self.new_session)
        self.btn_save.clicked.connect(self.save_session)
        self.btn_load.clicked.connect(self.load_session)
        self.btn_export.clicked.connect(self.export_transcript)
        self.btn_start.clicked.connect(self.start_debate)
        self.btn_pause.clicked.connect(self.pause_debate)
        self.btn_reset.clicked.connect(self.reset_debate)
        self.btn_test_red.clicked.connect(self._test_red_voice)
        self.btn_test_blue.clicked.connect(self._test_blue_voice)
        self.voice_red.currentIndexChanged.connect(
            lambda: self.tts and self.tts.set_bot_voice("Red", self.voice_red.currentData())
        )
        self.voice_blue.currentIndexChanged.connect(
            lambda: self.tts and self.tts.set_bot_voice("Blue", self.voice_blue.currentData())
        )
        self.rate_slider.valueChanged.connect(
            lambda v: self._update_tts_rate(v)
        )
        self.volume_slider.valueChanged.connect(
            lambda v: self._update_tts_volume(v)
        )
    
    def _update_tts_rate(self, value):
        self.rate_label.setText(f"TTS Rate: {value}")
        if self.tts:
            try:
                self.tts.set_rate(value)
            except Exception as e:
                self._update_status(f"TTS rate update error: {e}")
    
    def _update_tts_volume(self, value):
        self.volume_label.setText(f"TTS Volume: {value/100:.2f}")
        if self.tts:
            try:
                self.tts.set_volume(value / 100.0)
            except Exception as e:
                self._update_status(f"TTS volume update error: {e}")
    
    def _test_red_voice(self):
        if self.tts:
            try:
                self.tts.speak("Red", "This is a test of the Red bot's voice.")
            except Exception as e:
                self._update_status(f"Red TTS test error: {e}")
        else:
            self._update_status("TTS not available")
    
    def _test_blue_voice(self):
        if self.tts:
            try:
                self.tts.speak("Blue", "This is a test of the Blue bot's voice.")
            except Exception as e:
                self._update_status(f"Blue TTS test error: {e}")
        else:
            self._update_status("TTS not available")
    
    def _initialize_tools(self):
        # Initialize semantic model
        try:
            from sentence_transformers import SentenceTransformer
            self.sem_model = SentenceTransformer("all-MiniLM-L6-v2")
        except ImportError:
            print("SentenceTransformer not available - semantic features disabled")
            self.sem_model = None
        
        # Initialize TTS
        try:
            self.tts = TTSManager()
            if self.tts:
                self._update_tts_rate(150)
                self._update_tts_volume(80)
                for vid, name in self.tts.list_voices():
                    self.voice_red.addItem(name, vid)
                    self.voice_blue.addItem(name, vid)
        except Exception as e:
            print(f"TTS initialization error: {e}")
            self.tts = None
            QMessageBox.warning(self, "TTS Error", "TTS failed to initialize. TTS features disabled.")
        
        # Initialize defaults after tools
        self._initialize_defaults()
    
    def _initialize_defaults(self):
        self._refresh_model_list()
        self._on_format_change(self.format_combo.currentIndex())
    
    def _on_format_change(self, idx):
        if self.format_combo.currentText() == "Classic Debate":
            self.topic_edit.setText(
                "The role of political ideology in shaping scientific consensus."
            )
            self.persona_red.setPlainText(
                "You are RED, a conservative physicist component of Debate_AI. "
                "You prioritize rigorous empirical validation and proven frameworks."
            )
            self.persona_blue.setPlainText(
                "You are BLUE, a progressive physicist component of Debate_AI. "
                "You champion innovative hypotheses and adaptive experimentation."
            )
    
    def _apply_selfdiag_defaults(self, state):
        if state == Qt.Checked:
            self.topic_edit.setText(
                "Self-Diagnostic Analysis: Improve Debate_AI's architecture, UX, and AI-driven debate flow."
            )
            self.persona_red.setPlainText(
                "You are RED, an engineer subsystem of Debate_AI tasked with diagnosing system architecture and logic integrity."
            )
            self.persona_blue.setPlainText(
                "You are BLUE, an innovation subsystem of Debate_AI focused on feature expansion and UX enhancement."
            )
            idx = self.format_combo.findText("Self-Diagnostic Debate")
            if idx != -1:
                self.format_combo.setCurrentIndex(idx)
        else:
            self.persona_red.clear()
            self.persona_blue.clear()
    
    def _refresh_model_list(self):
        from debate_core import fetch_openai_models, fetch_ollama_models
        prov = self.provider_combo.currentText()
        self.model_combo.clear()
        self._update_status(f"Refreshing {prov} models...")
        if prov == "OpenAI":
            for m in fetch_openai_models():
                self.model_combo.addItem(m, ("openai", m))
        else:
            for m in fetch_ollama_models():
                self.model_combo.addItem(m, ("ollama", m))
        self._update_status(f"{prov} models updated")
    
    def new_session(self):
        if not self.sem_model:
            self._update_status("Semantic model unavailable, session creation limited.")
            return
        ts = int(time.time())
        self.session = f"sess_{ts}"
        self.session_dir = os.path.join("sessions", self.session)
        os.makedirs(self.session_dir, exist_ok=True)
        os.makedirs(os.path.join(self.session_dir, "backups"), exist_ok=True)
        self.log_path = os.path.join(self.session_dir, "log.txt")
        self.epoch = 0
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        self._update_status(f"New session created: {self.session}")
    
    def save_session(self):
        if not self.session:
            QMessageBox.warning(self, "No Session", "Create or load a session first.")
            return
        with open(os.path.join(self.session_dir, "index.json"), "w", encoding="utf-8") as f:
            json.dump({
                "semantic_store": self.semantic_store,
                "topic_index": self.topic_index,
                "score_updates": self.score_updates
            }, f, indent=2)
        self._update_status("Session saved")
    
    def load_session(self):
        path = QFileDialog.getExistingDirectory(self, "Select Session Folder", "sessions")
        if not path:
            return
        self.session = os.path.basename(path)
        self.session_dir = path
        self.log_path = os.path.join(path, "log.txt")
        try:
            with open(os.path.join(path, "index.json"), "r", encoding="utf-8") as f:
                data = json.load(f)
            self.semantic_store = data.get("semantic_store", {"Red": [], "Blue": []})
            self.topic_index = data.get("topic_index", [])
            self.score_updates = data.get("score_updates", [])
        except:
            self.semantic_store = {"Red": [], "Blue": []}
            self.topic_index = []
            self.score_updates = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        for topic in self.topic_index:
            self.idx_list.addItem(f"{topic['bot']}: {topic['text']}")
        for score in self.score_updates:
            color = "#FF4444" if score['bot'] == "Red" else "#4444FF"
            self.score_list.addItem(
                f"<span style='color:{color}'>{score['stage']}/{score['bot']}</span> → "
                f"Score: {score.get('score', 0):.2f}, Coherence: {score['coherence']:.2f}, Contradictions: {score['contradictions']}, "
                f"Relevance: {score['relevance']:.2f}"
            )
        self._update_status(f"Loaded session: {self.session}")
    
    def export_transcript(self):
        if not self.log_path or not os.path.exists(self.log_path):
            QMessageBox.warning(self, "No Transcript", "No transcript to export.")
            return
        dest, _ = QFileDialog.getSaveFileName(self, "Export Transcript", "debate.txt", "Text Files (*.txt)")
        if dest:
            shutil.copy(self.log_path, dest)
            self._update_status("Transcript exported")
    
    def start_debate(self):
        if not self.session:
            QMessageBox.warning(self, "Error", "Create or load a session first.")
            return
        if self.running:
            return
        
        self.running = True
        if self.monitor:
            self.monitor.log_path = self.log_path
            if not self.monitor.isRunning():
                self.monitor.start()
        
        if self.chk_enable_selfdiag.isChecked():
            try:
                self._run_project_analysis()
            except Exception as e:
                self._handle_error(f"Project analysis error: {str(e)}", recover=True)
        
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.worker = DebateWorker(self)
        self.worker.message_ready.connect(self._handle_message)
        self.worker.status_update.connect(self._update_status)
        self.worker.epoch_complete.connect(self._on_epoch_complete)
        self.worker.error_occurred.connect(self._handle_error)
        self.worker.topic_updated.connect(self._update_topic_index)
        self.worker.progress_update.connect(self._update_progress)
        self.worker.start_debate()
    
    def pause_debate(self):
        self.running = False
        if self.monitor and self.monitor.isRunning():
            self.monitor.stop()
        if hasattr(self, 'worker'):
            self.worker.stop_debate()
        self.progress_bar.setVisible(False)
        if self.tts:
            self.tts.stop()
        self._update_status("Debate paused")
    
    def reset_debate(self):
        self.pause_debate()
        if self.session_dir and os.path.exists(self.session_dir):
            shutil.rmtree(self.session_dir)
        self.session = None
        self.epoch = 0
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        self._update_status("Session reset")
    
    def _run_project_analysis(self):
        if not self.sem_model:
            raise ValueError("Semantic model not available for analysis")
        root = os.path.dirname(__file__)
        for subdir, _, files in os.walk(root):
            if subdir.endswith("sessions") or subdir.endswith("formats"):
                continue
            for fname in files:
                if fname.endswith(".py"):
                    full_path = os.path.join(subdir, fname)
                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        summary = self._call_model(
                            f"Summarize the structure and purpose of {fname} in two sentences.\n\n{content}"
                        )
                        for bot in ("Red", "Blue"):
                            self._semantic_save(bot, summary, "", "Diagnostic", is_diagnostic=True)
                        ts = time.strftime("%H:%M:%S")
                        safe_gui_call(self.red_mono.append, f"{ts} [Analysis] {fname}: {summary}")
                        safe_gui_call(self.blue_mono.append, f"{ts} [Analysis] {fname}: {summary}")
                    except Exception as e:
                        self._update_status(f"Error analyzing {fname}: {str(e)}")
    
    def _handle_message(self, bot, msg_type, message):
        ts = time.strftime("%H:%M:%S")
        if msg_type == "Monologue":
            target = self.red_mono if bot == "Red" else self.blue_mono
            safe_gui_call(target.append, f"{ts} [Monologue] {message}")
        elif msg_type == "Public":
            fmt = (self.resolution_format if "Closing" in self.worker.talking_points[self.worker.current_point_idx - 1]["name"]
                   else self.red_format if bot == "Red" else self.blue_format)
            safe_gui_call(self._append_chat, f"{ts} [{bot}] {message}", fmt)
            safe_gui_call(self.idx_list.addItem, f"{ts} {bot}: {message[:100]}...")
            if "Closing" in self.worker.talking_points[self.worker.current_point_idx - 1]["name"]:
                safe_gui_call(self.resolution_label.setText, "Resolution Status: In Progress")
        elif msg_type in ["Plan", "Summary", "Interjection"]:
            safe_gui_call(self.summary_panel.append, f"{ts} [{bot} {msg_type}] {message}")
        elif msg_type == "Stage":
            safe_gui_call(self._append_chat, f"{ts} {message}", self.system_format)
    
    def _append_chat(self, text, fmt):
        cursor = self.chat.textCursor()
        cursor.movePosition(QTextCursor.End)
        cursor.insertText(text + "\n", fmt)
        self.chat.setTextCursor(cursor)
        self.chat.ensureCursorVisible()
    
    def _update_status(self, message):
        self.status_bar.showMessage(message, 5000)
    
    def _update_scorebot_display(self, score_text):
        self.score_list.addItem(score_text)
        self.score_list.scrollToBottom()
    
    def _update_topic_index(self, topic, summary):
        self.idx_list.addItem(f"{topic}: {summary}")
        self.idx_list.scrollToBottom()
    
    def _update_progress(self, value):
        self.progress_bar.setValue(value)
        if value == 100:
            self.resolution_label.setText("Resolution Status: Complete")
    
    def _on_monitor_warning(self, msg):
        safe_gui_call(self._append_chat, f"[Monitor] {msg}", self.system_format)
        self._update_status(msg)
    
    def _on_epoch_complete(self, epoch):
        self.running = False
        if self.monitor and self.monitor.isRunning():
            self.monitor.stop()
        self.progress_bar.setValue(100)
        QTimer.singleShot(2000, lambda: self.progress_bar.setVisible(False))
        safe_gui_call(self._append_chat, f"✅ Epoch {epoch} complete", self.system_format)
        self.resolution_label.setText("Resolution Status: Complete")
    
    def _handle_error(self, error, recover=False):
        error_msg = f"Error occurred at {time.strftime('%H:%M:%S')}:\n{error}"
        msg_box = QMessageBox(self)
        msg_box.setWindowTitle("Error")
        msg_box.setText("An error has occurred. Choose an action.")
        msg_box.setDetailedText(error_msg)
        copy_button = msg_box.addButton("Copy", QMessageBox.ActionRole)
        if recover:
            continue_button = msg_box.addButton("Continue", QMessageBox.AcceptRole)
        close_button = msg_box.addButton("Close", QMessageBox.RejectRole)
        msg_box.exec_()
        if msg_box.clickedButton() == copy_button:
            QApplication.clipboard().setText(error_msg)
        elif recover and msg_box.clickedButton() == continue_button:
            self._update_status("Attempting to recover...")
        elif msg_box.clickedButton() == close_button:
            self.close()
    
    def _get_history_slice(self):
        if not self.log_path or not os.path.exists(self.log_path):
            return "No previous history."
        
        try:
            with open(self.log_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
            depth = self.rounds_spin.value() * 2 + 2
            return "".join(lines[-depth:]).strip()
        except:
            return "No previous history."
    
    def _get_semantic_context(self, bot, top_k=3):
        if not self.sem_model or not self.semantic_store.get(bot):
            return "No semantic context available."
        
        try:
            hist_emb = self.sem_model.encode(self._get_history_slice())
            sims = sorted(
                [(cosine_sim(hist_emb, r["emb"]), r["text"]) for r in self.semantic_store[bot] if r.get("emb")],
                key=lambda x: x[0],
                reverse=True
            )
            context = "\n".join(f"{s:.2f}: {t[:200]}..." for s, t in sims[:top_k])
            if self.chk_enable_selfdiag.isChecked():
                diag_sims = sorted(
                    [(cosine_sim(hist_emb, r["emb"]), r["text"]) for r in self.semantic_store[bot] if r.get("is_diagnostic", False) and r.get("emb")],
                    key=lambda x: x[0],
                    reverse=True
                )[:1]
                if diag_sims:
                    context += f"\n[Diagnostic] {diag_sims[0][1][:200]}..."
            return context
        except Exception as e:
            return f"No semantic context available (error: {str(e)})"
    
    def _semantic_save(self, bot, monologue, public_msg, stage_type, is_diagnostic=False, is_resolution=False):
        if not self.sem_model:
            return
        
        try:
            if is_resolution:
                content_type = "resolution"
                prompt = f"Convert the following resolution into a Q/A format:\n\n{public_msg}"
                qa_content = self._call_model(prompt)
            elif "Closing" in stage_type:
                content_type = "conclusion"
                prompt = f"Convert the following conclusion into a Q/A format:\n\n{public_msg}"
                qa_content = self._call_model(prompt)
            else:
                content_type = "speculation" if "Rebuttal" in stage_type or "Opening" in stage_type else "raw"
                qa_content = public_msg if content_type == "raw" else monologue
            
            embedding = self.sem_model.encode(qa_content).tolist()
            record = {
                "time": time.time(),
                "text": qa_content,
                "raw_text": public_msg if content_type == "raw" else None,
                "type": content_type,
                "is_diagnostic": is_diagnostic,
                "is_resolution": is_resolution,
                "emb": embedding,
                "tokens": len(qa_content.split()),
                "stage": stage_type
            }
            
            self.semantic_store[bot].append(record)
            
            if self.session_dir:
                sem_file = os.path.join(self.session_dir, f"{bot}_sem.jsonl")
                try:
                    with open(sem_file, "a", encoding="utf-8") as f:
                        f.write(json.dumps(record) + "\n")
                except Exception as e:
                    self._update_status(f"Error saving semantic data: {str(e)}")
        except Exception as e:
            self._update_status(f"Semantic save error: {str(e)}")
    
    def _log_message(self, bot, message):
        if not self.log_path:
            return
        
        try:
            with open(self.log_path, "a", encoding="utf-8") as f:
                timestamp = time.strftime("%H:%M:%S")
                f.write(f"{timestamp} [{bot}] {message}\n")
        except Exception as e:
            self._update_status(f"Log error: {str(e)}")
    
    def _check_contradiction(self, bot, text):
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return
        
        try:
            current_emb = self.sem_model.encode(text).tolist()
            for prev_entry in self.semantic_store[bot][-5:-1]:
                prev_emb = prev_entry.get('emb', [])
                if prev_emb and cosine_sim(current_emb, prev_emb) < 0.1:
                    self._on_monitor_warning(f"{bot} may have contradicted previous statements")
                    break
        except Exception as e:
            self._update_status(f"Contradiction check error: {str(e)}")
    
    def _calculate_coherence(self, bot):
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return 0.5
        
        entries = self.semantic_store[bot]
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0.5
        
        sims = [cosine_sim(last_emb, r.get('emb', [])) for r in entries[:-1] if r.get('emb')]
        return float(sum(sims) / len(sims)) if sims else 0.5
    
    def _count_contradictions(self, bot):
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return 0
        
        entries = self.semantic_store[bot]
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0
        
        return sum(1 for r in entries[:-1] if r.get('emb') and cosine_sim(last_emb, r['emb']) < 0.1)
    
    def _calculate_relevance(self, bot):
        topic = self.topic_edit.text().strip().lower()
        if not topic or not self.sem_model:
            return 0.5
        
        entries = self.semantic_store.get(bot, [])
        if not entries:
            return 0.5
        
        topic_emb = self.sem_model.encode(topic)
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0.5
        
        return cosine_sim(topic_emb, last_emb)
    
    def _call_model(self, prompt):
        prov_model = self.model_combo.currentData() or ("openai", "gpt-4o-mini")
        provider, model = prov_model
        try:
            import openai
            import requests
            if provider == "openai" and "openai_client" in globals() and openai_client:
                response = openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temp_slider.value() / 100.0,
                    max_tokens=self.token_spin.value()
                )
                return response.choices[0].message.content.strip()
            elif provider == "ollama":
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=30
                )
                return response.json().get("response", "").strip()
            else:
                return "[Model not available]"
        except Exception as e:
            if self.session_dir:
                with open(os.path.join(self.session_dir, "errors.log"), "a", encoding="utf-8") as f:
                    f.write(f"{time.strftime('%H:%M:%S')}: {str(e)} - Prompt: {prompt[:100]}\n")
            return f"[Model error: {str(e)}]"
    
    def _get_persona(self, bot):
        return self.persona_red.toPlainText() if bot == "Red" else self.persona_blue.toPlainText()

if __name__ == "__main__":
    app = QApplication(sys.argv)  # Create QApplication first
    try:
        window = DebateAI()  # Initialize the window
        window.show()        # Display the GUI
        
        # Delay monitor start to ensure GUI is ready
        def start_monitoring():
            if window.log_path:
                window.monitor = DebateMonitor(parent=window)
                window.monitor.warning.connect(window._on_monitor_warning)
                window.monitor.log_path = window.log_path
                window.monitor.start()
                print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Monitoring started for Debate_AI.py")

        QTimer.singleShot(100, start_monitoring)  # Start monitoring after 100ms
        
        app.aboutToQuit.connect(lambda: print(f"GUI closed at {datetime.now().strftime('%H:%M:%S')}"))
        sys.exit(app.exec_())  # Run the event loop
    except Exception as e:
        with open("error.log", "a", encoding="utf-8") as f:
            f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Error launching GUI: {str(e)}\n")
        sys.exit(1)
```

**Classes:** DebateAI


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\utils.py`

```python
# utils.py

import os
import sys
import subprocess
import traceback
from datetime import datetime
import requests
import openai

# === API KEY SETUP ===
API_KEY_PATH = os.path.join("api", "api_key.txt")
os.makedirs(os.path.dirname(API_KEY_PATH), exist_ok=True)

# Create placeholder if missing
if not os.path.exists(API_KEY_PATH):
    with open(API_KEY_PATH, "w", encoding="utf-8") as f:
        f.write("your-openai-api-key-here")

# Load key
try:
    with open(API_KEY_PATH, "r", encoding="utf-8") as f:
        OPENAI_API_KEY = f.read().strip()

    if OPENAI_API_KEY and OPENAI_API_KEY != "your-openai-api-key-here":
        openai.api_key = OPENAI_API_KEY
        openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [INFO] OpenAI client initialized")
    else:
        openai_client = None
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [WARN] OpenAI API key not set or invalid")
except Exception as e:
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [ERROR] Failed to initialize OpenAI: {e}")
    openai_client = None

# === MODEL FETCHERS ===

def fetch_openai_models():
    if not openai_client:
        return ["gpt-4o-mini", "gpt-3.5-turbo"]
    try:
        response = openai_client.models.list()
        return sorted([m.id for m in response.data if "gpt" in m.id.lower()])
    except Exception as e:
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [ERROR] OpenAI model fetch failed: {e}")
        return ["gpt-4o-mini", "gpt-3.5-turbo"]

def fetch_ollama_models():
    try:
        resp = requests.get("http://localhost:11434/api/tags", timeout=3)
        if resp.status_code == 200:
            data = resp.json()
            return [entry["name"] for entry in data.get("models", [])] or ["llama2:7b"]
    except Exception as e:
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [WARN] Ollama model fetch failed: {e}")
    return ["llama2:7b"]

# === GLOBAL EXCEPTION HANDLER ===

def except_hook(exc_type, exc_value, exc_traceback):
    err_msg = "".join(traceback.format_exception(exc_type, exc_value, exc_traceback))
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_line = f"[{timestamp}] Unhandled Exception:\n{err_msg}\n"
    print(log_line)

    try:
        os.makedirs("logs", exist_ok=True)
        with open("logs/fatal_error.log", "a", encoding="utf-8") as f:
            f.write(log_line)
    except:
        pass  # Avoid recursion if logging fails

    sys.exit(1)

# Set global exception handler
sys.excepthook = except_hook
```

**Functions:** fetch_openai_models(), fetch_ollama_models(), except_hook(exc_type, exc_value, exc_traceback)


## Module `Debate_AI_Old\Debate_AI_3\Debate_AI\old\Debate_AI.py`

```python
#!/usr/bin/env python3
# Debate_AI.py - Fully Enhanced Implementation with Adaptive Arbiter and Robust Flow
# Complete system with dynamic control, strict narrative, resolution negotiation, and rich UI

import os
import sys
import json
import time
import shutil
import threading
import subprocess
import requests
import openai
import numpy as np
import asyncio
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget,
    QVBoxLayout, QHBoxLayout, QLabel,
    QLineEdit, QTextEdit, QPushButton,
    QComboBox, QSlider, QDockWidget,
    QCheckBox, QListWidget, QMessageBox,
    QSpinBox, QStatusBar, QFileDialog,
    QProgressBar
)
from PyQt5.QtGui import QColor, QPalette, QTextCharFormat, QTextCursor
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal, QStateMachine, QState, QFinalState

# ─── Ensure local modules importable ─────────────────────────────────────────
sys.path.insert(0, os.path.dirname(__file__))

# Import TTS and Monitor
try:
    from tts import TTSManager
    from monitor_debate import DebateMonitor
except ImportError as e:
    QMessageBox.critical(None, "Error", f"Import error: {e}")
    sys.exit(1)

# ─── Create API directory and key file if needed ─────────────────────────────
os.makedirs("api", exist_ok=True)
API_KEY_PATH = os.path.join("api", "api_key.txt")
if not os.path.exists(API_KEY_PATH):
    with open(API_KEY_PATH, "w") as f:
        f.write("your-openai-api-key-here")

# ─── Load API Key & OpenAI client ────────────────────────────────────────────
try:
    with open(API_KEY_PATH, "r", encoding="utf-8") as f:
        OPENAI_API_KEY = f.read().strip()
    if OPENAI_API_KEY and OPENAI_API_KEY != "your-openai-api-key-here":
        openai.api_key = OPENAI_API_KEY
        openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    else:
        openai_client = None
except Exception as e:
    print(f"OpenAI setup error: {e}")
    openai_client = None

# ─── Dynamic Model Fetching ─────────────────────────────────────────────────
def fetch_openai_models():
    if not openai_client:
        return ["gpt-4o-mini", "gpt-3.5-turbo"]
    try:
        resp = openai_client.models.list()
        return [m.id for m in resp.data if "gpt" in m.id.lower()]
    except:
        return ["gpt-4o-mini", "gpt-3.5-turbo"]

def fetch_ollama_models():
    try:
        output = subprocess.check_output(["ollama", "list"], universal_newlines=True)
        models = [line.split()[0] for line in output.splitlines()[1:] if line.strip()]
        return models if models else ["llama2:7b"]
    except:
        return ["llama2:7b"]

# ─── Utility Functions ──────────────────────────────────────────────────────
def cosine_sim(a, b):
    a, b = np.array(a), np.array(b)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))

def safe_gui_call(func, *args, **kwargs):
    QTimer.singleShot(0, lambda: func(*args, **kwargs))

def parse_mad_lib(text, required_slots=["Stance", "reason", "opponent's point", "argument"]):
    """Parse text for Mad Lib structure and validate slots"""
    slots = {slot: None for slot in required_slots}
    for slot in required_slots:
        start = text.find(f"[{slot}]")
        if start != -1:
            end = text.find("]", start)
            if end != -1:
                slots[slot] = text[start + len(f"[{slot}]"):end].strip()
    return all(slots.values()), slots

# ─── Debate Worker Thread with State Machine ────────────────────────────────
class DebateWorker(QThread):
    message_ready = pyqtSignal(str, str, str)  # bot, type, message
    status_update = pyqtSignal(str)
    epoch_complete = pyqtSignal(int)
    error_occurred = pyqtSignal(str)
    topic_updated = pyqtSignal(str, str)  # topic, summary
    progress_update = pyqtSignal(int)  # Percentage

    def __init__(self, parent):
        super().__init__()
        self.parent = parent
        self.running = False
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        self.current_bot = "Red"
        self.talking_points = []
        self.current_point_idx = 0
        self.rebuttal_count = 0
        self.max_rebuttals = 0
        self.state_machine = None
        self.deadlock_count = 0
        self.MAX_DEADLOCKS = 2

    def start_debate(self):
        self.running = True
        self._setup_state_machine()
        self.start()

    def stop_debate(self):
        self.running = False
        self.executor.shutdown(wait=False)
        self.loop.call_soon_threadsafe(self.loop.stop)
        if self.state_machine:
            self.state_machine.stop()

    def _setup_state_machine(self):
        self.state_machine = QStateMachine()
        init_state = QState()
        rebuttal_state = QState()
        resolution_state = QState()
        final_state = QFinalState()

        init_state.addTransition(self._init_complete, rebuttal_state)
        rebuttal_state.addTransition(self._rebuttal_complete, resolution_state if self.current_point_idx >= len(self.talking_points) - 1 else rebuttal_state)
        resolution_state.addTransition(self._resolution_complete, final_state)

        init_state.entered.connect(self._enter_init)
        rebuttal_state.entered.connect(self._enter_rebuttal)
        resolution_state.entered.connect(self._enter_resolution)
        final_state.entered.connect(self._enter_final)

        self.state_machine.addState(init_state)
        self.state_machine.addState(rebuttal_state)
        self.state_machine.addState(resolution_state)
        self.state_machine.addState(final_state)
        self.state_machine.setInitialState(init_state)

    def _init_complete(self):
        return self.current_point_idx > 0

    def _rebuttal_complete(self):
        return self.rebuttal_count >= self.max_rebuttals or self._detect_deadlock()

    def _resolution_complete(self):
        return True

    def _enter_init(self):
        asyncio.run_coroutine_threadsafe(self._initialize_debate(), self.loop)

    def _enter_rebuttal(self):
        asyncio.run_coroutine_threadsafe(self._execute_rebuttal(), self.loop)

    def _enter_resolution(self):
        asyncio.run_coroutine_threadsafe(self._execute_resolution(), self.loop)

    def _enter_final(self):
        asyncio.run_coroutine_threadsafe(self._finalize_epoch(), self.loop)
        self.running = False

    def run(self):
        try:
            self.loop.run_until_complete(self._debate_flow())
        except Exception as e:
            self.error_occurred.emit(str(e))
        finally:
            self.loop.close()

    async def _debate_flow(self):
        if not await self._validate_preconditions():
            self.error_occurred.emit("Preconditions failed")
            return

        self.parent.epoch += 1
        eid = f"{self.parent.epoch:03d}"
        edir = os.path.join(self.parent.session_dir, f"epoch_{eid}")
        os.makedirs(edir, exist_ok=True)

        self.status_update.emit(f"Starting Epoch {self.parent.epoch}")
        self.progress_update.emit(0)

        if self.parent.log_path:
            open(self.parent.log_path, "w", encoding="utf-8").close()

        if self.parent.chk_enable_arbiter.isChecked():
            self.status_update.emit("Arbiter generating talking points...")
            talking_points = await self._call_arbiter_plan()
            if talking_points and talking_points.get("stages"):
                self.talking_points = talking_points["stages"]
                self.max_rebuttals = self.parent.rounds_spin.value()
                with open(os.path.join(edir, "talking_points.json"), "w", encoding="utf-8") as f:
                    json.dump({"talking_points": self.talking_points, "epoch": self.parent.epoch}, f, indent=2)
                self.message_ready.emit("Arbiter", "Plan", json.dumps(self.talking_points))
            else:
                self.error_occurred.emit("Failed to generate talking points")
                return

        self.state_machine.start()
        while self.state_machine.running():
            await asyncio.sleep(0.1)

        self.epoch_complete.emit(self.parent.epoch)
        self.status_update.emit(f"Epoch {self.parent.epoch} completed")
        self.progress_update.emit(100)

    async def _validate_preconditions(self):
        """Validate debate readiness"""
        if not self.parent.topic_edit.text().strip():
            return False
        if not self.parent.persona_red.toPlainText().strip() or not self.parent.persona_blue.toPlainText().strip():
            return False
        return True

    async def _initialize_debate(self):
        """Initialize debate with first turn"""
        if not self.talking_points:
            self.error_occurred.emit("No talking points available")
            return
        point = self.talking_points[self.current_point_idx]
        await self._execute_turn(self.current_bot, point["name"], point["questions"][0])
        self.current_bot = "Blue" if self.current_bot == "Red" else "Red"
        self.current_point_idx += 1
        self.progress_update.emit(10)

    async def _execute_rebuttal(self):
        """Execute rebuttal phase with alternation and interjection"""
        if not self.talking_points or self.current_point_idx >= len(self.talking_points):
            self.error_occurred.emit("No more stages")
            return

        point = self.talking_points[self.current_point_idx]
        stage_type = point["name"]
        self.status_update.emit(f"Stage: {stage_type}")
        self.message_ready.emit("System", "Stage", f"--- {stage_type} ---")

        await self._execute_turn(self.current_bot, stage_type, point["questions"][0])
        self.rebuttal_count += 1

        if self.rebuttal_count >= self.max_rebuttals or self._detect_deadlock():
            await self._arbiter_interjection()
            self.rebuttal_count = 0
            self.current_bot = "Red"
            self.current_point_idx += 1
        else:
            self.current_bot = "Blue" if self.current_bot == "Red" else "Red"

        self.progress_update.emit(min(50, int((self.current_point_idx / len(self.talking_points)) * 50)))

    async def _execute_resolution(self):
        """Execute resolution phase"""
        if self.current_point_idx >= len(self.talking_points):
            self.error_occurred.emit("No resolution stage")
            return

        point = self.talking_points[self.current_point_idx]
        await self._execute_resolution_turn(self.current_bot, point["name"])
        self.current_bot = "Blue" if self.current_bot == "Red" else "Red"
        self.current_point_idx += 1
        self.progress_update.emit(75)

    async def _execute_turn(self, bot, stage_type, question):
        """Execute a single debate turn with narrative enforcement"""
        try:
            history = await asyncio.get_event_loop().run_in_executor(
                self.executor, self.parent._get_history_slice
            )
            rag_context = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._get_semantic_context(bot)
            )
            persona = self.parent._get_persona(bot)

            mono_prompt = self._build_monologue_prompt(bot, stage_type, persona, history, rag_context, question)
            monologue = await self._call_model(mono_prompt)
            if monologue:
                valid, slots = parse_mad_lib(monologue)
                if not valid:
                    correction = self._correct_mad_lib(monologue, slots)
                    monologue = correction if correction else monologue
                self.message_ready.emit(bot, "Monologue", monologue)
            else:
                raise ValueError("Monologue generation failed")

            pub_prompt = self._build_public_prompt(bot, stage_type, persona, monologue, history)
            public_msg = await self._call_model(pub_prompt)
            if public_msg:
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent._log_message(bot, public_msg)
                )
                self.message_ready.emit(bot, "Public", public_msg)
            else:
                raise ValueError("Public response generation failed")

            if (bot == "Red" and self.parent.chk_enable_tts_red.isChecked()) or \
               (bot == "Blue" and self.parent.chk_enable_tts_blue.isChecked()):
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent.tts.speak(bot, public_msg)
                )
            await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._semantic_save(bot, monologue, public_msg, stage_type)
            )
            await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._check_contradiction(bot, public_msg)
            )
            await self._scorebot_analysis(bot, stage_type)
        except Exception as e:
            self.error_occurred.emit(f"Turn execution error: {str(e)}")

    async def _execute_resolution_turn(self, bot, stage_type):
        """Execute a resolution turn with negotiation"""
        try:
            history = await asyncio.get_event_loop().run_in_executor(
                self.executor, self.parent._get_history_slice
            )
            rag_context = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._get_semantic_context(bot)
            )
            persona = self.parent._get_persona(bot)

            resolution_prompt = self._build_resolution_prompt(bot, stage_type, persona, history, rag_context)
            monologue = await self._call_model(resolution_prompt)
            if monologue:
                valid, slots = parse_mad_lib(monologue, ["Resolution type", "reason", "evidence"])
                if not valid:
                    correction = self._correct_mad_lib(monologue, slots, is_resolution=True)
                    monologue = correction if correction else monologue
                self.message_ready.emit(bot, "Monologue", monologue)
            else:
                raise ValueError("Resolution monologue generation failed")

            pub_prompt = self._build_public_prompt(bot, stage_type, persona, monologue, history)
            public_msg = await self._call_model(pub_prompt)
            if public_msg:
                resolution_quality = await self._evaluate_resolution_quality(bot, public_msg)
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent._log_message(bot, f"{public_msg} (Quality: {resolution_quality:.2f})")
                )
                self.message_ready.emit(bot, "Public", public_msg)
            else:
                raise ValueError("Resolution public response generation failed")

            if (bot == "Red" and self.parent.chk_enable_tts_red.isChecked()) or \
               (bot == "Blue" and self.parent.chk_enable_tts_blue.isChecked()):
                await asyncio.get_event_loop().run_in_executor(
                    self.executor, lambda: self.parent.tts.speak(bot, public_msg)
                )
            await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._semantic_save(bot, monologue, public_msg, stage_type, is_resolution=True)
            )
        except Exception as e:
            self.error_occurred.emit(f"Resolution turn error: {str(e)}")

    async def _arbiter_interjection(self):
        """Arbiter provides a new guiding question with adaptive adjustment"""
        if self.current_point_idx + 1 >= len(self.talking_points):
            self.error_occurred.emit("No more stages for interjection")
            return

        coherence_red = await asyncio.get_event_loop().run_in_executor(
            self.executor, lambda: self.parent._calculate_coherence("Red")
        )
        coherence_blue = await asyncio.get_event_loop().run_in_executor(
            self.executor, lambda: self.parent._calculate_coherence("Blue")
        )

        if coherence_red < 0.3 or coherence_blue < 0.3 or self.deadlock_count >= self.MAX_DEADLOCKS:
            self._adjust_talking_points(coherence_red, coherence_blue)
            self.deadlock_count = 0

        next_point = self.talking_points[self.current_point_idx]
        question = next_point["questions"][0]
        self.message_ready.emit("Arbiter", "Interjection", f"Next question: {question} (Adjusted for coherence)")
        self.current_bot = "Red"

    def _detect_deadlock(self):
        """Detect deadlock based on low coherence or repetition"""
        coherence_red = self.parent._calculate_coherence("Red")
        coherence_blue = self.parent._calculate_coherence("Blue")
        if coherence_red < 0.2 or coherence_blue < 0.2:
            self.deadlock_count += 1
            return True
        return False

    def _adjust_talking_points(self, coherence_red, coherence_blue):
        """Adjust talking points based on coherence"""
        if coherence_red < 0.3:
            self.talking_points.insert(self.current_point_idx + 1, {
                "name": f"Rebuttal Clarification {self.rebuttal_count + 1}",
                "questions": [f"Red, clarify your stance due to low coherence ({coherence_red:.2f})"],
                "outcomes": "Red clarifies position",
                "max_duration": 300
            })
        if coherence_blue < 0.3:
            self.talking_points.insert(self.current_point_idx + 1, {
                "name": f"Rebuttal Clarification {self.rebuttal_count + 1}",
                "questions": [f"Blue, clarify your stance due to low coherence ({coherence_blue:.2f})"],
                "outcomes": "Blue clarifies position",
                "max_duration": 300
            })

    def _build_monologue_prompt(self, bot, stage_type, persona, history, rag_context, question):
        """Build prompt with strict Mad Lib enforcement"""
        opponent = "Blue" if bot == "Red" else "Red"
        opponent_history = "\n".join([m for m in history.split("\n") if f"[{opponent}]" in m])
        
        return f"""[INTERNAL MONOLOGUE - {bot}]
Stage: {stage_type}
Persona: {persona}
Arbiter Question: {question}

Recent History:
{history}

Opponent History:
{opponent_history}

Relevant Context (RAG):
{rag_context}

Generate your internal reasoning in this strict Mad Lib format: [Stance] because [reason], countering [opponent's point] with [argument]."""

    def _build_public_prompt(self, bot, stage_type, persona, monologue, history):
        """Build prompt for public response"""
        return f"""[PUBLIC RESPONSE - {bot}]
Stage: {stage_type}
Persona: {persona}

Internal Analysis:
{monologue}

Recent History:
{history}

Present a clear, persuasive argument addressing the Arbiter's question and engaging with the opponent's points."""

    def _build_resolution_prompt(self, bot, stage_type, persona, history, rag_context):
        """Build prompt for resolution phase with negotiation"""
        return f"""[INTERNAL MONOLOGUE - {bot} - RESOLUTION]
Stage: {stage_type}
Persona: {persona}

Recent History:
{history}

Relevant Context (RAG):
{rag_context}

Propose a resolution in this format: [Resolution type] because [reason], supported by [evidence]. Negotiate with the opponent’s last stance to seek agreement, disagreement, or conditional consensus."""

    def _correct_mad_lib(self, text, slots, is_resolution=False):
        """Correct missing Mad Lib slots with Arbiter guidance"""
        required_slots = ["Resolution type", "reason", "evidence"] if is_resolution else ["Stance", "reason", "opponent's point", "argument"]
        missing = [s for s in required_slots if not slots.get(s)]
        if missing:
            prompt = f"""The following monologue is missing Mad Lib slots: {missing}. Correct it to fit the format.
Text: {text}
Provide a corrected version with all slots filled."""
            return self.parent._call_model(prompt)
        return None

    def _get_persona(self, bot):
        """Get persona text for bot (delegate to parent)"""
        return self.parent._get_persona(bot)

    async def _call_arbiter_plan(self):
        """Generate Arbiter debate plan with context-aware adjustments"""
        topic = self.parent.topic_edit.text().strip()
        epoch = self.parent.epoch
        prior_summary = ""
        if self.parent.epoch > 1 and os.path.exists(os.path.join(self.parent.session_dir, f"epoch_{epoch-1:03d}", "summary_{epoch-1:03d}.txt")):
            with open(os.path.join(self.parent.session_dir, f"epoch_{epoch-1:03d}", "summary_{epoch-1:03d}.txt"), "r", encoding="utf-8") as f:
                prior_summary = f.read()

        prompt = f"""As the Arbiter AI, create a structured debate plan for Epoch {epoch}.

Topic: {topic}
Prior Summary (if any): {prior_summary}

Generate a debate plan with 3-5 key stages, each with:
1. Name (e.g., Opening, Rebuttal 1, Closing)
2. A list of 1-2 focus questions to guide the discussion
3. Expected outcomes for Red and Blue bots
4. Maximum duration in seconds (optional)

Incorporate prior conclusions or adjust questions based on the prior summary. Return the plan in JSON format."""
        
        plan_text = self.parent._call_model(prompt)
        if not plan_text or not plan_text.strip():
            self.error_occurred.emit("Arbiter plan generation failed")
            return {"stages": [
                {"name": "Opening", "questions": ["What is your stance?"], "outcomes": "Establish positions", "max_duration": 300},
                {"name": "Rebuttal 1", "questions": ["How do you counter the opponent?"], "outcomes": "Challenge arguments", "max_duration": 300},
                {"name": "Closing", "questions": ["What is your final resolution?"], "outcomes": "Conclude debate", "max_duration": 300}
            ]}
        try:
            return json.loads(plan_text)
        except json.JSONDecodeError:
            self.error_occurred.emit("Invalid JSON from Arbiter plan")
            return {"stages": [
                {"name": "Opening", "questions": ["What is your stance?"], "outcomes": "Establish positions", "max_duration": 300},
                {"name": "Rebuttal 1", "questions": ["How do you counter the opponent?"], "outcomes": "Challenge arguments", "max_duration": 300},
                {"name": "Closing", "questions": ["What is your final resolution?"], "outcomes": "Conclude debate", "max_duration": 300}
            ]}

    async def _call_model(self, prompt):
        """Call the selected AI model with error recovery"""
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._call_model(prompt)
            )
            return result if result and result.strip() else self._generate_fallback_response(prompt)
        except Exception as e:
            self.error_occurred.emit(f"Model call error: {str(e)}")
            return self._generate_fallback_response(prompt)

    def _generate_fallback_response(self, prompt):
        """Generate a fallback response if model fails"""
        return f"[Fallback] Unable to process: {prompt[:50]}... Please check API or model settings."

    async def _scorebot_analysis(self, bot, stage_type):
        """Perform adaptive ScoreBot analysis"""
        try:
            coherence = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._calculate_coherence(bot)
            )
            contradictions = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._count_contradictions(bot)
            )
            relevance = await asyncio.get_event_loop().run_in_executor(
                self.executor, lambda: self.parent._calculate_relevance(bot)
            )
            weight = 1.0 if "Closing" not in stage_type else 2.0  # Higher weight for resolution
            score = (coherence * 0.4 + (1 - contradictions / 5) * 0.3 + relevance * 0.3) * weight

            score_entry = {
                "bot": bot,
                "stage": stage_type,
                "coherence": coherence,
                "contradictions": contradictions,
                "relevance": relevance,
                "score": score,
                "timestamp": time.time()
            }
            self.parent.score_updates.append(score_entry)

            color = "#FF4444" if bot == "Red" else "#4444FF"
            score_text = (
                f"<span style='color:{color}'>{stage_type}/{bot}</span> → "
                f"Score: {score:.2f}, Coherence: {coherence:.2f}, Contradictions: {contradictions}, "
                f"Relevance: {relevance:.2f}"
            )
            safe_gui_call(self.parent._update_scorebot_display, score_text)
        except Exception as e:
            self.error_occurred.emit(f"ScoreBot error: {str(e)}")

    async def _evaluate_resolution_quality(self, bot, text):
        """Evaluate the quality of a resolution"""
        prompt = f"""Evaluate the quality of this resolution statement on a scale of 0 to 1:
{text}
Consider clarity, evidence strength, and negotiation effort. Return a single float value."""
        quality = await self._call_model(prompt)
        try:
            return float(quality.strip())
        except ValueError:
            return 0.5  # Default quality if parsing fails

    async def _update_topic_index(self, bot, stage_type):
        """Update topic index with subtopics"""
        await self.parent._update_topic_index(bot, stage_type)

    async def _finalize_epoch(self, epoch_id, epoch_dir, stages):
        """Finalize epoch with detailed logging"""
        try:
            if self.parent.log_path and os.path.exists(self.parent.log_path):
                shutil.copy(self.parent.log_path, os.path.join(epoch_dir, f"transcript_{epoch_id}.txt"))

            if self.parent.sem_model and self.parent.log_path:
                try:
                    with open(self.parent.log_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content:
                        embedding = self.parent.sem_model.encode(content).tolist()
                        with open(os.path.join(epoch_dir, f"vectors_{epoch_id}.json"), "w", encoding="utf-8") as f:
                            json.dump(embedding, f)
                except Exception as e:
                    self._update_status(f"Vector save error: {str(e)}")

            if self.parent.score_updates:
                with open(os.path.join(epoch_dir, f"scores_{epoch_id}.json"), "w", encoding="utf-8") as f:
                    json.dump(self.parent.score_updates, f, indent=2)

            if self.parent.chk_enable_arbiter.isChecked():
                summary = await self._generate_arbiter_summary(epoch_id, stages)
                with open(os.path.join(epoch_dir, f"summary_{epoch_id}.txt"), "w", encoding="utf-8") as f:
                    f.write(summary)
                self.message_ready.emit("Arbiter", "Summary", summary)

            # Log detailed diagnostics
            with open(os.path.join(epoch_dir, "diagnostics.log"), "w", encoding="utf-8") as f:
                f.write(f"Epoch {epoch_id} completed at {datetime.now()}\n")
                f.write(f"Talking Points: {json.dumps(self.talking_points, indent=2)}\n")
                f.write(f"Final Scores: {json.dumps(self.parent.score_updates[-2:], indent=2)}\n")
        except Exception as e:
            self._update_status(f"Epoch finalization error: {str(e)}")

    async def _generate_arbiter_summary(self, epoch_id, stages):
        """Generate Arbiter summary with epoch context"""
        try:
            transcript = ""
            if self.parent.log_path and os.path.exists(self.parent.log_path):
                with open(self.parent.log_path, "r", encoding="utf-8") as f:
                    transcript = f.read()

            prompt = f"""As the Arbiter AI, provide a comprehensive summary of Epoch {epoch_id}.

Debate Transcript (last 2000 characters):
{transcript[-2000:]}

Analyze:
1. Key arguments from Red and Blue
2. Strongest points made by each
3. Areas of agreement and disagreement
4. Logical consistency of each bot
5. Recommendations for the next epoch, incorporating prior vectors or conclusions

Provide a structured summary in markdown format."""
            
            return self.parent._call_model(prompt)
        except Exception as e:
            return f"Summary generation error: {str(e)}"

# =============================================================================
# Main Application
# =============================================================================
class DebateAI(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate AI - Enhanced System")
        self.resize(1600, 1000)
        
        # ── State variables
        self.session = None
        self.session_dir = None
        self.epoch = 0
        self.log_path = None
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        self.running = False
        
        # ── Initialize semantic model
        self.sem_model = None
        try:
            from sentence_transformers import SentenceTransformer
            self.sem_model = SentenceTransformer("all-MiniLM-L6-v2")
        except ImportError:
            print("SentenceTransformer not available - semantic features disabled")
        
        # ── Initialize tools
        try:
            self.tts = TTSManager()
        except Exception as e:
            print(f"TTS initialization error: {e}")
            self.tts = None
            QMessageBox.warning(self, "TTS Error", "TTS failed to initialize. TTS features disabled.")
        
        self.monitor = DebateMonitor(log_path="", warning_callback=self._on_monitor_warning)
        
        # ── Initialize worker thread
        self.worker = DebateWorker(self)
        self.worker.message_ready.connect(self._handle_message)
        self.worker.status_update.connect(self._update_status)
        self.worker.epoch_complete.connect(self._on_epoch_complete)
        self.worker.error_occurred.connect(self._handle_error)
        self.worker.topic_updated.connect(self._update_topic_index)
        self.worker.progress_update.connect(self._update_progress)
        
        # ── Load debate formats
        self.formats = self._load_debate_formats()
        
        # ── Text formats
        self.red_format = QTextCharFormat()
        self.red_format.setForeground(QColor("#FF4444"))
        self.blue_format = QTextCharFormat()
        self.blue_format.setForeground(QColor("#4444FF"))
        self.arbiter_format = QTextCharFormat()
        self.arbiter_format.setForeground(QColor("#800080"))
        self.system_format = QTextCharFormat()
        self.system_format.setForeground(QColor("#666666"))
        self.resolution_format = QTextCharFormat()
        self.resolution_format.setForeground(QColor("#006400"))
        
        # ── Build UI
        self._build_ui()
        
        # ── Apply styling
        self._apply_theme()
    
    def _load_debate_formats(self):
        """Load debate format templates"""
        formats = []
        formats_dir = "formats"
        os.makedirs(formats_dir, exist_ok=True)
        
        self_diag_file = os.path.join(formats_dir, "self_diagnostic_debate.json")
        if not os.path.exists(self_diag_file):
            default_format = {
                "name": "Self-Diagnostic Debate",
                "description": "Debate about improving the Debate_AI system itself",
                "stages": [
                    {"bot": "Red", "type": "Architectural Analysis", "max_duration": 300},
                    {"bot": "Blue", "type": "Feature Evaluation", "max_duration": 300},
                    {"bot": "Red", "type": "Failure Mode Review", "max_duration": 300},
                    {"bot": "Blue", "type": "Innovation Proposals", "max_duration": 300},
                    {"bot": "Red", "type": "Implementation Strategy", "max_duration": 300},
                    {"bot": "Blue", "type": "Summary & Recommendations", "max_duration": 300}
                ]
            }
            with open(self_diag_file, "w", encoding="utf-8") as f:
                json.dump(default_format, f, indent=2)
        
        for filename in os.listdir(formats_dir):
            if filename.endswith(".json"):
                try:
                    with open(os.path.join(formats_dir, filename), "r", encoding="utf-8") as f:
                        format_data = json.load(f)
                        formats.append(format_data)
                except Exception as e:
                    print(f"Error loading format {filename}: {e}")
        
        return formats
    
    def _build_ui(self):
        """Build the complete user interface with resolution indicator"""
        self.chat = QTextEdit()
        self.chat.setReadOnly(True)
        self.chat.setStyleSheet("background-color: #E6C8E6; color: #4A0E4E; font-size: 14px;")
        self.setCentralWidget(self.chat)
        
        self._create_monologue_docks()
        self._create_persona_docks()
        self._create_control_docks()
        self._create_analysis_docks()
        self._create_session_docks()
        
        self.status_bar = QStatusBar()
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        self.progress_bar.setMaximumWidth(200)
        self.resolution_label = QLabel("Resolution Status: Pending")
        self.resolution_label.setStyleSheet("color: #006400;")
        self.status_bar.addWidget(self.resolution_label)
        self.status_bar.addPermanentWidget(self.progress_bar)
        self.setStatusBar(self.status_bar)
        
        self._connect_signals()
        self._initialize_defaults()
    
    def _create_monologue_docks(self):
        """Create monologue display docks"""
        dock_red_mono = QDockWidget("Red Monologue", self)
        self.red_mono = QTextEdit()
        self.red_mono.setReadOnly(True)
        self.red_mono.setStyleSheet("background-color: #FFE6E6; color: #8B0000; font-size: 14px;")
        dock_red_mono.setWidget(self.red_mono)
        self.addDockWidget(Qt.LeftDockWidgetArea, dock_red_mono)
        
        dock_blue_mono = QDockWidget("Blue Monologue", self)
        self.blue_mono = QTextEdit()
        self.blue_mono.setReadOnly(True)
        self.blue_mono.setStyleSheet("background-color: #E6F3FF; color: #000080; font-size: 14px;")
        dock_blue_mono.setWidget(self.blue_mono)
        self.addDockWidget(Qt.RightDockWidgetArea, dock_blue_mono)
    
    def _create_persona_docks(self):
        """Create persona editing docks"""
        dock_red_persona = QDockWidget("Red Persona", self)
        widget_red = QWidget()
        layout_red = QVBoxLayout()
        self.persona_red = QTextEdit()
        self.persona_red.setMaximumHeight(100)
        self.persona_red.setStyleSheet("background-color: #FFF0F0; color: #8B0000; font-size: 14px;")
        layout_red.addWidget(QLabel("Red Bot Persona:"))
        layout_red.addWidget(self.persona_red)
        widget_red.setLayout(layout_red)
        dock_red_persona.setWidget(widget_red)
        self.addDockWidget(Qt.LeftDockWidgetArea, dock_red_persona)
        
        dock_blue_persona = QDockWidget("Blue Persona", self)
        widget_blue = QWidget()
        layout_blue = QVBoxLayout()
        self.persona_blue = QTextEdit()
        self.persona_blue.setMaximumHeight(100)
        self.persona_blue.setStyleSheet("background-color: #E6FFFF; color: #000080; font-size: 14px;")
        layout_blue.addWidget(QLabel("Blue Bot Persona:"))
        layout_blue.addWidget(self.persona_blue)
        widget_blue.setLayout(layout_blue)
        dock_blue_persona.setWidget(widget_blue)
        self.addDockWidget(Qt.RightDockWidgetArea, dock_blue_persona)
    
    def _create_control_docks(self):
        """Create control and TTS docks"""
        dock_struct = QDockWidget("Debate Structure", self)
        widget_struct = QWidget()
        layout_struct = QVBoxLayout()
        self.topic_edit = QLineEdit()
        self.topic_edit.setPlaceholderText("Enter debate topic or resolution...")
        layout_struct.addWidget(QLabel("Topic / Resolution"))
        layout_struct.addWidget(self.topic_edit)
        self.format_combo = QComboBox()
        self.format_combo.addItem("Classic Debate", None)
        for fmt in self.formats:
            self.format_combo.addItem(fmt["name"], fmt)
        layout_struct.addWidget(QLabel("Debate Format"))
        layout_struct.addWidget(self.format_combo)
        self.rounds_spin = QSpinBox()
        self.rounds_spin.setRange(0, 10)
        self.rounds_spin.setValue(2)
        layout_struct.addWidget(QLabel("Rebuttal Rounds"))
        layout_struct.addWidget(self.rounds_spin)
        self.chk_enable_arbiter = QCheckBox("Enable Arbiter AI")
        self.chk_enable_scorebot = QCheckBox("Enable ScoreBot")
        self.chk_enable_selfdiag = QCheckBox("Enable Self-Diagnostic Mode")
        layout_struct.addWidget(self.chk_enable_arbiter)
        layout_struct.addWidget(self.chk_enable_scorebot)
        layout_struct.addWidget(self.chk_enable_selfdiag)
        widget_struct.setLayout(layout_struct)
        dock_struct.setWidget(widget_struct)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_struct)
        
        dock_models = QDockWidget("Providers & Models", self)
        widget_models = QWidget()
        layout_models = QVBoxLayout()
        self.provider_combo = QComboBox()
        self.provider_combo.addItems(["OpenAI", "Ollama"])
        layout_models.addWidget(QLabel("Provider:"))
        layout_models.addWidget(self.provider_combo)
        self.model_combo = QComboBox()
        layout_models.addWidget(QLabel("Model:"))
        layout_models.addWidget(self.model_combo)
        btn_refresh = QPushButton("Refresh Models")
        layout_models.addWidget(btn_refresh)
        widget_models.setLayout(layout_models)
        dock_models.setWidget(widget_models)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_models)
        
        dock_settings = QDockWidget("Model Settings", self)
        widget_settings = QWidget()
        layout_settings = QVBoxLayout()
        self.temp_slider = QSlider(Qt.Horizontal)
        self.temp_slider.setRange(0, 100)
        self.temp_slider.setValue(70)
        self.temp_label = QLabel("Temperature: 0.70")
        layout_settings.addWidget(self.temp_label)
        layout_settings.addWidget(self.temp_slider)
        self.token_spin = QSpinBox()
        self.token_spin.setRange(64, 8192)
        self.token_spin.setValue(512)
        layout_settings.addWidget(QLabel("Max Tokens:"))
        layout_settings.addWidget(self.token_spin)
        widget_settings.setLayout(layout_settings)
        dock_settings.setWidget(widget_settings)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_settings)
        
        dock_controls = QDockWidget("Controls & TTS", self)
        widget_controls = QWidget()
        layout_controls = QVBoxLayout()
        layout_buttons = QHBoxLayout()
        self.btn_start = QPushButton("Start Debate")
        self.btn_pause = QPushButton("Pause Debate")
        self.btn_reset = QPushButton("Reset Session")
        self.btn_start.setStyleSheet("background-color: #90EE90; color: #000000;")
        self.btn_pause.setStyleSheet("background-color: #FFD700; color: #000000;")
        self.btn_reset.setStyleSheet("background-color: #FF4040; color: #FFFFFF;")
        layout_buttons.addWidget(self.btn_start)
        layout_buttons.addWidget(self.btn_pause)
        layout_buttons.addWidget(self.btn_reset)
        layout_tts = QVBoxLayout()
        tts_row1 = QHBoxLayout()
        self.chk_enable_tts_red = QCheckBox("Enable TTS Red")
        self.chk_enable_tts_blue = QCheckBox("Enable TTS Blue")
        self.voice_red = QComboBox()
        self.voice_blue = QComboBox()
        if self.tts:
            for vid, name in self.tts.list_voices():
                self.voice_red.addItem(name, vid)
                self.voice_blue.addItem(name, vid)
        self.btn_test_red = QPushButton("▶ Test Red Voice")
        self.btn_test_blue = QPushButton("▶ Test Blue Voice")
        tts_row1.addWidget(self.chk_enable_tts_red)
        tts_row1.addWidget(self.voice_red)
        tts_row1.addWidget(self.btn_test_red)
        tts_row1.addWidget(self.chk_enable_tts_blue)
        tts_row1.addWidget(self.voice_blue)
        tts_row1.addWidget(self.btn_test_blue)
        tts_row2 = QHBoxLayout()
        self.rate_slider = QSlider(Qt.Horizontal)
        self.rate_slider.setRange(100, 300)
        self.rate_slider.setValue(150)
        self.rate_label = QLabel("TTS Rate: 150")
        tts_row2.addWidget(self.rate_label)
        tts_row2.addWidget(self.rate_slider)
        self.volume_slider = QSlider(Qt.Horizontal)
        self.volume_slider.setRange(0, 100)
        self.volume_slider.setValue(80)
        self.volume_label = QLabel("TTS Volume: 0.80")
        tts_row2.addWidget(self.volume_label)
        tts_row2.addWidget(self.volume_slider)
        layout_tts.addLayout(tts_row1)
        layout_tts.addLayout(tts_row2)
        layout_controls.addLayout(layout_buttons)
        layout_controls.addLayout(layout_tts)
        widget_controls.setLayout(layout_controls)
        dock_controls.setWidget(widget_controls)
        self.addDockWidget(Qt.BottomDockWidgetArea, dock_controls)
        
        self.tabifyDockWidget(dock_models, dock_settings)
        self.tabifyDockWidget(dock_models, dock_controls)
        dock_models.raise_()
    
    def _create_analysis_docks(self):
        """Create analysis-related docks"""
        dock_topics = QDockWidget("Topics & Subtopics", self)
        self.idx_list = QListWidget()
        self.idx_list.setStyleSheet("background-color: #F0F0F0; color: #000000;")
        dock_topics.setWidget(self.idx_list)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_topics)
        
        dock_score = QDockWidget("ScoreBot Analysis", self)
        self.score_list = QListWidget()
        self.score_list.setStyleSheet("background-color: #F0F0F0; color: #000000;")
        dock_score.setWidget(self.score_list)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_score)
        
        dock_summary = QDockWidget("Arbiter Summary", self)
        self.summary_panel = QTextEdit()
        self.summary_panel.setReadOnly(True)
        self.summary_panel.setStyleSheet("background-color: #F0E6FF; color: #4A0E4E; font-size: 14px;")
        dock_summary.setWidget(self.summary_panel)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_summary)
    
    def _create_session_docks(self):
        """Create session management docks"""
        dock_session = QDockWidget("Session Workflows", self)
        widget_session = QWidget()
        layout_session = QHBoxLayout()
        self.btn_new = QPushButton("New Session")
        self.btn_save = QPushButton("Save Session")
        self.btn_load = QPushButton("Load Session")
        self.btn_export = QPushButton("Export Transcript")
        for btn in (self.btn_new, self.btn_save, self.btn_load, self.btn_export):
            btn.setStyleSheet("background-color: #D3D3D3; color: #000000;")
            layout_session.addWidget(btn)
        widget_session.setLayout(layout_session)
        dock_session.setWidget(widget_session)
        self.addDockWidget(Qt.TopDockWidgetArea, dock_session)
    
    def _apply_theme(self):
        """Apply consistent theme colors"""
        palette = self.palette()
        palette.setColor(QPalette.Window, QColor("#F0F0F0"))
        self.setPalette(palette)
    
    def _connect_signals(self):
        """Connect all UI signals"""
        self.format_combo.currentIndexChanged.connect(self._on_format_change)
        self.chk_enable_selfdiag.stateChanged.connect(self._apply_selfdiag_defaults)
        self.provider_combo.currentTextChanged.connect(self._refresh_model_list)
        self.temp_slider.valueChanged.connect(
            lambda v: self.temp_label.setText(f"Temperature: {v/100:.2f}")
        )
        self.btn_new.clicked.connect(self.new_session)
        self.btn_save.clicked.connect(self.save_session)
        self.btn_load.clicked.connect(self.load_session)
        self.btn_export.clicked.connect(self.export_transcript)
        self.btn_start.clicked.connect(self.start_debate)
        self.btn_pause.clicked.connect(self.pause_debate)
        self.btn_reset.clicked.connect(self.reset_debate)
        self.btn_test_red.clicked.connect(self._test_red_voice)
        self.btn_test_blue.clicked.connect(self._test_blue_voice)
        self.voice_red.currentIndexChanged.connect(
            lambda: self.tts and self.tts.set_bot_voice("Red", self.voice_red.currentData())
        )
        self.voice_blue.currentIndexChanged.connect(
            lambda: self.tts and self.tts.set_bot_voice("Blue", self.voice_blue.currentData())
        )
        self.rate_slider.valueChanged.connect(
            lambda v: self._update_tts_rate(v)
        )
        self.volume_slider.valueChanged.connect(
            lambda v: self._update_tts_volume(v)
        )
    
    def _update_tts_rate(self, value):
        """Update TTS rate and label"""
        self.rate_label.setText(f"TTS Rate: {value}")
        if self.tts:
            try:
                self.tts.set_rate(value)
            except Exception as e:
                self._update_status(f"TTS rate update error: {e}")
    
    def _update_tts_volume(self, value):
        """Update TTS volume and label"""
        self.volume_label.setText(f"TTS Volume: {value/100:.2f}")
        if self.tts:
            try:
                self.tts.set_volume(value / 100.0)
            except Exception as e:
                self._update_status(f"TTS volume update error: {e}")
    
    def _test_red_voice(self):
        """Test Red bot's voice"""
        if self.tts:
            try:
                self.tts.speak("Red", "This is a test of the Red bot's voice.")
            except Exception as e:
                self._update_status(f"Red TTS test error: {e}")
        else:
            self._update_status("TTS not available")
    
    def _test_blue_voice(self):
        """Test Blue bot's voice"""
        if self.tts:
            try:
                self.tts.speak("Blue", "This is a test of the Blue bot's voice.")
            except Exception as e:
                self._update_status(f"Blue TTS test error: {e}")
        else:
            self._update_status("TTS not available")
    
    def _initialize_defaults(self):
        """Initialize UI with default values"""
        self._refresh_model_list()
        self._on_format_change(self.format_combo.currentIndex())
        if self.tts:
            self._update_tts_rate(150)
            self._update_tts_volume(80)
    
    def _on_format_change(self, idx):
        """Handle format selection change"""
        if self.format_combo.currentText() == "Classic Debate":
            self.topic_edit.setText(
                "The role of political ideology in shaping scientific consensus."
            )
            self.persona_red.setPlainText(
                "You are RED, a conservative physicist component of Debate_AI. "
                "You prioritize rigorous empirical validation and proven frameworks."
            )
            self.persona_blue.setPlainText(
                "You are BLUE, a progressive physicist component of Debate_AI. "
                "You champion innovative hypotheses and adaptive experimentation."
            )
    
    def _apply_selfdiag_defaults(self, state):
        """Apply self-diagnostic mode defaults"""
        if state == Qt.Checked:
            self.topic_edit.setText(
                "Self-Diagnostic Analysis: Improve Debate_AI's architecture, UX, and AI-driven debate flow."
            )
            self.persona_red.setPlainText(
                "You are RED, an engineer subsystem of Debate_AI tasked with diagnosing system architecture and logic integrity."
            )
            self.persona_blue.setPlainText(
                "You are BLUE, an innovation subsystem of Debate_AI focused on feature expansion and UX enhancement."
            )
            idx = self.format_combo.findText("Self-Diagnostic Debate")
            if idx != -1:
                self.format_combo.setCurrentIndex(idx)
        else:
            self.persona_red.clear()
            self.persona_blue.clear()
    
    def _refresh_model_list(self):
        """Refresh available models"""
        prov = self.provider_combo.currentText()
        self.model_combo.clear()
        self._update_status(f"Refreshing {prov} models...")
        if prov == "OpenAI":
            for m in fetch_openai_models():
                self.model_combo.addItem(m, ("openai", m))
        else:
            for m in fetch_ollama_models():
                self.model_combo.addItem(m, ("ollama", m))
        self._update_status(f"{prov} models updated")
    
    def new_session(self):
        """Create a new session with vector reset"""
        if not self.sem_model:
            self._update_status("Semantic model unavailable, session creation limited.")
            return
        ts = int(time.time())
        self.session = f"sess_{ts}"
        self.session_dir = os.path.join("sessions", self.session)
        os.makedirs(self.session_dir, exist_ok=True)
        os.makedirs(os.path.join(self.session_dir, "backups"), exist_ok=True)
        self.log_path = os.path.join(self.session_dir, "log.txt")
        self.epoch = 0
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        self._update_status(f"New session created: {self.session}")

    def save_session(self):
        """Save current session"""
        if not self.session:
            QMessageBox.warning(self, "No Session", "Create or load a session first.")
            return
        with open(os.path.join(self.session_dir, "index.json"), "w", encoding="utf-8") as f:
            json.dump({
                "semantic_store": self.semantic_store,
                "topic_index": self.topic_index,
                "score_updates": self.score_updates
            }, f, indent=2)
        self._update_status("Session saved")
    
    def load_session(self):
        """Load existing session"""
        path = QFileDialog.getExistingDirectory(self, "Select Session Folder", "sessions")
        if not path:
            return
        self.session = os.path.basename(path)
        self.session_dir = path
        self.log_path = os.path.join(path, "log.txt")
        try:
            with open(os.path.join(path, "index.json"), "r", encoding="utf-8") as f:
                data = json.load(f)
            self.semantic_store = data.get("semantic_store", {"Red": [], "Blue": []})
            self.topic_index = data.get("topic_index", [])
            self.score_updates = data.get("score_updates", [])
        except:
            self.semantic_store = {"Red": [], "Blue": []}
            self.topic_index = []
            self.score_updates = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        for topic in self.topic_index:
            self.idx_list.addItem(f"{topic['bot']}: {topic['text']}")
        for score in self.score_updates:
            color = "#FF4444" if score['bot'] == "Red" else "#4444FF"
            self.score_list.addItem(
                f"<span style='color:{color}'>{score['stage']}/{score['bot']}</span> → "
                f"Score: {score.get('score', 0):.2f}, Coherence: {score['coherence']:.2f}, Contradictions: {score['contradictions']}, "
                f"Relevance: {score['relevance']:.2f}"
            )
        self._update_status(f"Loaded session: {self.session}")
    
    def export_transcript(self):
        """Export current transcript"""
        if not self.log_path or not os.path.exists(self.log_path):
            QMessageBox.warning(self, "No Transcript", "No transcript to export.")
            return
        dest, _ = QFileDialog.getSaveFileName(self, "Export Transcript", "debate.txt", "Text Files (*.txt)")
        if dest:
            shutil.copy(self.log_path, dest)
            self._update_status("Transcript exported")
    
    def start_debate(self):
        """Start the debate with recovery option"""
        if not self.session:
            QMessageBox.warning(self, "Error", "Create or load a session first.")
            return
        if self.running:
            return
        
        self.running = True
        self.monitor.log_path = self.log_path
        self.monitor.start()
        
        if self.chk_enable_selfdiag.isChecked():
            try:
                self._run_project_analysis()
            except Exception as e:
                self._handle_error(f"Project analysis error: {str(e)}", recover=True)
        
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.worker.start_debate()
    
    def pause_debate(self):
        """Pause the debate"""
        self.running = False
        self.monitor.stop()
        self.worker.stop_debate()
        self.progress_bar.setVisible(False)
        if self.tts:
            self.tts.stop()
        self._update_status("Debate paused")
    
    def reset_debate(self):
        """Reset the debate"""
        self.pause_debate()
        if self.session_dir and os.path.exists(self.session_dir):
            shutil.rmtree(self.session_dir)
        self.session = None
        self.epoch = 0
        self.semantic_store = {"Red": [], "Blue": []}
        self.score_updates = []
        self.topic_index = []
        for w in (self.chat, self.red_mono, self.blue_mono, self.idx_list, self.score_list, self.summary_panel):
            w.clear()
        self._update_status("Session reset")
    
    def _run_project_analysis(self):
        """Analyze project codebase for self-diagnostic mode"""
        if not self.sem_model:
            raise ValueError("Semantic model not available for analysis")
        root = os.path.dirname(__file__)
        for subdir, _, files in os.walk(root):
            if subdir.endswith("sessions") or subdir.endswith("formats"):
                continue
            for fname in files:
                if fname.endswith(".py"):
                    full_path = os.path.join(subdir, fname)
                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        summary = self._call_model(
                            f"Summarize the structure and purpose of {fname} in two sentences.\n\n{content}"
                        )
                        for bot in ("Red", "Blue"):
                            self._semantic_save(bot, summary, "", "Diagnostic", is_diagnostic=True)
                        ts = datetime.now().strftime("%H:%M:%S")
                        safe_gui_call(self.red_mono.append, f"{ts} [Analysis] {fname}: {summary}")
                        safe_gui_call(self.blue_mono.append, f"{ts} [Analysis] {fname}: {summary}")
                    except Exception as e:
                        self._update_status(f"Error analyzing {fname}: {str(e)}")
    
    def _handle_message(self, bot, msg_type, message):
        """Handle incoming messages with resolution indicator"""
        ts = datetime.now().strftime("%H:%M:%S")
        if msg_type == "Monologue":
            target = self.red_mono if bot == "Red" else self.blue_mono
            safe_gui_call(target.append, f"{ts} [Monologue] {message}")
        elif msg_type == "Public":
            fmt = self.resolution_format if "Closing" in self.worker.talking_points[self.worker.current_point_idx - 1]["name"] else \
                  self.red_format if bot == "Red" else self.blue_format
            safe_gui_call(self._append_chat, f"{ts} [{bot}] {message}", fmt)
            safe_gui_call(self.idx_list.addItem, f"{ts} {bot}: {message[:100]}...")
            if "Closing" in self.worker.talking_points[self.worker.current_point_idx - 1]["name"]:
                safe_gui_call(self.resolution_label.setText, "Resolution Status: In Progress")
        elif msg_type in ["Plan", "Summary", "Interjection"]:
            safe_gui_call(self.summary_panel.append, f"{ts} [{bot} {msg_type}] {message}")
        elif msg_type == "Stage":
            safe_gui_call(self._append_chat, f"{ts} {message}", self.system_format)
    
    def _append_chat(self, text, fmt):
        """Append text to chat with format"""
        cursor = self.chat.textCursor()
        cursor.movePosition(QTextCursor.End)
        cursor.insertText(text + "\n", fmt)
        self.chat.setTextCursor(cursor)
        self.chat.ensureCursorVisible()
    
    def _update_status(self, message):
        """Update status bar"""
        self.status_bar.showMessage(message, 5000)
    
    def _update_scorebot_display(self, score_text):
        """Update ScoreBot display"""
        self.score_list.addItem(score_text)
        self.score_list.scrollToBottom()
    
    def _update_topic_index(self, topic, summary):
        """Update topic index display"""
        self.idx_list.addItem(f"{topic}: {summary}")
        self.idx_list.scrollToBottom()
    
    def _update_progress(self, value):
        """Update progress bar"""
        self.progress_bar.setValue(value)
        if value == 100:
            self.resolution_label.setText("Resolution Status: Complete")

    def _on_monitor_warning(self, msg):
        """Handle monitor warnings"""
        safe_gui_call(self._append_chat, f"[Monitor] {msg}", self.system_format)
        self._update_status(msg)
    
    def _on_epoch_complete(self, epoch):
        """Handle epoch completion"""
        self.running = False
        self.monitor.stop()
        self.progress_bar.setValue(100)
        QTimer.singleShot(2000, lambda: self.progress_bar.setVisible(False))
        safe_gui_call(self._append_chat, f"✅ Epoch {epoch} complete", self.system_format)
        self.resolution_label.setText("Resolution Status: Complete")
    
    def _handle_error(self, error, recover=False):
        """Handle errors with recovery option"""
        error_msg = f"Error occurred at {datetime.now().strftime('%H:%M:%S')}:\n{error}"
        msg_box = QMessageBox(self)
        msg_box.setWindowTitle("Error")
        msg_box.setText("An error has occurred. Choose an action.")
        msg_box.setDetailedText(error_msg)
        copy_button = msg_box.addButton("Copy", QMessageBox.ActionRole)
        if recover:
            continue_button = msg_box.addButton("Continue", QMessageBox.AcceptRole)
        close_button = msg_box.addButton("Close", QMessageBox.RejectRole)
        msg_box.exec_()
        if msg_box.clickedButton() == copy_button:
            QApplication.clipboard().setText(error_msg)
        elif recover and msg_box.clickedButton() == continue_button:
            self._update_status("Attempting to recover...")
        elif msg_box.clickedButton() == close_button:
            self.close()
    
    def _get_history_slice(self):
        """Get recent conversation history"""
        if not self.log_path or not os.path.exists(self.log_path):
            return "No previous history."
        
        try:
            with open(self.log_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
            depth = self.rounds_spin.value() * 2 + 2
            return "".join(lines[-depth:]).strip()
        except:
            return "No previous history."

    def _get_semantic_context(self, bot, top_k=3):
        """Get semantic context using RAG with vector cache"""
        if not self.sem_model or not self.semantic_store.get(bot):
            return "No semantic context available."
        
        try:
            hist_emb = self.sem_model.encode(self._get_history_slice())
            sims = sorted(
                [(cosine_sim(hist_emb, r["emb"]), r["text"]) for r in self.semantic_store[bot] if r.get("emb")],
                key=lambda x: x[0],
                reverse=True
            )
            context = "\n".join(f"{s:.2f}: {t[:200]}..." for s, t in sims[:top_k])
            if self.chk_enable_selfdiag.isChecked():
                diag_sims = sorted(
                    [(cosine_sim(hist_emb, r["emb"]), r["text"]) for r in self.semantic_store[bot] if r.get("is_diagnostic", False) and r.get("emb")],
                    key=lambda x: x[0],
                    reverse=True
                )[:1]
                if diag_sims:
                    context += f"\n[Diagnostic] {diag_sims[0][1][:200]}..."
            return context
        except Exception as e:
            return f"No semantic context available (error: {str(e)})"
    
    def _semantic_save(self, bot, monologue, public_msg, stage_type, is_diagnostic=False, is_resolution=False):
        """Save text to semantic store with type differentiation"""
        if not self.sem_model:
            return
        
        try:
            if is_resolution:
                content_type = "resolution"
                prompt = f"Convert the following resolution into a Q/A format:\n\n{public_msg}"
                qa_content = self._call_model(prompt)
            elif "Closing" in stage_type:
                content_type = "conclusion"
                prompt = f"Convert the following conclusion into a Q/A format:\n\n{public_msg}"
                qa_content = self._call_model(prompt)
            else:
                content_type = "speculation" if "Rebuttal" in stage_type or "Opening" in stage_type else "raw"
                qa_content = public_msg if content_type == "raw" else monologue
            
            embedding = self.sem_model.encode(qa_content).tolist()
            record = {
                "time": time.time(),
                "text": qa_content,
                "raw_text": public_msg if content_type == "raw" else None,
                "type": content_type,
                "is_diagnostic": is_diagnostic,
                "is_resolution": is_resolution,
                "emb": embedding,
                "tokens": len(qa_content.split()),
                "stage": stage_type
            }
            
            self.semantic_store[bot].append(record)
            
            if self.session_dir:
                sem_file = os.path.join(self.session_dir, f"{bot}_sem.jsonl")
                try:
                    with open(sem_file, "a", encoding="utf-8") as f:
                        f.write(json.dumps(record) + "\n")
                except Exception as e:
                    self._update_status(f"Error saving semantic data: {str(e)}")
        except Exception as e:
            self._update_status(f"Semantic save error: {str(e)}")
    
    def _log_message(self, bot, message):
        """Log message to file"""
        if not self.log_path:
            return
        
        try:
            with open(self.log_path, "a", encoding="utf-8") as f:
                timestamp = datetime.now().strftime("%H:%M:%S")
                f.write(f"{timestamp} [{bot}] {message}\n")
        except Exception as e:
            self._update_status(f"Log error: {str(e)}")
    
    def _check_contradiction(self, bot, text):
        """Check for contradictions in bot's statements"""
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return
        
        try:
            current_emb = self.sem_model.encode(text).tolist()
            for prev_entry in self.semantic_store[bot][-5:-1]:
                prev_emb = prev_entry.get('emb', [])
                if prev_emb and cosine_sim(current_emb, prev_emb) < 0.1:
                    self._on_monitor_warning(f"{bot} may have contradicted previous statements")
                    break
        except Exception as e:
            self._update_status(f"Contradiction check error: {str(e)}")
    
    def _calculate_coherence(self, bot):
        """Calculate coherence score for bot"""
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return 0.5
        
        entries = self.semantic_store[bot]
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0.5
        
        sims = [cosine_sim(last_emb, r.get('emb', [])) for r in entries[:-1] if r.get('emb')]
        return float(sum(sims) / len(sims)) if sims else 0.5
    
    def _count_contradictions(self, bot):
        """Count potential contradictions"""
        if not self.sem_model or len(self.semantic_store.get(bot, [])) < 2:
            return 0
        
        entries = self.semantic_store[bot]
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0
        
        return sum(1 for r in entries[:-1] if r.get('emb') and cosine_sim(last_emb, r['emb']) < 0.1)
    
    def _calculate_relevance(self, bot):
        """Calculate relevance score"""
        topic = self.topic_edit.text().strip().lower()
        if not topic or not self.sem_model:
            return 0.5
        
        entries = self.semantic_store.get(bot, [])
        if not entries:
            return 0.5
        
        topic_emb = self.sem_model.encode(topic)
        last_emb = entries[-1].get('emb', [])
        if not last_emb:
            return 0.5
        
        return cosine_sim(topic_emb, last_emb)
    
    def _call_model(self, prompt):
        """Synchronous model call with detailed logging"""
        prov_model = self.model_combo.currentData() or ("openai", "gpt-4o-mini")
        provider, model = prov_model
        try:
            if provider == "openai" and openai_client:
                response = openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temp_slider.value() / 100.0,
                    max_tokens=self.token_spin.value()
                )
                return response.choices[0].message.content.strip()
            elif provider == "ollama":
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=30
                )
                return response.json().get("response", "").strip()
            else:
                return "[Model not available]"
        except Exception as e:
            with open(os.path.join(self.session_dir, "errors.log"), "a", encoding="utf-8") as f:
                f.write(f"{datetime.now()}: {str(e)} - Prompt: {prompt[:100]}\n")
            return f"[Model error: {str(e)}]"

    async def _finalize_epoch(self, epoch_id, epoch_dir, stages):
        """Finalize epoch with detailed logging"""
        try:
            if self.log_path and os.path.exists(self.log_path):
                shutil.copy(self.log_path, os.path.join(epoch_dir, f"transcript_{epoch_id}.txt"))

            if self.sem_model and self.log_path:
                try:
                    with open(self.log_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content:
                        embedding = self.sem_model.encode(content).tolist()
                        with open(os.path.join(epoch_dir, f"vectors_{epoch_id}.json"), "w", encoding="utf-8") as f:
                            json.dump(embedding, f)
                except Exception as e:
                    self._update_status(f"Vector save error: {str(e)}")

            if self.score_updates:
                with open(os.path.join(epoch_dir, f"scores_{epoch_id}.json"), "w", encoding="utf-8") as f:
                    json.dump(self.score_updates, f, indent=2)

            if self.chk_enable_arbiter.isChecked():
                summary = await self._generate_arbiter_summary(epoch_id, stages)
                with open(os.path.join(epoch_dir, f"summary_{epoch_id}.txt"), "w", encoding="utf-8") as f:
                    f.write(summary)
                self.message_ready.emit("Arbiter", "Summary", summary)

            with open(os.path.join(epoch_dir, "diagnostics.log"), "w", encoding="utf-8") as f:
                f.write(f"Epoch {epoch_id} completed at {datetime.now()}\n")
                f.write(f"Talking Points: {json.dumps(self.worker.talking_points, indent=2)}\n")
                f.write(f"Final Scores: {json.dumps(self.score_updates[-2:], indent=2)}\n")
        except Exception as e:
            self._update_status(f"Epoch finalization error: {str(e)}")

    async def _generate_arbiter_summary(self, epoch_id, stages):
        """Generate Arbiter summary with epoch context"""
        try:
            transcript = ""
            if self.log_path and os.path.exists(self.log_path):
                with open(self.log_path, "r", encoding="utf-8") as f:
                    transcript = f.read()

            prompt = f"""As the Arbiter AI, provide a comprehensive summary of Epoch {epoch_id}.

Debate Transcript (last 2000 characters):
{transcript[-2000:]}

Analyze:
1. Key arguments from Red and Blue
2. Strongest points made by each
3. Areas of agreement and disagreement
4. Logical consistency of each bot
5. Recommendations for the next epoch, incorporating prior vectors or conclusions

Provide a structured summary in markdown format."""
            
            return self._call_model(prompt)
        except Exception as e:
            return f"Summary generation error: {str(e)}"

# ─── Global Exception Handler ───────────────────────────────────────────────
def except_hook(cls, exception, traceback):
    error_msg = f"Unhandled exception at {datetime.now().strftime('%H:%M:%S')}:\n{''.join(traceback.format_exception(exception
```



## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\Analyze_All.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get the script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")

        for root, dirs, files in os.walk(start_path):
            # Ensure only directories below the script's location are analyzed
            if not root.startswith(start_path):
                continue  # Skip any directory outside the scope

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and analyze.txt from files list
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file_path = os.path.join(root, f)
                file.write(f"{file_indent}|-- {f}\n")

                # Capture file content
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f_content:
                        file_data = f_content.read()
                        file.write(f"\n{file_indent}    --- File Content ---\n")
                        file.write(f"{file_data}\n")
                        file.write(f"{file_indent}    -------------------\n\n")
                except Exception as e:
                    file.write(f"{file_indent}    [Error reading file: {str(e)}]\n\n")

    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.abspath(os.path.dirname(__file__))  # Get the script's location
    analyze_folders(script_directory)  # Analyze only from script's directory downward
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\Analyze_folders.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")
        
        for root, dirs, files in os.walk(start_path):
            # Skip directories named 'venv'
            if "venv" in root.split(os.sep):
                continue

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and output file from the list of files
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file.write(f"{file_indent}|-- {f}\n")
    
    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.dirname(os.path.abspath(__file__))
    analyze_folders(script_directory)
```

**Functions:** analyze_folders(start_path)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\Debate_AI.py`

```python
#!/usr/bin/env python3
# File: Debate_AI.py
# Location: C:/Users/Art PC/Desktop/Debate_AI/

import os
import sys
import json
import threading
import requests
import speech_recognition as sr
from datetime import datetime

# ─── Make sure our local "core" package and tts.py are importable ─────────────
BASE_DIR = os.path.dirname(__file__)
sys.path.insert(0, BASE_DIR)

from PyQt5.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QTextEdit, QPushButton, QLabel, QComboBox,
    QStackedWidget, QLineEdit, QCheckBox, QFileDialog,
    QMessageBox, QStatusBar, QTreeWidget, QTreeWidgetItem,
    QSplitter, QSizePolicy, QDialog
)
from PyQt5.QtCore import Qt, QTimer

import openai
from tts import (
    initialize_tts, set_tts_enabled,
    skip_tts, stop_tts, speak_text
)

# ─── Pull in our core modules ─────────────────────────────────────────────────
from core.engine        import DebateEngine, append_shared, append_bot, tail
from core.jarvis_engine import summarize, interpret_user
from core.structure     import load_structure as _load_structure

# ─── StructureChatDialog for “Auto-Generate Debate” ───────────────────────────
class StructureChatDialog(QDialog):
    """
    A lightweight chat window for “Auto-Generate Debate”.
    """
    def __init__(self, parent, engine):
        super().__init__(parent)
        self.engine = engine
        self.setWindowTitle("Auto-Generate Debate")
        self.setMinimumSize(600, 500)

        layout = QVBoxLayout(self)

        # System prompt editor
        layout.addWidget(QLabel("System Prompt (you can edit):"))
        self.prompt_edit = QTextEdit(self)
        self.prompt_edit.setPlainText(
            "You are an expert debate architect. Based on the conversation, produce one detailed talking-point JSON:\n"
            "{'name': <string>, 'desc': <string>, 'topics': [<string>, ...], 'subtopics': [<string>, ...]}"
        )
        self.prompt_edit.setFixedHeight(100)
        layout.addWidget(self.prompt_edit)

        # Chat display
        layout.addWidget(QLabel("Chat:"))
        self.chat_view = QTextEdit(self)
        self.chat_view.setReadOnly(True)
        layout.addWidget(self.chat_view, 1)

        # Input + Send
        row = QHBoxLayout()
        self.user_input = QLineEdit(self)
        self.user_input.setPlaceholderText("Type your message here…")
        row.addWidget(self.user_input, 1)
        send_btn = QPushButton("Send", self)
        send_btn.clicked.connect(self.on_send)
        row.addWidget(send_btn)
        layout.addLayout(row)

        # Finalize button
        fin_row = QHBoxLayout()
        self.finalize_btn = QPushButton("Finalize Structure", self)
        self.finalize_btn.clicked.connect(self.on_finalize)
        fin_row.addWidget(self.finalize_btn)
        fin_row.addStretch()
        layout.addLayout(fin_row)

    def on_send(self):
        user_txt = self.user_input.text().strip()
        if not user_txt:
            return
        self.chat_view.append(f"<b>You:</b> {user_txt}")
        self.user_input.clear()

        if not openai_client:
            self.chat_view.append("<i>[OpenAI not configured]</i>")
            return

        sys_prompt = self.prompt_edit.toPlainText().strip()
        try:
            resp = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user",   "content": user_txt}
                ]
            )
            ai_txt = resp.choices[0].message.content.strip()
        except Exception as e:
            ai_txt = f"[Error: {e}]"
        self.chat_view.append(f"<b>AI:</b> {ai_txt}")

    def on_finalize(self):
        chat_text     = self.chat_view.toPlainText()
        system_prompt = self.prompt_edit.toPlainText().strip()
        if not openai_client:
            QMessageBox.warning(self, "Error", "OpenAI not configured.")
            return
        try:
            resp     = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": chat_text}
                ]
            )
            json_txt = resp.choices[0].message.content.strip()
            tp_obj   = json.loads(json_txt)
        except Exception as e:
            QMessageBox.warning(self, "Finalize Error", str(e))
            return

        # Merge into current context
        tp_name, topic_name, _ = self.engine.current_context()
        struct = _load_structure()
        for tp in struct.get("talking_points", []):
            if tp["name"] == tp_name:
                for topic in tp.get("topics", []):
                    if topic["name"] == topic_name:
                        if "subtopics" in tp_obj:
                            topic.setdefault("subtopics", []).extend(tp_obj["subtopics"])
                        else:
                            topic.setdefault("subtopics", []).append(tp_obj.get("name"))
        with open(STRUCT_PATH, "w", encoding="utf-8") as f:
            json.dump(struct, f, indent=2)
        self.accept()

# ─── Load OpenAI key with fallback to environment ─────────────────────────────
API_KEY_PATH = os.path.join(BASE_DIR, "api", "api_key.txt")
OPENAI_KEY = ""
if os.path.isfile(API_KEY_PATH):
    OPENAI_KEY = open(API_KEY_PATH, "r", encoding="utf-8").read().strip()
if not OPENAI_KEY:
    OPENAI_KEY = os.getenv("OPENAI_API_KEY", "").strip()
if not OPENAI_KEY:
    print("Warning: OpenAI API key not found. OpenAI provider disabled.")
    openai_client = None
else:
    openai_client = openai.OpenAI(api_key=OPENAI_KEY)

# ─── Initialize TTS manager ─────────────────────────────────────────────────────
initialize_tts(voice_name="Zira", speed=1.0)

# ─── Ensure data dirs & files exist ─────────────────────────────────────────────
STRUCT_PATH = os.path.join(BASE_DIR, "data", "structure.json")
HIST_PATH   = os.path.join(BASE_DIR, "data", "history.log")
SESS_DIR    = os.path.join(BASE_DIR, "data", "sessions")
BOTS_DIR    = os.path.join(BASE_DIR, "bots")
for p in (STRUCT_PATH, HIST_PATH):
    os.makedirs(os.path.dirname(p), exist_ok=True)
os.makedirs(SESS_DIR, exist_ok=True)
os.makedirs(BOTS_DIR, exist_ok=True)
if not os.path.exists(STRUCT_PATH) or os.path.getsize(STRUCT_PATH) == 0:
    json.dump(_load_structure(), open(STRUCT_PATH, "w", encoding="utf-8"), indent=2)

# ─── Main Application ─────────────────────────────────────────────────────────
class DebateAIApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Debate AI + Jarvis")
        self.setGeometry(30, 30, 1400, 900)

        self.mode    = "debate"
        self.running = False
        self.engine  = DebateEngine()

        self._build_ui()
        QTimer.singleShot(120000, self._auto_summary)

    def _build_ui(self):
        main = QVBoxLayout(self)

        # Top controls
        top = QHBoxLayout()
        top.addWidget(QLabel("Provider:"))
        self.provider_cb = QComboBox(self)
        providers = ["Ollama"]
        if openai_client:
            providers.insert(0, "OpenAI")
        self.provider_cb.addItems(providers)
        self.provider_cb.currentTextChanged.connect(self._update_models)
        top.addWidget(self.provider_cb)

        top.addWidget(QLabel("Model:"))
        self.model_cb = QComboBox(self)
        self.model_cb.setMinimumWidth(300)
        top.addWidget(self.model_cb)
        self._update_models(self.provider_cb.currentText())

        self.mode_btn = QPushButton("Switch to Jarvis", self)
        self.mode_btn.clicked.connect(self._toggle_mode)
        top.addWidget(self.mode_btn)

        top.addWidget(QLabel("Sessions:"))
        self.session_cb = QComboBox(self)
        self._refresh_sessions()
        top.addWidget(self.session_cb)

        load_btn = QPushButton("Load", self)
        load_btn.clicked.connect(self._load_session)
        save_btn = QPushButton("Save As…", self)
        save_btn.clicked.connect(self._save_session)
        top.addWidget(load_btn)
        top.addWidget(save_btn)

        top.addStretch()
        main.addLayout(top)

        # Splitter: left panel shows bot logs, right panel the debate/jarvis pages
        splitter = QSplitter(Qt.Horizontal)

        # Left panel: logs for David (blue) and Zira (red)
        left = QWidget()
        left_l = QVBoxLayout(left)
        left_l.addWidget(QLabel("David (Blue) Log:"))
        self.david_log = QTextEdit(self)
        self.david_log.setReadOnly(True)
        self.david_log.setStyleSheet("background:#E3F2FD;")
        self.david_log.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        left_l.addWidget(self.david_log, 1)

        left_l.addWidget(QLabel("Zira (Red) Log:"))
        self.zira_log = QTextEdit(self)
        self.zira_log.setReadOnly(True)
        self.zira_log.setStyleSheet("background:#FFEBEE;")
        self.zira_log.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        left_l.addWidget(self.zira_log, 1)

        splitter.addWidget(left)
        splitter.setStretchFactor(0, 1)

        # Right panel: contains debate and jarvis pages in a stacked widget
        self.stack = QStackedWidget(self)
        self.stack.addWidget(self._debate_page())
        self.stack.addWidget(self._jarvis_page())
        splitter.addWidget(self.stack)
        splitter.setStretchFactor(1, 3)

        main.addWidget(splitter, 1)

        # Status bar
        self.status = QStatusBar(self)
        main.addWidget(self.status)

    def _refresh_sessions(self):
        self.session_cb.clear()
        if os.path.isdir(SESS_DIR):
            for fname in os.listdir(SESS_DIR):
                if fname.endswith(".json"):
                    self.session_cb.addItem(fname)

    def _update_models(self, provider):
        self.model_cb.clear()
        if provider == "OpenAI" and openai_client:
            self.model_cb.addItems(["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4"])
        else:
            try:
                tags = requests.get("http://localhost:11434/api/tags", timeout=2).json()
                models = [m["name"] for m in tags.get("models", [])]
            except Exception:
                models = ["ollama-base"]
            self.model_cb.addItems(models)

    def _debate_page(self):
        page = QWidget()
        v = QVBoxLayout(page)

        # Structure tree pane
        self.tree = QTreeWidget()
        self.tree.setHeaderLabels(["Structure"])
        self.tree.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self._populate_tree()
        self.tree.itemClicked.connect(self._on_tree_click)
        v.addWidget(self.tree, 2)

        # Context label to show current talking point info
        self.ctx = QLabel("", self)
        v.addWidget(self.ctx)

        # Chat panes for debate: one for David (blue) and one for Zira (red)
        chat_h = QHBoxLayout()
        for lbl, attr, color in [
            ("David Chat:", "blue_chat", "#E0F7FA"),
            ("Zira Chat:",  "red_chat",  "#FFEBEE")
        ]:
            col = QVBoxLayout()
            col.addWidget(QLabel(lbl))
            pane = QTextEdit(self)
            pane.setReadOnly(True)
            pane.setStyleSheet(f"background:{color};")
            pane.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
            setattr(self, attr, pane)
            col.addWidget(pane)
            chat_h.addLayout(col, 1)
        v.addLayout(chat_h, 3)

        # Bottom controls (debate controls)
        ctl = QHBoxLayout()
        self.start_btn = QPushButton("Start Debate", self)
        self.start_btn.clicked.connect(self._toggle_run)
        ctl.addWidget(self.start_btn)

        self.skip_btn = QPushButton("Skip Turn", self)
        self.skip_btn.clicked.connect(self._skip)
        ctl.addWidget(self.skip_btn)

        self.stop_btn = QPushButton("Stop Voice", self)
        self.stop_btn.clicked.connect(stop_tts)
        ctl.addWidget(self.stop_btn)

        ctl.addWidget(QLabel("Blue voice:"))
        self.blue_voice = QComboBox(self)
        self.blue_voice.addItems(["David", "Zira", "DavidVoice-1", "DavidVoice-2"])
        ctl.addWidget(self.blue_voice)

        ctl.addWidget(QLabel("Red voice:"))
        self.red_voice = QComboBox(self)
        self.red_voice.addItems(["Zira", "Zira-Alt"])
        ctl.addWidget(self.red_voice)

        self.tts_cb = QCheckBox("Enable TTS", self)
        self.tts_cb.setChecked(True)
        self.tts_cb.stateChanged.connect(lambda state: set_tts_enabled(state == Qt.Checked))
        ctl.addWidget(self.tts_cb)

        self.gen_btn = QPushButton("Auto-Generate Debate", self)
        self.gen_btn.clicked.connect(self._auto_generate)
        ctl.addWidget(self.gen_btn)

        ctl.addStretch()
        v.addLayout(ctl, 0)
        return page

    def _jarvis_page(self):
        page = QWidget()
        v = QVBoxLayout(page)

        v.addWidget(QLabel("Debate Summary:"))
        self.summary = QTextEdit(self)
        self.summary.setReadOnly(True)
        v.addWidget(self.summary, 1)

        v.addWidget(QLabel("Jarvis Conversation:"))
        self.chat_display = QTextEdit(self)
        self.chat_display.setReadOnly(True)
        v.addWidget(self.chat_display, 2)

        h = QHBoxLayout()
        self.user_in = QLineEdit(self)
        h.addWidget(self.user_in)
        btn_send = QPushButton("Send", self)
        btn_send.clicked.connect(self._handle_user)
        h.addWidget(btn_send)
        btn_mic = QPushButton("Speak", self)
        btn_mic.clicked.connect(self._voice_input)
        h.addWidget(btn_mic)
        h.addStretch()
        v.addLayout(h, 0)
        return page

    def _populate_tree(self):
        self.tree.clear()
        struct = _load_structure()
        for tp in struct.get("talking_points", []):
            tp_item = QTreeWidgetItem(self.tree, [tp["name"]])
            for topic in tp.get("topics", []):
                t_item = QTreeWidgetItem(tp_item, [topic["name"]])
                for sub in topic.get("subtopics", []):
                    QTreeWidgetItem(t_item, [sub])
        self.tree.expandAll()

    def _on_tree_click(self, item, _):
        path = []
        cur = item
        while cur:
            path.insert(0, cur.text(0))
            cur = cur.parent()
        self.ctx.setText("<b>Selected:</b> " + " > ".join(path))

    def _toggle_mode(self):
        if self.mode == "debate":
            self.mode = "jarvis"
            self.stack.setCurrentIndex(1)
            self.mode_btn.setText("Switch to Debate")
            stop_tts()
            self._do_summary()
            self.running = False
            self.start_btn.setText("Start Debate")
        else:
            self.mode = "debate"
            self.stack.setCurrentIndex(0)
            self.mode_btn.setText("Switch to Jarvis")
        self._update_context()

    def _do_summary(self):
        text = summarize()
        self.summary.setPlainText(text)
        self.status.showMessage("Jarvis summary generated", 5000)
        append_bot("Jarvis", "SUMMARY", text)
        threading.Thread(
            target=lambda: speak_text(text, self.blue_voice.currentText(), 1.0),
            daemon=True
        ).start()

    def _auto_summary(self):
        if self.mode == "jarvis":
            self._do_summary()
        QTimer.singleShot(120000, self._auto_summary)

    def _handle_user(self):
        txt = self.user_in.text().strip()
        if not txt:
            return
        append_bot("Jarvis", "USER", txt)
        delta = interpret_user(txt)
        if delta:
            msg = f"Structure updated: {delta}"
            self.chat_display.append(f"<i>{msg}</i>")
            append_bot("Jarvis", "INFO", msg)
            self._populate_tree()
            self._do_summary()
        else:
            ans = self._answer_question(txt)
            self.chat_display.append(f"<b>Jarvis:</b> {ans}")
        self.user_in.clear()

    def _answer_question(self, question):
        struct = {}
        try:
            struct = json.load(open(STRUCT_PATH, "r", encoding="utf-8"))
        except Exception:
            pass
        history = "".join(tail(50))
        prompt = (
            "You are Jarvis, an expert AI assistant summarizing a debate.\n\n"
            f"Structure:\n{json.dumps(struct, indent=2)}\n\n"
            f"History:\n{history}\n\n"
            f"User: {question}\n\n"
            "Answer using only the debate context."
        )
        if openai_client:
            try:
                resp = openai_client.chat.completions.create(
                    model=self.model_cb.currentText(),
                    messages=[{"role": "user", "content": prompt}]
                )
                ans = resp.choices[0].message.content.strip()
            except Exception as e:
                ans = f"[Error: {e}]"
        else:
            ans = "[OpenAI unavailable]"
        append_bot("Jarvis", "RESPONSE", ans)
        threading.Thread(
            target=lambda: speak_text(ans, self.blue_voice.currentText(), 1.0),
            daemon=True
        ).start()
        return ans

    def _voice_input(self):
        r = sr.Recognizer()
        try:
            with sr.Microphone() as mic:
                audio = r.listen(mic, phrase_time_limit=5)
            txt = r.recognize_google(audio)
            self.user_in.setText(txt)
            self._handle_user()
        except Exception:
            QMessageBox.warning(self, "Jarvis", "Speech not recognized.")

    def _toggle_run(self):
        self.running = not self.running
        self.start_btn.setText("Pause Debate" if self.running else "Start Debate")
        if self.running:
            self._run()

    def _run(self):
        if not self.running or self.mode != "debate":
            return
        prompt, speaker = self.engine.next_prompt()
        self._update_context()
        panel = self.david_log if speaker == "David" else self.zira_log
        panel.append(f"{speaker} PROMPT: {prompt}")
        threading.Thread(
            target=self._ask,
            args=(prompt, speaker),
            daemon=True
        ).start()

    def _ask(self, prompt, speaker):
        prov  = self.provider_cb.currentText()
        model = self.model_cb.currentText()
        try:
            if prov == "Ollama":
                resp = requests.post(
                    "http://localhost:11434/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=5
                ).json()
                text = resp.get("choices", [{"text": ""}])[0]["text"].strip()
            elif openai_client:
                resp = openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}]
                )
                text = resp.choices[0].message.content.strip()
            else:
                text = "[No AI available]"
        except Exception as e:
            text = f"Error: {e}"

        append_shared(f"{speaker}: {text}")
        append_bot(speaker, "RESPONSE", text)
        panel = self.david_log if speaker == "David" else self.zira_log
        panel.append(f"{speaker} RESPONSE: {text}")

        if self.tts_cb.isChecked():
            voice = self.blue_voice.currentText() if speaker == "David" else self.red_voice.currentText()
            threading.Thread(
                target=lambda: speak_text(text, voice, 1.1),
                daemon=True
            ).start()

        self.engine.process_response(speaker, text)
        if self.running:
            QTimer.singleShot(1000, self._run)

    def _skip(self):
        skip_tts()
        if self.running:
            self._run()

    def _update_context(self):
        tp, topic, sub = self.engine.current_context()
        self.ctx.setText(
            f"<b>TP:</b> {tp} | <b>Topic:</b> {topic} | <b>Subtopic:</b> {sub}"
        )

    def _auto_generate(self):
        """
        Always open the small chat dialog for 'Auto-Generate Debate'.
        From there you can iterate with the AI and then 'Finalize Structure'
        to inject new talking points/subtopics into your debate schema.
        """
        dlg = StructureChatDialog(self, self.engine)
        if dlg.exec() == QDialog.Accepted:
            self._populate_tree()
            self.status.showMessage("Structure updated from chat", 5000)

    def _save_session(self):
        fname, _ = QFileDialog.getSaveFileName(
            self, "Save Session", SESS_DIR, "JSON Files (*.json)"
        )
        if not fname:
            return
        session = {
            "structure": _load_structure(),
            "history": "".join(tail(1000))
        }
        with open(fname, "w", encoding="utf-8") as f:
            json.dump(session, f, indent=2)
        self._refresh_sessions()
        self.status.showMessage("Session saved", 5000)

    def _load_session(self):
        choice = self.session_cb.currentText()
        if not choice:
            QMessageBox.warning(self, "Load Session", "No session selected")
            return
        path = os.path.join(SESS_DIR, choice)
        session = json.load(open(path, "r", encoding="utf-8"))
        json.dump(session["structure"], open(STRUCT_PATH, "w", encoding="utf-8"), indent=2)
        open(HIST_PATH, "w", encoding="utf-8").write(session["history"])
        self.engine = DebateEngine()
        self._populate_tree()
        QMessageBox.information(self, "Load Session", f"Loaded {choice}")
        self.status.showMessage("Session loaded", 5000)


if __name__ == "__main__":
    app = QApplication(sys.argv)
    win = DebateAIApp()
    win.show()
    sys.exit(app.exec())
```

**Classes:** StructureChatDialog, DebateAIApp


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\Jarvis_Driver.py`

```python
#!/usr/bin/env python3
"""
File: Jarvis_Driver.py
Standalone Jarvis interface for summarizing debate state and handling structure commands.
"""

import os
import sys
import json
import threading
import requests
import speech_recognition as sr
from datetime import datetime

from PyQt6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QLabel, QTextEdit, QLineEdit, QPushButton, QComboBox,
    QStatusBar, QMessageBox
)
from PyQt6.QtCore import QTimer

from tts import speak_text, stop_tts
from core.jarvis_engine import summarize as jarvis_summarize, interpret_user
from core.engine import tail, openai_client

# === Paths & Setup ===
BASE_DIR     = os.path.dirname(os.path.abspath(__file__))
API_KEY_PATH = os.path.join(BASE_DIR, "api", "api_key.txt")
STRUCT_PATH  = os.path.join(BASE_DIR, "data", "structure.json")
HIST_PATH    = os.path.join(BASE_DIR, "data", "history.log")
JARVIS_LOG   = os.path.join(BASE_DIR, "bots", "jarvis_log.txt")

# Ensure directories exist
os.makedirs(os.path.dirname(STRUCT_PATH), exist_ok=True)
os.makedirs(os.path.dirname(HIST_PATH), exist_ok=True)
os.makedirs(os.path.dirname(JARVIS_LOG), exist_ok=True)

# Mute Debate_AI TTS
stop_tts()

# === JarvisDriver GUI ===
class JarvisDriver(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Jarvis – Debate Assistant")
        self.setGeometry(300, 200, 600, 700)
        self._build_ui()
        self._do_summary()
        QTimer.singleShot(120000, self._auto_summary)

    def _build_ui(self):
        layout = QVBoxLayout(self)

        # Voice dropdown
        hl = QHBoxLayout()
        hl.addWidget(QLabel("Jarvis Voice:"))
        self.voice_cb = QComboBox(self)
        self.voice_cb.addItems(["Zira", "David"])
        hl.addWidget(self.voice_cb)
        hl.addStretch()
        layout.addLayout(hl)

        # Summary pane
        layout.addWidget(QLabel("Debate Summary:"))
        self.summary_display = QTextEdit(self)
        self.summary_display.setReadOnly(True)
        layout.addWidget(self.summary_display)

        # Chat/Q&A pane
        layout.addWidget(QLabel("Jarvis Conversation:"))
        self.chat_display = QTextEdit(self)
        self.chat_display.setReadOnly(True)
        layout.addWidget(self.chat_display)

        # Input + buttons
        inp_layout = QHBoxLayout()
        self.user_input = QLineEdit(self)
        self.user_input.setPlaceholderText("Ask Jarvis or give structure command…")
        inp_layout.addWidget(self.user_input)

        send_btn = QPushButton("Send", self)
        send_btn.clicked.connect(self._handle_user)
        inp_layout.addWidget(send_btn)

        mic_btn = QPushButton("Speak", self)
        mic_btn.clicked.connect(self._voice_input)
        inp_layout.addWidget(mic_btn)

        layout.addLayout(inp_layout)

        # Status bar
        self.status = QStatusBar(self)
        layout.addWidget(self.status)

    def _log_jarvis(self, role, text):
        ts = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
        with open(JARVIS_LOG, "a", encoding="utf-8") as f:
            f.write(f"{ts} {role}: {text}\n\n")

    def _do_summary(self):
        summary = jarvis_summarize()
        self.summary_display.setPlainText(summary)
        self.status.showMessage("Jarvis summary generated", 5000)
        self._log_jarvis("JARVIS_SUMMARY", summary)
        voice = self.voice_cb.currentText()
        threading.Thread(target=speak_text, args=(summary, voice, 1.0), daemon=True).start()

    def _auto_summary(self):
        self._do_summary()
        QTimer.singleShot(120000, self._auto_summary)

    def _handle_user(self):
        text = self.user_input.text().strip()
        if not text:
            return
        self._log_jarvis("USER", text)

        delta = interpret_user(text)
        if delta:
            msg = f"Structure updated: {delta}"
            self.chat_display.append(f"<i>{msg}</i>")
            self.status.showMessage("Structure modified", 5000)
            self._log_jarvis("JARVIS", msg)
            self._do_summary()
        else:
            answer = self._answer_question(text)
            self.chat_display.append(f"<b>Jarvis:</b> {answer}")
            self._log_jarvis("JARVIS", answer)

        self.user_input.clear()

    def _answer_question(self, question):
        try:
            struct = json.load(open(STRUCT_PATH, "r", encoding="utf-8"))
        except Exception:
            struct = {}

        history = "".join(tail(50))
        prompt = (
            "You are Jarvis, an expert AI assistant summarizing and explaining\n"
            "an ongoing debate between two agents (David & Zira).\n\n"
            f"Structure:\n{json.dumps(struct, indent=2)}\n\n"
            f"Recent dialogue:\n{history}\n\n"
            f"User asks: {question}\n\n"
            "Provide a clear, concise answer based strictly on the debate context."
        )

        try:
            resp = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            answer = resp.choices[0].message.content.strip()
        except Exception as e:
            answer = f"[Error generating response: {e}]"

        voice = self.voice_cb.currentText()
        threading.Thread(target=speak_text, args=(answer, voice, 1.0), daemon=True).start()
        return answer

    def _voice_input(self):
        recognizer = sr.Recognizer()
        try:
            with sr.Microphone() as mic:
                audio = recognizer.listen(mic, phrase_time_limit=5)
            text = recognizer.recognize_google(audio)
            self.user_input.setText(text)
            self._handle_user()
        except Exception:
            QMessageBox.warning(self, "Jarvis", "Speech not recognized.")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    win = JarvisDriver()
    win.show()
    sys.exit(app.exec())
```

File: Jarvis_Driver.py
Standalone Jarvis interface for summarizing debate state and handling structure commands.
**Classes:** JarvisDriver


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\tts.py`

```python
#!/usr/bin/env python3
"""
File: tts.py

Advanced TTS Manager for Debate AI and Jarvis modes:

  - Monitors multiple bot logs (David, Zira, Jarvis) in real time.
  - Queues every new line for speech; no responses skipped.
  - Provides skip and stop controls.
  - Supports enable/disable via a checkbox in the GUI.
  - Uses pyttsx3 for offline TTS.
  - Exposes speak_text() for on-demand utterances.
"""

import os
import threading
import time
import queue
import pyttsx3

# === Configuration ===
BASE_DIR    = os.path.dirname(os.path.abspath(__file__))
BOTS_DIR    = os.path.join(BASE_DIR, "bots")
LOG_FILES   = [
    os.path.join(BOTS_DIR, "david_log.txt"),
    os.path.join(BOTS_DIR, "zira_log.txt"),
    os.path.join(BOTS_DIR, "jarvis_log.txt")
]
POLL_INTERVAL = 1.0  # seconds between file checks

class TTSManager:
    def __init__(self, log_paths, voice_name="Zira", speed=1.0):
        """
        :param log_paths: List of file paths to monitor for new lines.
        :param voice_name: Default voice for pyttsx3.
        :param speed: Rate multiplier (1.0 = normal speed).
        """
        self.log_paths = log_paths
        self.voice_name = voice_name
        self.speed = speed

        # File offsets to track read position
        self.offsets = {}
        for path in self.log_paths:
            try:
                with open(path, "rb") as f:
                    f.seek(0, os.SEEK_END)
                    self.offsets[path] = f.tell()
            except FileNotFoundError:
                self.offsets[path] = 0

        # Thread-safe queue of utterances
        self.queue = queue.Queue()

        # TTS engine setup
        self.engine = pyttsx3.init()
        self._configure_engine()

        # Control flags
        self.enabled = True
        self._running = True

        # Start background threads
        self._file_thread = threading.Thread(target=self._file_watcher, daemon=True)
        self._speech_thread = threading.Thread(target=self._speech_loop, daemon=True)
        self._file_thread.start()
        self._speech_thread.start()

    def _configure_engine(self):
        """Configure voice and rate on the pyttsx3 engine."""
        base_rate = self.engine.getProperty("rate")
        self.engine.setProperty("rate", int(base_rate * self.speed))
        for v in self.engine.getProperty("voices"):
            if self.voice_name.lower() in v.name.lower():
                self.engine.setProperty("voice", v.id)
                break

    def set_voice(self, voice_name: str):
        """Change the TTS voice dynamically."""
        self.voice_name = voice_name
        self._configure_engine()

    def set_speed(self, speed: float):
        """Change speech speed multiplier."""
        self.speed = speed
        self._configure_engine()

    def set_enabled(self, enabled: bool):
        """Enable or disable speaking (checkbox in UI)."""
        self.enabled = enabled

    def skip(self):
        """Stop current utterance and proceed to next in queue."""
        self.engine.stop()

    def stop(self):
        """Stop TTS manager threads and engine."""
        self._running = False
        self.engine.stop()

    def _file_watcher(self):
        """Background thread: poll log files for new lines and queue them."""
        while self._running:
            for path in self.log_paths:
                try:
                    with open(path, "r", encoding="utf-8", errors="ignore") as f:
                        f.seek(self.offsets[path])
                        lines = f.readlines()
                        self.offsets[path] = f.tell()
                except FileNotFoundError:
                    lines = []
                for line in lines:
                    text = line.strip()
                    if text:
                        self.queue.put(text)
            time.sleep(POLL_INTERVAL)

    def _speech_loop(self):
        """Background thread: read queued utterances one by one."""
        while self._running:
            try:
                utterance = self.queue.get(timeout=0.5)
            except queue.Empty:
                continue

            if not self.enabled:
                continue

            self.engine.say(utterance)
            self.engine.runAndWait()

# === Module-level manager ===
_manager: TTSManager = None

def initialize_tts(voice_name="Zira", speed=1.0, log_paths=None):
    """
    Create the global TTS manager. Call once from main GUI.
    :param voice_name: Default voice.
    :param speed: Speech rate multiplier.
    :param log_paths: Optional list of log file paths to monitor.
    """
    global _manager
    if _manager is None:
        paths = log_paths if log_paths is not None else LOG_FILES
        _manager = TTSManager(paths, voice_name, speed)

def set_tts_enabled(enabled: bool):
    """Toggle TTS on/off from GUI checkbox."""
    if _manager:
        _manager.set_enabled(enabled)

def skip_tts():
    """Skip current utterance and move to the next."""
    if _manager:
        _manager.skip()

def stop_tts():
    """Stop all TTS activity and background threads."""
    if _manager:
        _manager.stop()

def speak_text(text: str, voice_name: str = None, speed: float = None):
    """
    On-demand TTS: optionally update voice/speed, then enqueue `text`.
    :param text: The text to speak.
    :param voice_name: If provided, switch manager to this voice.
    :param speed: If provided, switch manager to this speed.
    """
    if _manager:
        if voice_name:
            _manager.set_voice(voice_name)
        if speed:
            _manager.set_speed(speed)
        _manager.queue.put(text)
```

File: tts.py

Advanced TTS Manager for Debate AI and Jarvis modes:

  - Monitors multiple bot logs (David, Zira, Jarvis) in real time.
  - Queues every new line for speech; no responses skipped.
  - Provides skip and stop controls.
  - Supports enable/disable via a checkbox in the GUI.
  - Uses pyttsx3 for offline TTS.
  - Exposes speak_text() for on-demand utterances.
**Classes:** TTSManager
**Functions:** initialize_tts(voice_name, speed, log_paths), set_tts_enabled(enabled), skip_tts(), stop_tts(), speak_text(text, voice_name, speed)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\engine.py`

```python
#!/usr/bin/env python3
"""
core/engine.py

Unified engine module for Debate AI and Jarvis modes:
  • DebateEngine       – two-bot debate flow, rule checks, monologue, rotations
  • JarvisEngine       – structure summaries & user command interpretation
  • SessionManager     – new/save/load/delete sessions
  • StructureController – CRUD: TP → Topic → Subtopic
  • PromptRouter       – builds LLM prompts with bot identity
  • RuleEngine         – enforces debate rules
  • HistoryManager     – shared & per-bot logs + tail()
  • JarvisInjector     – apply external Jarvis JSON commands
"""

import os
import json
import shutil
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import openai
import requests

# ─── Paths ─────────────────────────────────────────────────────────────────────
ROOT_DIR      = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
DATA_DIR      = os.path.join(ROOT_DIR, "data")
BOTS_DIR      = os.path.join(ROOT_DIR, "bots")
SESSIONS_DIR  = os.path.join(DATA_DIR, "sessions")
API_KEY_PATH  = os.path.join(ROOT_DIR, "api", "api_key.txt")
STRUCT_PATH   = os.path.join(DATA_DIR, "structure.json")
HIST_PATH     = os.path.join(DATA_DIR, "history.log")
ROT_PATH      = os.path.join(DATA_DIR, "rotations.json")
MONO_DIR      = os.path.join(DATA_DIR, "monologues")

BOT_LOGS = {
    "David":  os.path.join(BOTS_DIR, "david_log.txt"),
    "Zira":   os.path.join(BOTS_DIR, "zira_log.txt"),
    "Jarvis": os.path.join(BOTS_DIR, "jarvis_log.txt"),
}

# ensure dirs
for d in (DATA_DIR, BOTS_DIR, SESSIONS_DIR, MONO_DIR):
    os.makedirs(d, exist_ok=True)

# ─── API Clients ───────────────────────────────────────────────────────────────
OPENAI_KEY = ""
if os.path.isfile(API_KEY_PATH):
    OPENAI_KEY = open(API_KEY_PATH, "r", encoding="utf-8").read().strip()
if not OPENAI_KEY:
    OPENAI_KEY = os.getenv("OPENAI_API_KEY", "").strip()
openai_client = openai.OpenAI(api_key=OPENAI_KEY) if OPENAI_KEY else None
OLLAMA_URL = "http://localhost:11434/api"

# ─── History Manager ───────────────────────────────────────────────────────────
def append_shared(text: str):
    with open(HIST_PATH, "a", encoding="utf-8") as f:
        f.write(text + "\n")

def append_bot(bot: str, label: str, content: str):
    path = BOT_LOGS[bot]
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"[{label}] {content}\n")

def tail(n: int = 100) -> List[str]:
    if not os.path.exists(HIST_PATH):
        return []
    with open(HIST_PATH, "r", encoding="utf-8") as f:
        return f.readlines()[-n:]

# ─── Rule Engine ───────────────────────────────────────────────────────────────
class RuleEngine:
    @staticmethod
    def validate(text: str, context: Dict[str,str]) -> Tuple[bool,List[str]]:
        violations = []
        # Example rule: no '@' mentions
        if "@" in text:
            violations.append("No mentions allowed")
        # Extend with tone, relevance, insults checks...
        return (len(violations) == 0, violations)

# ─── Structure Controller ─────────────────────────────────────────────────────
def load_structure() -> Dict[str,Any]:
    if not os.path.exists(STRUCT_PATH):
        return {"talking_points": []}
    return json.load(open(STRUCT_PATH, "r", encoding="utf-8"))

def save_structure(struct: Dict[str,Any]):
    with open(STRUCT_PATH, "w", encoding="utf-8") as f:
        json.dump(struct, f, indent=2)

def _find_list(path: List[str], struct: Optional[Dict]=None):
    struct = struct or load_structure()
    tps = struct["talking_points"]
    if len(path) == 1:
        for i, tp in enumerate(tps):
            if tp["name"] == path[0]:
                return tps, i
    elif len(path) == 2:
        for tp in tps:
            if tp["name"] == path[0]:
                for j, topic in enumerate(tp["topics"]):
                    if topic["name"] == path[1]:
                        return tp["topics"], j
    else:
        for tp in tps:
            if tp["name"] == path[0]:
                for topic in tp["topics"]:
                    if topic["name"] == path[1]:
                        for k, sub in enumerate(topic["subtopics"]):
                            if sub == path[2]:
                                return topic["subtopics"], k
    raise KeyError(path)

def create_node(ntype: str, name: str, desc: str = "", details: str = "", parent: Optional[List[str]] = None):
    struct = load_structure()
    if ntype == "talking_point":
        struct["talking_points"].append({"name": name, "desc": desc, "details": details, "topics": []})
    else:
        lst, i = _find_list(parent, struct)
        if ntype == "topic":
            lst[i]["topics"].append({"name": name, "desc": desc, "details": details, "subtopics": []})
        elif ntype == "subtopic":
            lst[i]["subtopics"].append(name)
        else:
            raise ValueError(ntype)
    save_structure(struct)

def update_node(path: List[str], name: Optional[str] = None, desc: Optional[str] = None, details: Optional[str] = None):
    struct = load_structure()
    lst, i = _find_list(path, struct)
    node = lst[i]
    if name:
        node["name"] = name
    if desc is not None and "desc" in node:
        node["desc"] = desc
    if details is not None and "details" in node:
        node["details"] = details
    save_structure(struct)

def delete_node(path: List[str]):
    struct = load_structure()
    lst, i = _find_list(path, struct)
    del lst[i]
    save_structure(struct)

# ─── Rotations & Monologues ────────────────────────────────────────────────────
def _init_rot():
    if not os.path.exists(ROT_PATH):
        with open(ROT_PATH, "w", encoding="utf-8") as f:
            json.dump([], f)

def append_rotation(msgs: List[Dict[str, str]], resolved: bool = False) -> str:
    _init_rot()
    rots = json.load(open(ROT_PATH, "r", encoding="utf-8"))
    rid = str(uuid.uuid4())
    entry = {
        "rotation_id": rid,
        "messages": msgs,
        "resolved": resolved,
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }
    rots.append(entry)
    with open(ROT_PATH, "w", encoding="utf-8") as f:
        json.dump(rots, f, indent=2)
    return rid

def load_rotations() -> List[Dict[str, Any]]:
    _init_rot()
    return json.load(open(ROT_PATH, "r", encoding="utf-8"))

def append_monologue(bot: str, thought: str):
    mono_file = os.path.join(MONO_DIR, f"{bot.lower()}_mono.json")
    logs = []
    if os.path.exists(mono_file):
        logs = json.load(open(mono_file, "r", encoding="utf-8"))
    logs.append({
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "thought": thought
    })
    with open(mono_file, "w", encoding="utf-8") as f:
        json.dump(logs[-1000:], f, indent=2)

# ─── Jarvis Injector ───────────────────────────────────────────────────────────
def apply_queued_commands(cmds: List[Dict[str, Any]]):
    for c in cmds:
        act      = c["action"]
        ctype    = c["type"]
        path     = c.get("path")
        if act == "create":
            create_node(ctype, c["name"], c.get("desc", ""), c.get("details",""), parent=path)
        elif act == "update":
            update_node(path, c.get("name"), c.get("desc"), c.get("details"))
        elif act == "delete":
            delete_node(path)

# ─── Prompt Router ─────────────────────────────────────────────────────────────
class PromptRouter:
    def __init__(self):
        self.templates = {
            "debate": (
                "You are {bot} ({tone}). Beliefs: {beliefs}. Goals: {goals}.\n"
                "Debate subtopic '{subtopic}' under topic '{topic}' and TP '{tp}'.\n"
                "History:\n{history}\n\nRespond:"
            ),
            "jarvis_sum": "Summarize:\nStructure:\n{structure}\nHistory:\n{history}",
            "jarvis_int": "User: {txt}\nReturn JSON edits."
        }

    def debate_prompt(self, bot: str, tp: str, topic: str, sub: str,
                      ident: Dict[str,str], history: List[str]) -> str:
        return self.templates["debate"].format(
            bot=bot, tone=ident["tone"], beliefs=ident["beliefs"], goals=ident["goals"],
            tp=tp, topic=topic, subtopic=sub,
            history="\n".join(history[-5:])
        )

    def jarvis_summary(self, struct: Dict, history: List[str]) -> str:
        return self.templates["jarvis_sum"].format(
            structure=json.dumps(struct, indent=2),
            history="".join(history[-20:])
        )

    def jarvis_interpret(self, txt: str) -> str:
        return self.templates["jarvis_int"].format(txt=txt)

# ─── Debate Engine ─────────────────────────────────────────────────────────────
class DebateEngine:
    def __init__(self):
        self.struct    = load_structure()
        self.router    = PromptRouter()
        self._idx      = {"tp":0,"topic":0,"sub":0}
        self.next_bot  = "David"
        self.identity  = {
            "David": {"tone":"assertive","beliefs":"Traditional","goals":"Defend heritage"},
            "Zira":  {"tone":"analytical","beliefs":"Progressive","goals":"Advance ideas"}
        }
        self._buffer   = []

    def current_context(self) -> Tuple[str,str,str]:
        tp     = self.struct["talking_points"][self._idx["tp"]]["name"]
        topic  = self.struct["talking_points"][self._idx["tp"]]["topics"][self._idx["topic"]]["name"]
        sub    = self.struct["talking_points"][self._idx["tp"]]["topics"][self._idx["topic"]]["subtopics"][self._idx["sub"]]
        return tp, topic, sub

    def next_prompt(self) -> Tuple[str,str]:
        bot  = self.next_bot
        tp, topic, sub = self.current_context()
        ident = self.identity[bot]
        prompt = self.router.debate_prompt(bot, tp, topic, sub, ident, tail())
        self.next_bot = "Zira" if bot=="David" else "David"
        return prompt, bot

    def process_response(self, bot: str, text: str):
        ok, viol = RuleEngine.validate(text, {"tp":str(self._idx["tp"]), "topic":str(self._idx["topic"]), "sub":str(self._idx["sub"])})
        if not ok:
            append_shared(f"[Violation:{','.join(viol)}]")
            append_bot(bot, "VIOLATION", ",".join(viol))
        append_shared(f"{bot}: {text}")
        append_bot(bot, "RESPONSE", text)
        append_monologue(bot, f"Thought length {len(text)}")
        self._buffer.append({
            "bot": bot, "content": text,
            "tp": str(self._idx["tp"]), "topic": str(self._idx["topic"]), "sub": str(self._idx["sub"])
        })
        if bot == "Zira":
            rid = append_rotation(self._buffer, False)
            append_shared(f"[Rotation recorded:{rid}]")
            self._buffer.clear()
            self._advance()

    def _advance(self):
        tps    = self.struct["talking_points"]
        topics = tps[self._idx["tp"]]["topics"]
        subs   = topics[self._idx["tp"]]["subtopics"]
        self._idx["sub"] += 1
        if self._idx["sub"] >= len(subs):
            self._idx["sub"] = 0
            self._idx["topic"] += 1
            if self._idx["topic"] >= len(topics):
                self._idx["topic"] = 0
                self._idx["tp"] = (self._idx["tp"] + 1) % len(tps)

# ─── Jarvis Engine ─────────────────────────────────────────────────────────────
class JarvisEngine:
    def __init__(self):
        self.router = PromptRouter()

    def summarize(self) -> str:
        prompt = self.router.jarvis_summary(load_structure(), tail())
        return self._call_llm(prompt)

    def interpret_user(self, txt: str) -> List[Dict[str,Any]]:
        prompt = self.router.jarvis_interpret(txt)
        raw    = self._call_llm(prompt)
        try:
            cmds = json.loads(raw)
        except:
            cmds = []
        if cmds:
            apply_queued_commands(cmds)
            save_structure(load_structure())
        return cmds

    def _call_llm(self, prompt: str) -> str:
        if openai_client:
            resp = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"user","content":prompt}]
            )
            return resp.choices[0].message.content.strip()
        else:
            r = requests.post(f"{OLLAMA_URL}/generate", json={"model":"ollama-base","prompt":prompt, "stream": False})
            return r.json().get("choices",[{"text":""}])[0]["text"].strip()

# ─── Session Manager ───────────────────────────────────────────────────────────
class SessionManager:
    @staticmethod
    def new():
        save_structure({"talking_points":[]})
        open(HIST_PATH,"w").close()
        open(ROT_PATH,"w").close()
        for log in BOT_LOGS.values():
            open(log,"w").close()
        for bot in BOT_LOGS:
            open(os.path.join(MONO_DIR,f"{bot.lower()}_mono.json"),"w").close()

    @staticmethod
    def save(name: str):
        dst = os.path.join(SESSIONS_DIR, name)
        os.makedirs(dst, exist_ok=True)
        for fn in ("structure.json","history.log","rotations.json"):
            shutil.copy2(os.path.join(DATA_DIR, fn), os.path.join(dst, fn))
        for bot, log in BOT_LOGS.items():
            shutil.copy2(log, os.path.join(dst, os.path.basename(log)))
            mono = os.path.join(MONO_DIR, f"{bot.lower()}_mono.json")
            shutil.copy2(mono, os.path.join(dst, os.path.basename(mono)))

    @staticmethod
    def load(name: str):
        src = os.path.join(SESSIONS_DIR, name)
        for fn in ("structure.json","history.log","rotations.json"):
            shutil.copy2(os.path.join(src, fn), os.path.join(DATA_DIR, fn))
        for bot, log in BOT_LOGS.items():
            shutil.copy2(os.path.join(src, os.path.basename(log)), log)
            shutil.copy2(os.path.join(src, f"{bot.lower()}_mono.json"),
                        os.path.join(MONO_DIR, f"{bot.lower()}_mono.json"))

# ─── Exports ───────────────────────────────────────────────────────────────────
__all__ = [
    "DebateEngine", "JarvisEngine", "SessionManager",
    "load_structure", "save_structure", "create_node", "update_node", "delete_node",
    "append_shared", "append_bot", "tail", "PromptRouter"
]
```

core/engine.py

Unified engine module for Debate AI and Jarvis modes:
  • DebateEngine       – two-bot debate flow, rule checks, monologue, rotations
  • JarvisEngine       – structure summaries & user command interpretation
  • SessionManager     – new/save/load/delete sessions
  • StructureController – CRUD: TP → Topic → Subtopic
  • PromptRouter       – builds LLM prompts with bot identity
  • RuleEngine         – enforces debate rules
  • HistoryManager     – shared & per-bot logs + tail()
  • JarvisInjector     – apply external Jarvis JSON commands
**Classes:** RuleEngine, PromptRouter, DebateEngine, JarvisEngine, SessionManager
**Functions:** append_shared(text), append_bot(bot, label, content), tail(n), load_structure(), save_structure(struct), _find_list(path, struct), create_node(ntype, name, desc, details, parent), update_node(path, name, desc, details), delete_node(path), _init_rot(), append_rotation(msgs, resolved), load_rotations(), append_monologue(bot, thought), apply_queued_commands(cmds)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\history.py`

```python
#!/usr/bin/env python3
"""
core/history.py

Advanced HistoryManager for Debate AI sessions:

  - Per-session storage under data/sessions/{session}/
    ├── logs/
    │     ├── David_monologue.json
    │     └── Zira_monologue.json
    ├── debate/
    │     └── debate_rotations.json
  - Global shared log in data/history.log
  - API for appending bot and user messages
"""

import os
import json
from datetime import datetime
from typing import List, Dict, Any, Optional

# ─── Paths & Helpers ───────────────────────────────────────────────────────────

ROOT_DIR     = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
DATA_DIR     = os.path.join(ROOT_DIR, "data")
SESSIONS_DIR = os.path.join(DATA_DIR, "sessions")
LOG_FILE     = os.path.join(DATA_DIR, "history.log")

def _ensure_dirs(session: str):
    base   = os.path.join(SESSIONS_DIR, session)
    logs   = os.path.join(base, "logs")
    debate = os.path.join(base, "debate")
    os.makedirs(logs, exist_ok=True)
    os.makedirs(debate, exist_ok=True)
    return logs, debate

def _load_json(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        return []

def _save_json(path: str, data: List[Dict[str, Any]]):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

# ─── HistoryManager (Per-Session Rotation/Monologue) ──────────────────────────

class HistoryManager:
    def __init__(self, session: str):
        self.session = session
        self.logs_dir, self.debate_dir = _ensure_dirs(session)
        self.bot_files = {
            "David": os.path.join(self.logs_dir, "David_monologue.json"),
            "Zira":  os.path.join(self.logs_dir, "Zira_monologue.json"),
        }
        self.rotations_file = os.path.join(self.debate_dir, "debate_rotations.json")

    # ─── Monologue APIs ───────────────────────────────────────────────────────

    def load_monologue(self, bot_name: str) -> List[Dict[str, Any]]:
        path = self.bot_files.get(bot_name)
        return _load_json(path) if path else []

    def add_monologue_entry(
        self,
        bot: str,
        thought: str,
        response: str,
        topic_id: str,
        subtopic_id: str,
        talking_point_id: str
    ) -> Dict[str, Any]:
        path = self.bot_files.get(bot)
        if not path:
            raise ValueError(f"Unknown bot '{bot}'")

        entries = _load_json(path)
        entry = {
            "id":                f"m_{bot[0].lower()}_{len(entries)+1:03d}",
            "bot":               bot,
            "thought":           thought,
            "response":          response,
            "talking_point_id":  talking_point_id,
            "topic_id":          topic_id,
            "subtopic_id":       subtopic_id,
            "timestamp":         datetime.utcnow().isoformat() + "Z"
        }
        entries.append(entry)
        _save_json(path, entries[-1000:])
        return entry

    def get_recent_monologues(self, bot_name: str, limit: int = 100) -> List[Dict[str, Any]]:
        return self.load_monologue(bot_name)[-limit:]

    # ─── Debate Rotation APIs ─────────────────────────────────────────────────

    def load_debate_rotations(self) -> List[Dict[str, Any]]:
        return _load_json(self.rotations_file)

    def add_debate_rotation(
        self,
        david_msg: str,
        zira_msg: str,
        topic_id: str,
        subtopic_id: str,
        talking_point_id: str,
        resolved: bool = False
    ) -> Dict[str, Any]:
        entries = _load_json(self.rotations_file)
        rotation = {
            "rotation_id": f"r{len(entries)+1:03d}",
            "messages": [
                {
                    "bot": "David",
                    "content": david_msg,
                    "talking_point_id": talking_point_id,
                    "topic_id": topic_id,
                    "subtopic_id": subtopic_id
                },
                {
                    "bot": "Zira",
                    "content": zira_msg,
                    "talking_point_id": talking_point_id,
                    "topic_id": topic_id,
                    "subtopic_id": subtopic_id
                }
            ],
            "resolved":  resolved,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        entries.append(rotation)
        _save_json(self.rotations_file, entries)
        return rotation

    def get_recent_rotations(self, limit: int = 100) -> List[Dict[str, Any]]:
        return self.load_debate_rotations()[-limit:]

# ─── Global Shared History ────────────────────────────────────────────────────

def append_shared(text: str):
    ts = datetime.utcnow().isoformat() + "Z"
    line = f"[{ts}] {text.strip()}\n"
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(line)

def append_bot(bot: str, role: str, content: str):
    line = f"{bot.upper()} [{role.upper()}]: {content.strip()}"
    append_shared(line)

def tail(n: int = 50) -> List[str]:
    if not os.path.exists(LOG_FILE):
        return []
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
    return lines[-n:]

# ─── Exports ──────────────────────────────────────────────────────────────────

__all__ = [
    "HistoryManager",
    "append_shared",
    "append_bot",
    "tail"
]
```

core/history.py

Advanced HistoryManager for Debate AI sessions:

  - Per-session storage under data/sessions/{session}/
    ├── logs/
    │     ├── David_monologue.json
    │     └── Zira_monologue.json
    ├── debate/
    │     └── debate_rotations.json
  - Global shared log in data/history.log
  - API for appending bot and user messages
**Classes:** HistoryManager
**Functions:** _ensure_dirs(session), _load_json(path), _save_json(path, data), append_shared(text), append_bot(bot, role, content), tail(n)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\jarvis_engine.py`

```python
#!/usr/bin/env python3
"""
core/jarvis_engine.py

Jarvis summarization and structure-editing engine:

  - Summarize debate structure & recent rotations
  - Interpret free-form user commands into structure-editing JSON patches
  - Apply those patches via StructureController
"""

import os
import json
import requests
from typing import Any, Dict, List, Optional

from core.structure import apply_queued_commands, load_structure, save_structure
from core.history import HistoryManager
from core.engine import PromptRouter, openai_client, OLLAMA_URL

# =============================================================================
# JarvisEngine
# =============================================================================

class JarvisEngine:
    """
    Handles:
      1. Summarization of the current debate state (structure + recent rotations)
      2. Interpretation of user input into structure edit commands
    """

    def __init__(self, session: str = "default"):
        self.session = session
        self.history_mgr = HistoryManager(session)
        self.router = PromptRouter()

    def summarize(self) -> str:
        """
        Generate a summary of structure and recent rotation history.
        """
        try:
            struct = load_structure()
            rotations = self.history_mgr.get_recent_rotations(limit=20)
            hist_lines = [
                f'{msg["bot"]}: {msg["content"]}'
                for rot in rotations
                for msg in rot.get("messages", [])
            ]
            prompt = self.router.jarvis_summary(struct, hist_lines)
            return self._call_llm(prompt)
        except Exception as e:
            return f"[Summary Error: {e}]"

    def interpret_user(self, user_text: str) -> List[Dict[str, Any]]:
        """
        Interpret user text into structure-editing commands and apply them.
        """
        if not user_text.strip():
            return []
        try:
            prompt = self.router.jarvis_interpret(user_text)
            raw = self._call_llm(prompt)
            cmds = json.loads(raw)
            if not isinstance(cmds, list):
                return []
            apply_queued_commands(cmds)
            save_structure(load_structure())
            return cmds
        except Exception:
            return []

    def _call_llm(self, prompt: str) -> str:
        """
        Call OpenAI or Ollama with a prompt.
        """
        try:
            if openai_client:
                resp = openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}]
                )
                return resp.choices[0].message.content.strip()
            else:
                r = requests.post(
                    f"{OLLAMA_URL}/generate",
                    json={"model": "ollama-base", "prompt": prompt, "stream": False},
                    timeout=10
                )
                data = r.json()
                return data.get("choices", [{"text": ""}])[0]["text"].strip()
        except Exception as e:
            return f"[LLM Error: {e}]"

# =============================================================================
# Module API for Debate_AI.py
# =============================================================================

_jarvis_instance: Optional[JarvisEngine] = None

def _ensure_jarvis() -> JarvisEngine:
    global _jarvis_instance
    if _jarvis_instance is None:
        _jarvis_instance = JarvisEngine()
    return _jarvis_instance

def summarize() -> str:
    return _ensure_jarvis().summarize()

def interpret_user(user_text: str) -> List[Dict[str, Any]]:
    return _ensure_jarvis().interpret_user(user_text)

# =============================================================================
# Exports
# =============================================================================

__all__ = [
    "JarvisEngine",
    "summarize",
    "interpret_user"
]
```

core/jarvis_engine.py

Jarvis summarization and structure-editing engine:

  - Summarize debate structure & recent rotations
  - Interpret free-form user commands into structure-editing JSON patches
  - Apply those patches via StructureController
**Classes:** JarvisEngine
**Functions:** _ensure_jarvis(), summarize(), interpret_user(user_text)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\prompt_router.py`

```python
#!/usr/bin/env python3
"""
core/prompt_router.py

Builds and routes prompts for both Debate and Jarvis modes, injecting
bot identities, debate context, and user instructions into LLM-friendly templates.
"""

import json
from typing import Dict, List

class PromptRouter:
    """
    Central prompt‐building logic. Supports:

      • debate_turn: for David/Zira back‐and‐forth under a subtopic
      • jarvis_summary: for summarizing structure + history
      • jarvis_interpret: for translating user text into JSON edit commands
    """

    DEFAULT_TEMPLATES = {
        "debate_turn": (
            "You are {bot}.\n"
            "Identity:\n"
            "  Name: {bot}\n"
            "  Tone: {tone}\n"
            "  Beliefs: {beliefs}\n"
            "  Goals: {goals}\n\n"
            "Debate this subtopic:\n"
            "  Talking Point: {tp}\n"
            "  Topic: {topic}\n"
            "  Subtopic: {subtopic}\n\n"
            "Recent Dialogue:\n"
            "{history}\n\n"
            "Respond now, engaging the argument accordingly."
        ),
        "jarvis_summary": (
            "Summarize the following debate session.\n\n"
            "Structure:\n{structure}\n\n"
            "History:\n{history}\n\n"
            "Provide a concise, human‐readable summary."
        ),
        "jarvis_interpret": (
            "A user instruction is given below:\n\n"
            "\"{user_text}\"\n\n"
            "Translate this instruction into a JSON array of commands to modify the debate structure. "
            "Each command should be an object with keys: action (create/update/delete), type "
            "(talking_point/topic/subtopic), path (array of names), name, desc (optional), details (optional)."
        )
    }

    def __init__(self, templates: Dict[str, str] = None):
        self.templates = templates or self.DEFAULT_TEMPLATES

    def build_debate_prompt(
        self,
        bot: str,
        identity: Dict[str, str],
        tp: str,
        topic: str,
        subtopic: str,
        history: List[str]
    ) -> str:
        """
        Build the prompt for a debate turn.

        :param bot: "David" or "Zira"
        :param identity: dict with keys 'tone','beliefs','goals'
        :param tp: current talking point name
        :param topic: current topic name
        :param subtopic: current subtopic name
        :param history: last N lines of shared chat history
        """
        template = self.templates["debate_turn"]
        return template.format(
            bot=bot,
            tone=identity.get("tone", ""),
            beliefs=identity.get("beliefs", ""),
            goals=identity.get("goals", ""),
            tp=tp,
            topic=topic,
            subtopic=subtopic,
            history="\n".join(history[-5:])
        )

    def jarvis_summary(self, structure: Dict, history: List[str]) -> str:
        """
        Build the prompt for a Jarvis summary.

        :param structure: the debate structure dict
        :param history: last N lines of shared chat history
        """
        template = self.templates["jarvis_summary"]
        return template.format(
            structure=json.dumps(structure, indent=2),
            history="\n".join(history[-20:])
        )

    def jarvis_interpret(self, user_text: str) -> str:
        """
        Build the prompt for interpreting a user's instruction.

        :param user_text: raw instruction from the user
        """
        template = self.templates["jarvis_interpret"]
        return template.format(user_text=user_text)
```

core/prompt_router.py

Builds and routes prompts for both Debate and Jarvis modes, injecting
bot identities, debate context, and user instructions into LLM-friendly templates.
**Classes:** PromptRouter


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\core\structure.py`

```python
#!/usr/bin/env python3
"""
core/structure.py

Unified structure, session, history, monologue, and rotation management:

  - Load/save structure.json
  - CRUD for talking points, topics, subtopics
  - Apply “inject” commands from Jarvis (JSON patches)
  - Monologue logs per bot (JSON arrays)
  - Debate rotation logs (JSON list of rotations)
  - New/load/save/delete sessions (structure + history + bot logs + monologues + rotations)
"""

import os
import json
import shutil
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

# ─── Paths & Constants ─────────────────────────────────────────────────────────

PROJECT_ROOT   = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
DATA_DIR       = os.path.join(PROJECT_ROOT, "data")
STRUCT_PATH    = os.path.join(DATA_DIR, "structure.json")
HIST_PATH      = os.path.join(DATA_DIR, "history.log")
ROTATIONS_PATH = os.path.join(DATA_DIR, "rotations.json")
SESSIONS_DIR   = os.path.join(DATA_DIR, "sessions")

BOTS_DIR       = os.path.join(PROJECT_ROOT, "bots")
BOT_LOGS       = {
    "David":  os.path.join(BOTS_DIR, "david_log.txt"),
    "Zira":   os.path.join(BOTS_DIR, "zira_log.txt"),
    "Jarvis": os.path.join(BOTS_DIR, "jarvis_log.txt"),
}
MONOLOGUE_DIR  = os.path.join(DATA_DIR, "monologues")

# Ensure directories exist
for path in (DATA_DIR, SESSIONS_DIR, BOTS_DIR, MONOLOGUE_DIR):
    os.makedirs(path, exist_ok=True)

# ─── Structure I/O ─────────────────────────────────────────────────────────────

def load_structure() -> Dict[str, Any]:
    """Load or initialize an empty structure."""
    if not os.path.exists(STRUCT_PATH) or os.path.getsize(STRUCT_PATH) == 0:
        default = {"talking_points": []}
        save_structure(default)
        return default
    with open(STRUCT_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def save_structure(struct: Dict[str, Any]):
    """Persist the structure to disk."""
    with open(STRUCT_PATH, "w", encoding="utf-8") as f:
        json.dump(struct, f, indent=2)

# ─── History Reset ──────────────────────────────────────────────────────────────

def clear_history_and_logs():
    """Wipe shared history, rotations, and each bot's logs & monologues."""
    open(HIST_PATH, "w", encoding="utf-8").close()
    open(ROTATIONS_PATH, "w", encoding="utf-8").close()
    for path in BOT_LOGS.values():
        open(path, "w", encoding="utf-8").close()
    for bot in BOT_LOGS:
        mono = os.path.join(MONOLOGUE_DIR, f"{bot.lower()}_mono.json")
        with open(mono, "w", encoding="utf-8") as f:
            json.dump([], f)

# ─── Monologue Logs ────────────────────────────────────────────────────────────

def append_monologue(bot: str, thought: str):
    """Append an inner-thought entry to bot's monologue JSON log."""
    path = os.path.join(MONOLOGUE_DIR, f"{bot.lower()}_mono.json")
    logs: List[Dict[str, Any]] = []
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            logs = json.load(f)
    logs.append({
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "thought": thought
    })
    # keep only last 1000
    with open(path, "w", encoding="utf-8") as f:
        json.dump(logs[-1000:], f, indent=2)

def load_monologue(bot: str) -> List[Dict[str, Any]]:
    """Load full monologue JSON for a bot."""
    path = os.path.join(MONOLOGUE_DIR, f"{bot.lower()}_mono.json")
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ─── Rotation Logs ─────────────────────────────────────────────────────────────

def _init_rotations():
    if not os.path.exists(ROTATIONS_PATH) or os.path.getsize(ROTATIONS_PATH) == 0:
        with open(ROTATIONS_PATH, "w", encoding="utf-8") as f:
            json.dump([], f)

def append_rotation(messages: List[Dict[str, Any]], resolved: bool = False) -> str:
    """
    Add a debate rotation entry.
    messages: [
      {"bot":..., "content":..., "tp":..., "topic":..., "sub":...}, ...
    ]
    Returns new rotation_id.
    """
    _init_rotations()
    with open(ROTATIONS_PATH, "r+", encoding="utf-8") as f:
        rotations = json.load(f)
        rotation_id = str(uuid.uuid4())
        entry = {
            "rotation_id": rotation_id,
            "messages": messages,
            "resolved": resolved,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        rotations.append(entry)
        f.seek(0)
        json.dump(rotations, f, indent=2)
        f.truncate()
    return rotation_id

def load_rotations() -> List[Dict[str, Any]]:
    """Return the list of all rotations."""
    _init_rotations()
    with open(ROTATIONS_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def update_rotation_resolution(rotation_id: str, resolved: bool):
    """Set the `resolved` flag on an existing rotation."""
    rots = load_rotations()
    for rot in rots:
        if rot["rotation_id"] == rotation_id:
            rot["resolved"] = resolved
            break
    with open(ROTATIONS_PATH, "w", encoding="utf-8") as f:
        json.dump(rots, f, indent=2)

# ─── Session Management ────────────────────────────────────────────────────────

def list_sessions() -> List[str]:
    """Return all saved session names."""
    return sorted([
        name for name in os.listdir(SESSIONS_DIR)
        if os.path.isdir(os.path.join(SESSIONS_DIR, name))
    ])

def new_session():
    """
    Clear live data for a fresh session:
      structure, history, rotations, bot logs, monologues.
    """
    save_structure({"talking_points": []})
    clear_history_and_logs()

def save_session(name: str):
    """
    Save structure, history, rotations, bot logs, and monologues
    under data/sessions/{name}/
    """
    if not name.strip() or any(c in name for c in r'\/:*?"<>|'):
        raise ValueError("Invalid session name")
    dest = os.path.join(SESSIONS_DIR, name)
    os.makedirs(dest, exist_ok=True)

    # copy core files
    for fname in ("structure.json", "history.log", "rotations.json"):
        shutil.copy2(os.path.join(DATA_DIR, fname), os.path.join(dest, fname))

    # copy bot logs and monologues
    for bot, log in BOT_LOGS.items():
        base = bot.lower()
        txt_src = log
        json_src = os.path.join(MONOLOGUE_DIR, f"{base}_mono.json")
        shutil.copy2(txt_src, os.path.join(dest, f"{base}_log.txt"))
        shutil.copy2(json_src, os.path.join(dest, f"{base}_mono.json"))

def load_session(name: str):
    """
    Load session data from data/sessions/{name}/ into live directory.
    """
    srcdir = os.path.join(SESSIONS_DIR, name)
    if not os.path.isdir(srcdir):
        raise FileNotFoundError(f"No session '{name}'")
    for fname in ("structure.json", "history.log", "rotations.json"):
        shutil.copy2(os.path.join(srcdir, fname), os.path.join(DATA_DIR, fname))
    for bot in BOT_LOGS.keys():
        base = bot.lower()
        txt_dst = BOT_LOGS[bot]
        json_dst = os.path.join(MONOLOGUE_DIR, f"{base}_mono.json")
        shutil.copy2(os.path.join(srcdir, f"{base}_log.txt"), txt_dst)
        shutil.copy2(os.path.join(srcdir, f"{base}_mono.json"), json_dst)

def delete_session(name: str):
    """Remove a saved session."""
    dest = os.path.join(SESSIONS_DIR, name)
    if os.path.isdir(dest):
        shutil.rmtree(dest)
    else:
        raise FileNotFoundError(f"No session '{name}'")

# ─── CRUD Operations ───────────────────────────────────────────────────────────

def _find_list_and_index(
    path: List[str], struct: Optional[Dict[str, Any]] = None
) -> Tuple[List[Any], int]:
    """
    Given ['TP'], ['TP','Topic'], or ['TP','Topic','Subtopic'],
    return (parent_list, idx) where that element lives.
    """
    struct = struct or load_structure()
    tps = struct["talking_points"]

    # talking_point
    if len(path) == 1:
        for i, tp in enumerate(tps):
            if tp["name"] == path[0]:
                return tps, i
        raise KeyError(f"TP '{path[0]}' not found.")

    # topic
    if len(path) == 2:
        for tp in tps:
            if tp["name"] == path[0]:
                for j, topic in enumerate(tp["topics"]):
                    if topic["name"] == path[1]:
                        return tp["topics"], j
                raise KeyError(f"Topic '{path[1]}' not in TP '{path[0]}'.")
        raise KeyError(f"TP '{path[0]}' not found.")

    # subtopic
    if len(path) == 3:
        for tp in tps:
            if tp["name"] == path[0]:
                for topic in tp["topics"]:
                    if topic["name"] == path[1]:
                        for k, sub in enumerate(topic["subtopics"]):
                            if sub == path[2]:
                                return topic["subtopics"], k
                        raise KeyError(f"Sub '{path[2]}' not in Topic '{path[1]}'.")
        raise KeyError(f"TP '{path[0]}' not found.")

    raise ValueError(f"Invalid path length: {path}")

def create_node(
    node_type: str,
    name: str,
    desc: str = "",
    details: str = "",
    parent_path: Optional[List[str]] = None
):
    """Create a new talking_point, topic or subtopic."""
    struct = load_structure()
    if node_type == "talking_point":
        struct["talking_points"].append({
            "name": name, "desc": desc, "details": details, "topics": []
        })
    else:
        lst, idx = _find_list_and_index(parent_path, struct)
        if node_type == "topic":
            lst[idx]["topics"].append({
                "name": name, "desc": desc, "details": details, "subtopics": []
            })
        elif node_type == "subtopic":
            lst[idx]["subtopics"].append(name)
        else:
            raise ValueError(f"Unknown node_type '{node_type}'")
    save_structure(struct)

def update_node(
    path: List[str],
    name: Optional[str] = None,
    desc: Optional[str] = None,
    details: Optional[str] = None
):
    """Update fields of an existing node identified by path."""
    struct = load_structure()
    lst, idx = _find_list_and_index(path, struct)
    node = lst[idx]
    if name:
        node["name"] = name
    if desc is not None and "desc" in node:
        node["desc"] = desc
    if details is not None and "details" in node:
        node["details"] = details
    save_structure(struct)

def delete_node(path: List[str]):
    """Delete a node and its subtree."""
    struct = load_structure()
    lst, idx = _find_list_and_index(path, struct)
    del lst[idx]
    save_structure(struct)

def apply_queued_commands(commands: List[Dict[str, Any]]):
    """
    Apply a list of Jarvis-generated commands:
      {action, type, path, name, desc, details}
    """
    for cmd in commands:
        act = cmd["action"]
        ntype = cmd["type"]
        path  = cmd.get("path")
        if act == "create":
            create_node(ntype, cmd["name"], cmd.get("desc",""), cmd.get("details",""), parent_path=path)
        elif act == "update":
            update_node(path, name=cmd.get("name"), desc=cmd.get("desc"), details=cmd.get("details"))
        elif act == "delete":
            delete_node(path)

# ─── Utility ──────────────────────────────────────────────────────────────────

def get_node(path: List[str]) -> Any:
    """Return the node at the given path."""
    struct = load_structure()
    lst, idx = _find_list_and_index(path, struct)
    return lst[idx]

# Aliases
get_all_sessions = list_sessions
```

core/structure.py

Unified structure, session, history, monologue, and rotation management:

  - Load/save structure.json
  - CRUD for talking points, topics, subtopics
  - Apply “inject” commands from Jarvis (JSON patches)
  - Monologue logs per bot (JSON arrays)
  - Debate rotation logs (JSON list of rotations)
  - New/load/save/delete sessions (structure + history + bot logs + monologues + rotations)
**Functions:** load_structure(), save_structure(struct), clear_history_and_logs(), append_monologue(bot, thought), load_monologue(bot), _init_rotations(), append_rotation(messages, resolved), load_rotations(), update_rotation_resolution(rotation_id, resolved), list_sessions(), new_session(), save_session(name), load_session(name), delete_session(name), _find_list_and_index(path, struct), create_node(node_type, name, desc, details, parent_path), update_node(path, name, desc, details), delete_node(path), apply_queued_commands(commands), get_node(path)


## Module `Debate_AI_Old\Debate_AI_fail\Debate_AI\ui\config_panel.py`

```python
# Config panel UI (optional)
```



## Module `Debate_AI_Old\old\tts.py`

```python
#!/usr/bin/env python3
"""
tts.py – Text-to-Speech Manager for Debate_AI
---------------------------------------------
Provides robust, non-blocking TTS for multiple bots ("Red" and "Blue") using pyttsx3.
Features:
 - Listing available voices
 - Assigning voices per bot
 - Adjusting rate and volume
 - Sentence-by-sentence speaking with skip & stop controls
 - speak() for single messages, speak_history() for history.txt
"""

import os
import re
import threading
import pyttsx3

# Path to the debate history file
SCRIPT_DIR   = os.path.dirname(os.path.abspath(__file__))
HISTORY_PATH = os.path.join(SCRIPT_DIR, "history.txt")

class TTSManager:
    def __init__(self):
        # Initialize pyttsx3 engine
        self.engine = pyttsx3.init()
        # Available voices list
        self.voices = self.engine.getProperty("voices")
        # Map bot names to voice IDs
        self.voice_map = {}
        # Threading lock for engine access
        self._lock = threading.Lock()
        # Default rate and volume
        self.rate   = self.engine.getProperty("rate")
        self.volume = self.engine.getProperty("volume")
        # Control flags
        self._stop_global  = False
        self._skip_current = False
        # Thread handle
        self._thread = None
        # Assign defaults (Zira→Red, David→Blue or first two)
        self._initialize_default_voices()

    def _initialize_default_voices(self):
        """Default: 'Zira'→Red, 'David'→Blue; fallback to first two voices."""
        zira  = next((v.id for v in self.voices if "zira"  in v.name.lower()), None)
        david = next((v.id for v in self.voices if "david" in v.name.lower()), None)

        if zira:  self.voice_map["Red"]  = zira
        if david: self.voice_map["Blue"] = david

        # Fallback to first voices
        if "Red" not in self.voice_map and self.voices:
            self.voice_map["Red"] = self.voices[0].id
        if "Blue" not in self.voice_map:
            self.voice_map["Blue"] = (self.voices[1].id if len(self.voices)>1 
                                      else self.voice_map.get("Red"))

    def list_voices(self):
        """Return list of (voice_id, voice_name) for all installed voices."""
        return [(v.id, v.name) for v in self.voices]

    def set_bot_voice(self, bot_name: str, voice_id: str):
        """Assign a specific voice ID to 'Red' or 'Blue'."""
        if bot_name not in ("Red", "Blue"):
            raise ValueError("bot_name must be 'Red' or 'Blue'")
        if voice_id not in [v.id for v in self.voices]:
            raise ValueError(f"Voice ID '{voice_id}' not available")
        self.voice_map[bot_name] = voice_id

    def set_rate(self, rate: int):
        """Set speech rate (words per minute) for all subsequent speech."""
        with self._lock:
            self.rate = rate
            self.engine.setProperty("rate", rate)

    def set_volume(self, volume: float):
        """Set volume (0.0 to 1.0) for all subsequent speech."""
        if not 0.0 <= volume <= 1.0:
            raise ValueError("Volume must be between 0.0 and 1.0")
        with self._lock:
            self.volume = volume
            self.engine.setProperty("volume", volume)

    def _speak_worker(self, bot_name: str, text: str):
        """
        Internal: speak `text` sentence by sentence.
        Honors _skip_current and _stop_global flags.
        """
        # Reset flags for this run
        self._stop_global  = False
        self._skip_current = False

        # Split into sentences (keep punctuation)
        sentences = re.split(r'(?<=[.!?])\s+', text)

        for sentence in sentences:
            if self._stop_global:
                break
            with self._lock:
                # Apply voice, rate, volume
                self.engine.setProperty("voice",  self.voice_map.get(bot_name))
                self.engine.setProperty("rate",   self.rate)
                self.engine.setProperty("volume", self.volume)
                try:
                    self.engine.say(sentence)
                    self.engine.runAndWait()
                except Exception as e:
                    print(f"[TTS Error] {e}")
            # If user requested skip, clear skip flag and continue
            if self._skip_current:
                self._skip_current = False
                continue

    def speak(self, bot_name: str, text: str):
        """
        Speak the given text as `bot_name` in a dedicated background thread.
        Stops any in-progress speech first.
        """
        # Stop any ongoing speech
        self.stop()
        # Launch new thread
        self._thread = threading.Thread(
            target=self._speak_worker, args=(bot_name, text), daemon=True
        )
        self._thread.start()

    def speak_history(self, bot_name: str):
        """
        Read the entire history.txt and speak it in one continuous session.
        """
        if not os.path.exists(HISTORY_PATH):
            print(f"[TTS Warning] No history file at {HISTORY_PATH}")
            return
        with open(HISTORY_PATH, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        if content:
            self.speak(bot_name, content)

    def skip(self):
        """
        Skip the remainder of the current sentence, moving to the next.
        """
        self._skip_current = True
        with self._lock:
            self.engine.stop()

    def stop(self):
        """
        Immediately stop all ongoing speech.
        """
        self._stop_global = True
        with self._lock:
            self.engine.stop()


# ---------------- Example Usage ----------------
if __name__ == "__main__":
    import time

    tts = TTSManager()

    print("Available voices:")
    for vid, name in tts.list_voices():
        print(f"  {vid}: {name}")

    # Test speaking
    tts.speak("Red",  "Hello. This is a test of the Red bot. Each sentence stops at punctuation.")
    time.sleep(2)
    tts.skip()   # skip to next sentence
    time.sleep(1)
    tts.speak("Blue", "Now the Blue bot speaks. It also respects skip and stop commands.")
    time.sleep(3)
    tts.stop()   # stop entirely

    # Test reading history.txt
    with open(HISTORY_PATH, 'w', encoding='utf-8') as f:
        f.write("Agent A: First point. Agent A: Second point.\nAgent B: Counterpoint.")
    print("Reading history:")
    tts.speak_history("Red")
    time.sleep(10)
```

tts.py – Text-to-Speech Manager for Debate_AI
---------------------------------------------
Provides robust, non-blocking TTS for multiple bots ("Red" and "Blue") using pyttsx3.
Features:
 - Listing available voices
 - Assigning voices per bot
 - Adjusting rate and volume
 - Sentence-by-sentence speaking with skip & stop controls
 - speak() for single messages, speak_history() for history.txt
**Classes:** TTSManager


## Module `Dual_AI\Analyze_All.py`

```python

```



## Module `Dual_AI\Analyze_folders.py`

```python
import os

def analyze_folders(start_path):
    script_name = os.path.basename(__file__)  # Get script file name
    analyze_file = os.path.join(start_path, "analyze.txt")  # Output file path

    with open(analyze_file, "w", encoding="utf-8") as file:
        file.write(f"Folder Analysis Report\n")
        file.write(f"{'='*50}\n\n")
        file.write(f"Root Directory: {start_path}\n\n")
        
        for root, dirs, files in os.walk(start_path):
            # Skip directories named 'venv'
            if "venv" in root.split(os.sep):
                continue

            level = root.replace(start_path, "").count(os.sep)
            indent = "|   " * level  # Tree structure formatting
            file.write(f"{indent}|-- {os.path.basename(root)}/\n")

            # Filter out the script and output file from the list of files
            filtered_files = [f for f in files if f not in {script_name, "analyze.txt"}]

            # List files in the directory
            for f in filtered_files:
                file_indent = "|   " * (level + 1)
                file.write(f"{file_indent}|-- {f}\n")
    
    print(f"Analysis complete. Results saved in {analyze_file}")

if __name__ == "__main__":
    script_directory = os.path.dirname(os.path.abspath(__file__))
    analyze_folders(script_directory)
```

**Functions:** analyze_folders(start_path)


## Module `Dual_AI\Dual_AI.py`

```python
# Dual_AI.py

import os, sys, json, subprocess, threading, requests
from PyQt6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout, QGridLayout,
    QPushButton, QLabel, QComboBox, QTextEdit, QLineEdit, QCheckBox,
    QFileDialog, QMessageBox
)
from PyQt6.QtCore import Qt, QTimer

from tts import TTSManager
from debate_engine import DebateEngine

ROOT = os.path.dirname(os.path.abspath(__file__))
STAGING_PATH = os.path.join(ROOT, "staging", "idea_output.json")
SESSIONS_DIR = os.path.join(ROOT, "sessions")
os.makedirs(SESSIONS_DIR, exist_ok=True)

class DualAIGUI(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Dual_AI Debate System")
        self.setMinimumSize(1000, 700)
        self.setStyleSheet("background-color: #111; color: #ddd;")
        self.tts = TTSManager()
        self.idea_process = None
        self.debate = None

        self.monitor_timer = QTimer()
        self.monitor_timer.timeout.connect(self.check_idea_output)

        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout(self)

        # Session Row
        session_row = QHBoxLayout()
        self.session_combo = QComboBox()
        self.refresh_sessions()
        self.new_session_button = QPushButton("New Session")
        self.new_session_button.clicked.connect(self.create_session)
        session_row.addWidget(QLabel("Session:"))
        session_row.addWidget(self.session_combo)
        session_row.addWidget(self.new_session_button)
        layout.addLayout(session_row)

        # Bot Config
        bot_layout = QGridLayout()
        self.red_persona = QLineEdit()
        self.blue_persona = QLineEdit()
        self.red_provider = QComboBox()
        self.red_model = QComboBox()
        self.blue_provider = QComboBox()
        self.blue_model = QComboBox()

        for cb in (self.red_provider, self.blue_provider):
            cb.addItems(["OpenAI", "Ollama"])
            cb.currentTextChanged.connect(self.update_models)

        bot_layout.addWidget(QLabel("Red Bot (David)"), 0, 0)
        bot_layout.addWidget(QLabel("Blue Bot (Zira)"), 0, 2)

        bot_layout.addWidget(QLabel("Provider:"), 1, 0)
        bot_layout.addWidget(self.red_provider, 1, 1)
        bot_layout.addWidget(QLabel("Provider:"), 1, 2)
        bot_layout.addWidget(self.blue_provider, 1, 3)

        bot_layout.addWidget(QLabel("Model:"), 2, 0)
        bot_layout.addWidget(self.red_model, 2, 1)
        bot_layout.addWidget(QLabel("Model:"), 2, 2)
        bot_layout.addWidget(self.blue_model, 2, 3)

        bot_layout.addWidget(QLabel("Persona:"), 3, 0)
        bot_layout.addWidget(self.red_persona, 3, 1)
        bot_layout.addWidget(QLabel("Persona:"), 3, 2)
        bot_layout.addWidget(self.blue_persona, 3, 3)

        layout.addLayout(bot_layout)
        self.update_models()

        # Conversation Display
        self.center_text = QTextEdit()
        self.center_text.setReadOnly(True)
        self.center_text.setStyleSheet("background-color: #222; color: #ccc;")
        layout.addWidget(QLabel("🟣 Conversation Window"))
        layout.addWidget(self.center_text)

        # Control Row
        ctrl_row = QHBoxLayout()
        self.start_button = QPushButton("Start Debate")
        self.stop_button = QPushButton("Stop")
        self.clear_button = QPushButton("Clear")
        self.idea_checkbox = QCheckBox("Enable IDEA")
        self.idea_checkbox.stateChanged.connect(self.toggle_idea)

        self.start_button.clicked.connect(self.start_debate)
        self.stop_button.clicked.connect(self.stop_debate)
        self.clear_button.clicked.connect(self.clear_text)

        ctrl_row.addWidget(self.start_button)
        ctrl_row.addWidget(self.stop_button)
        ctrl_row.addWidget(self.clear_button)
        ctrl_row.addStretch()
        ctrl_row.addWidget(self.idea_checkbox)
        layout.addLayout(ctrl_row)

        # TTS row
        tts_row = QHBoxLayout()
        self.tts_enable = QCheckBox("Enable TTS")
        self.tts_skip = QPushButton("Skip")
        self.tts_stop = QPushButton("Stop TTS")
        self.tts_test = QPushButton("Test TTS")
        self.tts_test.clicked.connect(self.test_tts)
        tts_row.addWidget(self.tts_enable)
        tts_row.addWidget(self.tts_test)
        tts_row.addWidget(self.tts_skip)
        tts_row.addWidget(self.tts_stop)
        layout.addLayout(tts_row)

    def refresh_sessions(self):
        self.session_combo.clear()
        sessions = [d for d in os.listdir(SESSIONS_DIR) if os.path.isdir(os.path.join(SESSIONS_DIR, d))]
        self.session_combo.addItems(sessions)

    def create_session(self):
        name, ok = QFileDialog.getSaveFileName(self, "New Session", SESSIONS_DIR, options=QFileDialog.Option.DontUseNativeDialog)
        if name:
            name = os.path.basename(name)
            os.makedirs(os.path.join(SESSIONS_DIR, name), exist_ok=True)
            self.refresh_sessions()

    def update_models(self):
        red_models = ["gpt-4o", "gpt-4", "gpt-3.5-turbo"] if self.red_provider.currentText() == "OpenAI" else ["phi4:latest", "mistral:latest", "deepseek-r1:14b"]
        blue_models = ["gpt-4o", "gpt-4", "gpt-3.5-turbo"] if self.blue_provider.currentText() == "OpenAI" else ["phi4:latest", "mistral:latest", "deepseek-r1:14b"]
        self.red_model.clear()
        self.blue_model.clear()
        self.red_model.addItems(red_models)
        self.blue_model.addItems(blue_models)

    def toggle_idea(self, state):
        if state == Qt.CheckState.Checked.value:
            self.launch_idea()
        else:
            self.close_idea()

    def launch_idea(self):
        if not self.idea_process:
            script = os.path.join(ROOT, "idea_bot.py")
            self.idea_process = subprocess.Popen([sys.executable, script])
            self.monitor_timer.start(2000)

    def close_idea(self):
        if self.idea_process:
            self.idea_process.terminate()
            self.idea_process = None
        self.monitor_timer.stop()

    def check_idea_output(self):
        if os.path.exists(STAGING_PATH):
            try:
                with open(STAGING_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f)
                cat = data.get("category", "Unknown Category")
                topic = data.get("topic", "Unknown Topic")
                subs = data.get("subtopics", [])
                display = f"[IDEA Input]\nCategory: {cat}\nTopic: {topic}\nSubtopics:\n - " + "\n - ".join(subs)
                self.center_text.append(display)
                os.remove(STAGING_PATH)
            except Exception as e:
                self.center_text.append(f"[IDEA Error] {e}")

    def start_debate(self):
        try:
            schema = json.loads(open(STAGING_PATH, "r", encoding="utf-8").read())
        except Exception as e:
            QMessageBox.warning(self, "Schema Error", f"Failed to load debate schema: {e}")
            return

        red_cfg = {
            "provider": self.red_provider.currentText(),
            "model": self.red_model.currentText(),
            "persona": self.red_persona.text() or "You are David, a strong debater."
        }
        blue_cfg = {
            "provider": self.blue_provider.currentText(),
            "model": self.blue_model.currentText(),
            "persona": self.blue_persona.text() or "You are Zira, an insightful thinker."
        }

        self.debate = DebateEngine(
            schema=schema,
            red_config=red_cfg,
            blue_config=blue_cfg,
            on_output=self.on_debate_output,
            on_complete=self.on_debate_complete
        )
        self.center_text.append("[System] Debate started.")
        self.debate.start()

    def stop_debate(self):
        if self.debate:
            self.debate.stop()
            self.center_text.append("[System] Debate stopped.")
            self.debate = None

    def on_debate_output(self, role, text):
        self.center_text.append(f"<b>{role}:</b> {text}")
        if self.tts_enable.isChecked():
            self.tts.speak(role, text)

    def on_debate_complete(self):
        self.center_text.append("[System] Debate finished.")
        self.debate = None

    def test_tts(self):
        if self.tts_enable.isChecked():
            self.tts.speak("Red", "Testing voice for Red bot.")
            self.tts.speak("Blue", "Testing voice for Blue bot.")

    def clear_text(self):
        self.center_text.clear()

# Main entry
if __name__ == "__main__":
    app = QApplication(sys.argv)
    win = DualAIGUI()
    win.show()
    sys.exit(app.exec())
```

**Classes:** DualAIGUI


## Module `Dual_AI\Quick_error.py`

```python
import subprocess
import time
import os

# Define the log file path (write next to wherever the .py is run from)
log_file_path = os.path.join(os.getcwd(), 'quick_error.log')

def monitor_bot():
    with open(log_file_path, 'w') as log_file:
        log_file.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] QuickError started monitoring Dual_AI.py\n")

    try:
        process = subprocess.Popen(
            ['python', r'C:\Users\Art PC\Desktop\Dual_AI\Dual_AI.py'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        with open(log_file_path, 'a') as log_file:
            while True:
                output = process.stdout.readline()
                error = process.stderr.readline()

                if error:
                    log_line = f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] ERROR: {error.strip()}\n"
                    print(log_line)
                    log_file.write(log_line)
                    log_file.flush()

                if output:
                    print(f"[OUTPUT] {output.strip()}")

                if process.poll() is not None:
                    break

            log_file.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Dual_AI.py has exited.\n")

    except Exception as e:
        with open(log_file_path, 'a') as log_file:
            log_file.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Exception: {str(e)}\n")

if __name__ == "__main__":
    print("QuickError is monitoring Dual_AI.py for errors...")
    monitor_bot()
    print("Dual_AI Monitor has stopped. QuickError is closing.")
```

**Functions:** monitor_bot()


## Module `Talking_Bots\Analyze_All.py`

```python

```



## Module `Talking_Bots\Analyze_folders.py`

```python
#!/usr/bin/env python3
"""
Analyze_All.py – Conversation Analysis & Visualization

Provides a GUI to browse debate sessions, categories, topics, and threads,
and perform analysis on any selected conversation file:
 • Parses conversation into turns (user / Blue Bot / Red Bot)
 • Computes basic metrics: turns count, avg tokens per turn, word counts
 • Generates an AI‐driven summary (via OpenAI)
 • Displays a simple matplotlib plot of turn lengths
"""

import sys
import os
import json
import time

from PyQt6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout, QLabel,
    QPushButton, QTreeWidget, QTreeWidgetItem, QPlainTextEdit,
    QFileDialog, QMessageBox
)
from PyQt6.QtCore import Qt

import openai
import tiktoken
import matplotlib.pyplot as plt

# -------------------- GLOBAL PATHS & CONFIG --------------------
BASE_DIR     = os.path.dirname(os.path.abspath(__file__))
SESSIONS_DIR = os.path.join(BASE_DIR, "sessions")
PROJECTS_DIR = os.path.join(BASE_DIR, "projects")
CONFIG_PATH  = os.path.join(BASE_DIR, "config", "settings.json")

with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    CONFIG = json.load(f)
openai.api_key = CONFIG["openai_api_key"]
ENCODER = tiktoken.encoding_for_model(CONFIG["model"])

# -------------------- PARSE CONVERSATION --------------------
def parse_conversation(file_path):
    """
    Returns a list of (speaker, text) tuples in order.
    Speakers are 'Blue Bot' or 'Red Bot'.
    """
    turns = []
    if not os.path.isfile(file_path):
        return turns
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("Blue Bot:"):
                turns.append(("Blue Bot", line[len("Blue Bot:"):].strip()))
            elif line.startswith("Red Bot:"):
                turns.append(("Red Bot", line[len("Red Bot:"):].strip()))
            else:
                # continuation of previous
                if turns:
                    speaker, prev = turns[-1]
                    turns[-1] = (speaker, prev + "\n" + line)
    return turns

# -------------------- COMPUTE METRICS --------------------
def compute_metrics(turns):
    """
    Given list of (speaker, text), return:
      - total_turns
      - avg_tokens_per_turn
      - word_count per speaker
    """
    total = len(turns)
    token_counts = [len(ENCODER.encode(text)) for _, text in turns]
    avg_tokens = sum(token_counts) / total if total else 0
    word_counts = {}
    for speaker, text in turns:
        wc = len(text.split())
        word_counts[speaker] = word_counts.get(speaker, 0) + wc
    return {
        "total_turns": total,
        "avg_tokens": avg_tokens,
        "word_counts": word_counts,
        "token_counts": token_counts
    }

# -------------------- AI SUMMARY --------------------
def ai_summary(turns):
    """
    Calls OpenAI to produce a concise summary of the debate.
    """
    convo = "\n".join(f"{spk}: {txt}" for spk, txt in turns[-10:])  # last 10 turns
    prompt = (
        "Summarize the key arguments and outcome of the following debate:\n\n"
        f"{convo}\n\n"
        "Provide bullet points of main arguments by each bot and any resolution."
    )
    resp = openai.ChatCompletion.create(
        model=CONFIG["model"],
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return resp.choices[0].message.content.strip()

# -------------------- MAIN APP --------------------
class AnalysisApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Conversation Analyzer")
        self.resize(1000, 700)
        self._build_ui()
        self._populate_tree()

    def _build_ui(self):
        main = QHBoxLayout(self)
        # Left: tree
        self.tree = QTreeWidget()
        self.tree.setHeaderLabel("Sessions / Projects")
        self.tree.itemClicked.connect(self._on_tree_item)
        main.addWidget(self.tree, 1)

        # Right: analysis pane
        pane = QVBoxLayout()

        self.metrics_label = QLabel("Select a conversation to analyze.")
        self.metrics_label.setWordWrap(True)
        pane.addWidget(self.metrics_label)

        btn_layout = QHBoxLayout()
        self.analyze_btn = QPushButton("Analyze", clicked=self._analyze_selected)
        btn_layout.addWidget(self.analyze_btn)
        self.plot_btn    = QPushButton("Show Plot",  clicked=self._show_plot)
        btn_layout.addWidget(self.plot_btn)
        pane.addLayout(btn_layout)

        self.summary_edit = QPlainTextEdit()
        self.summary_edit.setReadOnly(True)
        pane.addWidget(self.summary_edit, 1)

        main.addLayout(pane, 2)

    def _populate_tree(self):
        self.tree.clear()
        # Sessions
        sess_root = QTreeWidgetItem(self.tree, ["<Sessions>"])
        for f in sorted(os.listdir(SESSIONS_DIR)):
            if f.endswith(".json"):
                QTreeWidgetItem(sess_root, [f])
        # Projects / Topics / Threads
        proj_root = QTreeWidgetItem(self.tree, ["<Projects>"])
        for cat in sorted(os.listdir(PROJECTS_DIR)):
            cat_item = QTreeWidgetItem(proj_root, [cat])
            cat_path = os.path.join(PROJECTS_DIR, cat)
            for topic in sorted(os.listdir(cat_path)):
                topic_item = QTreeWidgetItem(cat_item, [topic])
                threads_dir = os.path.join(cat_path, topic, "workspace",)
                if os.path.isdir(threads_dir):
                    for fname in sorted(os.listdir(threads_dir)):
                        if fname.endswith(".txt"):
                            thread_item = QTreeWidgetItem(topic_item, [fname])

        self.tree.expandAll()

    def _on_tree_item(self, item, _):
        # store selected path
        names = []
        it = item
        while it:
            names.append(it.text(0))
            it = it.parent()
        names.reverse()
        # Determine file path
        if names[0] == "<Projects>" and len(names) == 4:
            cat, topic, fname = names[1], names[2], names[3]
            self.selected_file = os.path.join(PROJECTS_DIR, cat, topic, "workspace", fname)
        else:
            self.selected_file = None
        self.metrics_label.setText(f"Selected: {item.text(0)}")

    def _analyze_selected(self):
        if not getattr(self, "selected_file", None):
            QMessageBox.warning(self, "No file", "Please select a thread file under Projects.")
            return
        turns = parse_conversation(self.selected_file)
        if not turns:
            QMessageBox.information(self, "Empty", "No turns found in file.")
            return
        metrics = compute_metrics(turns)
        text = (
            f"Total turns: {metrics['total_turns']}\n"
            f"Avg tokens/turn: {metrics['avg_tokens']:.1f}\n"
            f"Word counts: {metrics['word_counts']}\n"
        )
        self.metrics_label.setText(text)
        # AI summary
        summary = ai_summary(turns)
        self.summary_edit.setPlainText(summary)
        self.current_metrics = metrics

    def _show_plot(self):
        if not hasattr(self, "current_metrics"):
            QMessageBox.warning(self, "No data", "Run analysis first.")
            return
        tokens = self.current_metrics["token_counts"]
        plt.figure()
        plt.plot(range(1, len(tokens)+1), tokens, marker='o')
        plt.title("Tokens per Turn")
        plt.xlabel("Turn #")
        plt.ylabel("Token Count")
        plt.grid(True)
        plt.show()

# -------------------- ENTRYPOINT --------------------
if __name__ == "__main__":
    app = QApplication(sys.argv)
    win = AnalysisApp()
    win.show()
    sys.exit(app.exec())
```

Analyze_All.py – Conversation Analysis & Visualization

Provides a GUI to browse debate sessions, categories, topics, and threads,
and perform analysis on any selected conversation file:
 • Parses conversation into turns (user / Blue Bot / Red Bot)
 • Computes basic metrics: turns count, avg tokens per turn, word counts
 • Generates an AI‐driven summary (via OpenAI)
 • Displays a simple matplotlib plot of turn lengths
**Classes:** AnalysisApp
**Functions:** parse_conversation(file_path), compute_metrics(turns), ai_summary(turns)


## Module `Talking_Bots\run_all.py`

```python
#!/usr/bin/env python3
"""
run_all.py – Launcher for the Talking_Bots System

This script starts the TTS daemon (tts.py) in the background and then
launches the main Talking Bots GUI (talking_bots.py). Logs both outputs
to separate files under logs/. Ensures that all required directories
exist before startup.
"""

import os
import sys
import subprocess
import time
import signal

# -------------------- PATH SETUP --------------------
BASE_DIR       = os.path.dirname(os.path.abspath(__file__))
SCRIPTS_DIR    = BASE_DIR  # all scripts live here
LOG_DIR        = os.path.join(BASE_DIR, "logs")
TTS_LOG        = os.path.join(LOG_DIR, "tts_daemon.log")
GUI_LOG        = os.path.join(LOG_DIR, "talking_bots.log")
TTS_SCRIPT     = os.path.join(SCRIPTS_DIR, "tts.py")
GUI_SCRIPT     = os.path.join(SCRIPTS_DIR, "talking_bots.py")

for d in (LOG_DIR,):
    os.makedirs(d, exist_ok=True)

# -------------------- LAUNCH TTS DAEMON --------------------
print("Starting TTS daemon...")
tts_out = open(TTS_LOG, "a", encoding="utf-8")
tts_err = open(TTS_LOG, "a", encoding="utf-8")
tts_proc = subprocess.Popen(
    [sys.executable, TTS_SCRIPT],
    stdout=tts_out,
    stderr=tts_err,
    cwd=SCRIPTS_DIR
)
time.sleep(1)  # give tts.py a moment to initialize

# -------------------- LAUNCH TALKING_BOTS GUI --------------------
print("Starting Talking_Bots GUI...")
gui_out = open(GUI_LOG, "a", encoding="utf-8")
gui_err = open(GUI_LOG, "a", encoding="utf-8")
try:
    gui_proc = subprocess.Popen(
        [sys.executable, GUI_SCRIPT],
        stdout=gui_out,
        stderr=gui_err,
        cwd=SCRIPTS_DIR
    )
except Exception as e:
    print(f"Failed to launch GUI: {e}")
    tts_proc.terminate()
    sys.exit(1)

# -------------------- WAIT & CLEANUP --------------------
def shutdown(signum, frame):
    print("Shutting down processes...")
    if gui_proc.poll() is None:
        gui_proc.terminate()
    if tts_proc.poll() is None:
        tts_proc.terminate()
    sys.exit(0)

# Handle SIGINT/SIGTERM for graceful exit
signal.signal(signal.SIGINT, shutdown)
signal.signal(signal.SIGTERM, shutdown)

# Wait for GUI to exit; then stop TTS and exit
gui_proc.wait()
print("GUI exited; stopping TTS daemon.")
if tts_proc.poll() is None:
    tts_proc.terminate()
print("Shutdown complete.")
```

run_all.py – Launcher for the Talking_Bots System

This script starts the TTS daemon (tts.py) in the background and then
launches the main Talking Bots GUI (talking_bots.py). Logs both outputs
to separate files under logs/. Ensures that all required directories
exist before startup.
**Functions:** shutdown(signum, frame)


## Module `Talking_Bots\talking_bots.py`

```python
#!/usr/bin/env python3
"""
talking_bots.py – Autonomous Dual-Agent Debate System

Two AI agents (“Blue Bot” and “Red Bot”) debate a topic in a central purple canvas.
Features:
 • Session / Category / Topic / Subtopic hierarchy
 • Persistent conversation history per session
 • Autonomous turn-taking via QThread
 • Syntax-highlighted, collapsible code blocks for any code in responses
 • Simple TTS hook (delegates to tts.py)
"""

import sys
import os
import json
import time
import re
import threading

from PyQt6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout, QLabel,
    QPushButton, QComboBox, QScrollArea, QPlainTextEdit,
    QFrame, QInputDialog, QMessageBox
)
from PyQt6.QtGui import QFont, QColor, QSyntaxHighlighter, QTextCharFormat
from PyQt6.QtCore import Qt, QThread, pyqtSignal

import openai
import tiktoken

# -------------------- GLOBAL PATHS & CONFIG --------------------
BASE_DIR       = os.path.dirname(os.path.abspath(__file__))
SESSIONS_DIR   = os.path.join(BASE_DIR, "sessions")
PROJECTS_DIR   = os.path.join(BASE_DIR, "projects")
CONFIG_DIR     = os.path.join(BASE_DIR, "config")
LOG_DIR        = os.path.join(BASE_DIR, "logs")
for d in (SESSIONS_DIR, PROJECTS_DIR, CONFIG_DIR, LOG_DIR):
    os.makedirs(d, exist_ok=True)

DEFAULT_CFG = {
    "openai_api_key": "",
    "model":            "gpt-4o-mini",
    "temperature":      0.7,
    "max_turns":        10
}
CFG_PATH = os.path.join(CONFIG_DIR, "settings.json")
if not os.path.exists(CFG_PATH):
    with open(CFG_PATH, "w", encoding="utf-8") as f:
        json.dump(DEFAULT_CFG, f, indent=4)
with open(CFG_PATH, "r", encoding="utf-8") as f:
    CONFIG = json.load(f)

openai.api_key = CONFIG["openai_api_key"]
ENCODER = tiktoken.encoding_for_model(CONFIG["model"])

# -------------------- LOGGING --------------------
def log(msg):
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{timestamp}] {msg}\n"
    with open(os.path.join(LOG_DIR, "debate.log"), "a", encoding="utf-8") as lf:
        lf.write(line)

# -------------------- SYNTAX HIGHLIGHTER --------------------
class PythonHighlighter(QSyntaxHighlighter):
    def __init__(self, doc):
        super().__init__(doc)
        self.rules = []
        # Keywords
        fmt_kw = QTextCharFormat()
        fmt_kw.setForeground(QColor("#569CD6"))
        for kw in ["def","class","if","else","while","for","try","except","return","import","from"]:
            self.rules.append((re.compile(rf"\b{kw}\b"), fmt_kw))
        # Strings
        fmt_str = QTextCharFormat()
        fmt_str.setForeground(QColor("#CE9178"))
        self.rules.append((re.compile(r"'.*?'"), fmt_str))
        self.rules.append((re.compile(r'".*?"'), fmt_str))
        # Comments
        fmt_c = QTextCharFormat()
        fmt_c.setForeground(QColor("#6A9955"))
        self.rules.append((re.compile(r"#.*"), fmt_c))

    def highlightBlock(self, text):
        for pattern, fmt in self.rules:
            for m in pattern.finditer(text):
                self.setFormat(m.start(), m.end()-m.start(), fmt)

# -------------------- CODE BLOCK WIDGET --------------------
class CodeBlockWidget(QWidget):
    def __init__(self, code: str):
        super().__init__()
        self.layout = QVBoxLayout(self)
        self.editor = QPlainTextEdit()
        self.editor.setReadOnly(True)
        self.editor.setFont(QFont("Courier", 10))
        self.editor.setPlainText(code)
        PythonHighlighter(self.editor.document())
        # start collapsed
        full_h = self.editor.document().size().height() * self.editor.fontMetrics().height() + 10
        self.editor.setFixedHeight(min(full_h, 150))
        self.layout.addWidget(self.editor)

# -------------------- BOT AGENT --------------------
class BotAgent:
    def __init__(self, name: str, color: QColor, system_prompt: str):
        self.name = name
        self.color = color
        self.sys = system_prompt

    def respond(self, history: str) -> str:
        msgs = [
            {"role": "system", "content": self.sys},
            {"role": "user",   "content": history}
        ]
        res = openai.ChatCompletion.create(
            model=CONFIG["model"],
            messages=msgs,
            temperature=CONFIG["temperature"]
        )
        return res.choices[0].message.content.strip()

# -------------------- DEBATE WORKER --------------------
class DebateWorker(QThread):
    new_msg = pyqtSignal(str, str, QColor)
    finished = pyqtSignal()

    def __init__(self, blue: BotAgent, red: BotAgent, topic: str, max_turns: int):
        super().__init__()
        self.blue = blue
        self.red  = red
        self.topic = topic
        self.max_turns = max_turns
        self._running = True

    def run(self):
        history = f"Topic: {self.topic}\n\n"
        turn = 0
        current = self.blue
        while self._running and turn < self.max_turns:
            try:
                resp = current.respond(history)
            except Exception as e:
                resp = f"<Error: {e}>"
            log(f"{current.name} → {resp}")
            self.new_msg.emit(current.name, resp, current.color)
            history += f"{current.name}: {resp}\n\n"
            current = self.red if current is self.blue else self.blue
            turn += 1
            time.sleep(0.5)
        self.finished.emit()

    def stop(self):
        self._running = False

# -------------------- SESSION MANAGER --------------------
class SessionManager:
    def __init__(self, combo: QComboBox):
        self.combo = combo
        self._refresh()

    def _refresh(self):
        self.combo.clear()
        names = [f[:-5] for f in os.listdir(SESSIONS_DIR) if f.endswith(".json")]
        self.combo.addItems(["<New>"] + names)

    def load(self, name: str) -> dict:
        if name == "<New>": return {}
        path = os.path.join(SESSIONS_DIR, name + ".json")
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    def save(self, name: str, data: dict):
        path = os.path.join(SESSIONS_DIR, name + ".json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        self._refresh()

# -------------------- CONVERSATION CANVAS --------------------
class ConversationCanvas(QWidget):
    def __init__(self):
        super().__init__()
        self.setStyleSheet("background-color: #4B0082;")  # purple
        self.vbox = QVBoxLayout(self)
        self.vbox.setSpacing(10)
        self.vbox.setContentsMargins(5,5,5,5)

    def add_message(self, bot: str, text: str, color: QColor):
        frm = QFrame()
        frm.setStyleSheet(f"background-color: {color.name()}; border-radius:5px;")
        lay = QVBoxLayout(frm)
        lbl = QLabel(f"<b>{bot}:</b>")
        lbl.setStyleSheet("color:white;")
        lay.addWidget(lbl)
        # split out code blocks
        parts = re.split(r"```(?:\w+)?\n(.*?)```", text, flags=re.DOTALL)
        for i, part in enumerate(parts):
            if i % 2 == 1:
                lay.addWidget(CodeBlockWidget(part.strip()))
            else:
                txt = QPlainTextEdit()
                txt.setReadOnly(True)
                txt.setPlainText(part.strip())
                txt.setStyleSheet("background:#1E1E1E;color:#DDD;font-family:Courier;")
                txt.setFixedHeight(min(100, txt.document().size().height()*txt.fontMetrics().height()+10))
                lay.addWidget(txt)
        self.vbox.addWidget(frm)
        # auto-scroll
        parent = self.parent().scroll_area.verticalScrollBar()
        parent.setValue(parent.maximum())

# -------------------- MAIN APP --------------------
class DebateApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Dual-Agent Debate")
        self.resize(900, 650)

        # Layout
        main = QVBoxLayout(self)

        # Session / Category / Topic / Subtopic row
        row1 = QHBoxLayout()
        row1.addWidget(QLabel("Session:"))
        self.sess_cb = QComboBox()
        self.sess_mgr = SessionManager(self.sess_cb)
        row1.addWidget(self.sess_cb)
        row1.addWidget(QPushButton("Save", clicked=self._save_session))
        row1.addStretch()
        row1.addWidget(QLabel("Category:"))
        self.cat_cb = QComboBox()
        self._load_categories()
        row1.addWidget(self.cat_cb)
        row1.addWidget(QLabel("Topic:"))
        self.topic_le = QPlainTextEdit(); self.topic_le.setFixedHeight(40)
        row1.addWidget(self.topic_le)
        main.addLayout(row1)

        # Canvas
        self.scroll_area = QScrollArea()
        self.scroll_area.setWidgetResizable(True)
        self.canvas = ConversationCanvas()
        self.scroll_area.setWidget(self.canvas)
        main.addWidget(self.scroll_area, 1)

        # Controls
        row2 = QHBoxLayout()
        self.start_btn = QPushButton("Start Debate", clicked=self._start)
        self.stop_btn  = QPushButton("Stop Debate",  clicked=self._stop)
        self.stop_btn.setEnabled(False)
        row2.addWidget(self.start_btn)
        row2.addWidget(self.stop_btn)
        row2.addStretch()
        main.addLayout(row2)

        # Signals
        self.sess_cb.currentTextChanged.connect(self._load_session)

        self.worker = None
        self.session_data = {}

    def _load_categories(self):
        cats = [d for d in os.listdir(PROJECTS_DIR) if os.path.isdir(os.path.join(PROJECTS_DIR, d))]
        self.cat_cb.clear()
        self.cat_cb.addItems(["<New>"] + cats)

    def _load_session(self, name: str):
        if self.worker and self.worker.isRunning(): return
        data = self.sess_mgr.load(name)
        self.session_data = data
        self.topic_le.setPlainText(data.get("topic", ""))
        self.canvas.vbox.takeAt(0)  # clear
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)

    def _save_session(self):
        name, ok = QInputDialog.getText(self, "Session Name", "Enter name:")
        if not ok or not name.strip(): return
        self.session_data["topic"] = self.topic_le.toPlainText().strip()
        self.sess_mgr.save(name.strip(), self.session_data)
        QMessageBox.information(self, "Saved", f"Session '{name}' saved.")

    def _start(self):
        topic = self.topic_le.toPlainText().strip()
        if not topic:
            QMessageBox.warning(self, "Error", "Enter a topic first."); return
        # clear
        while self.canvas.vbox.count():
            w = self.canvas.vbox.takeAt(0).widget()
            if w: w.deleteLater()
        # setup bots
        blue = BotAgent("Blue Bot", QColor("#1E90FF"), "You are Blue Bot; argue coherently.")
        red  = BotAgent("Red Bot",  QColor("#DC143C"), "You are Red Bot; counter Blue Bot's points.")
        self.worker = DebateWorker(blue, red, topic, CONFIG["max_turns"])
        self.worker.new_msg.connect(self.canvas.add_message)
        self.worker.finished.connect(self._on_finish)
        self.worker.start()
        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)

    def _stop(self):
        if self.worker:
            self.worker.stop()
            self.stop_btn.setEnabled(False)

    def _on_finish(self):
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        QMessageBox.information(self, "Debate", "Debate finished.")

# -------------------- ENTRYPOINT --------------------
if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = DebateApp()
    window.show()
    sys.exit(app.exec())
```

talking_bots.py – Autonomous Dual-Agent Debate System

Two AI agents (“Blue Bot” and “Red Bot”) debate a topic in a central purple canvas.
Features:
 • Session / Category / Topic / Subtopic hierarchy
 • Persistent conversation history per session
 • Autonomous turn-taking via QThread
 • Syntax-highlighted, collapsible code blocks for any code in responses
 • Simple TTS hook (delegates to tts.py)
**Classes:** PythonHighlighter, CodeBlockWidget, BotAgent, DebateWorker, SessionManager, ConversationCanvas, DebateApp
**Functions:** log(msg)


## Module `Talking_Bots\tts.py`

```python
#!/usr/bin/env python3
"""
TTS.py – Text‑to‑Speech Manager for Debate Simulator
• Reads history.txt top‑to‑bottom, grouping by speaker tags
• Speaks each full response in one chunk with the correct voice
• Watches for new lines and continues until stopped
• Persists voice selections across sessions
"""

import os
import time
import threading
import re
import queue
import json
import pyttsx3
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# where we store persistent TTS settings
SCRIPT_DIR   = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH  = os.path.join(SCRIPT_DIR, "tts_config.json")

class HistoryMonitor(FileSystemEventHandler):
    def __init__(self, history_path, callback):
        self.history_path  = history_path
        self.callback      = callback
        self.lock          = threading.Lock()
        self.last_read_pos = 0

    def _safe_read(self):
        # try a few times if the file is locked
        for _ in range(5):
            try:
                with open(self.history_path, 'r', encoding='utf-8') as f:
                    f.seek(self.last_read_pos)
                    text = f.read()
                    self.last_read_pos = f.tell()
                    return text
            except (IOError, OSError):
                time.sleep(0.1)
        return ""

    def on_modified(self, event):
        if event.src_path == self.history_path:
            with self.lock:
                new_text = self._safe_read()
            if new_text:
                for line in new_text.splitlines():
                    self.callback(line)

class TTSManager:
    def __init__(self):
        # initialize engine and build default voice map
        self.engine   = pyttsx3.init()
        voices        = self.engine.getProperty('voices')
        self.voice_map = {}
        for v in voices:
            nl = v.name.lower()
            if 'david' in nl:
                self.voice_map['David'] = v.id
            if 'zira'  in nl:
                self.voice_map['Zira']  = v.id
        # fallbacks
        if 'David' not in self.voice_map and voices:
            self.voice_map['David'] = voices[0].id
        if 'Zira' not in self.voice_map and voices:
            self.voice_map['Zira'] = voices[-1].id

        # load saved voice names
        self._config_names = {}
        self._load_config()

        # reading control
        self.auto_read_enabled = True
        self.queue             = queue.Queue()
        self._stop_event       = threading.Event()
        self._current_speaker  = None
        self._buffer_lines     = []

        # one‐at‐a‐time semaphore
        self._semaphore = threading.Semaphore(1)

        # start dispatcher thread
        self.dispatcher = threading.Thread(target=self._dispatch_loop, daemon=True)
        self.dispatcher.start()

        self.observer     = None
        self.history_path = None

    def _load_config(self):
        if os.path.isfile(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as cf:
                    data = json.load(cf)
                name_to_id = {v.name: v.id for v in self.engine.getProperty('voices')}
                for speaker, vname in data.items():
                    vid = name_to_id.get(vname)
                    if vid:
                        self.voice_map[speaker]    = vid
                        self._config_names[speaker] = vname
            except Exception:
                pass

    def _save_config(self):
        try:
            with open(CONFIG_PATH, 'w', encoding='utf-8') as cf:
                json.dump(self._config_names, cf, indent=2)
        except Exception:
            pass

    def set_session_path(self, session_path):
        """Point to a fresh history.txt for this session."""
        self.history_path = os.path.join(session_path, 'history.txt')
        os.makedirs(os.path.dirname(self.history_path), exist_ok=True)
        open(self.history_path, 'a', encoding='utf-8').close()

    def start_monitoring(self):
        """Read existing lines, announce start, then watch for updates."""
        self.manual_read()
        self.queue.put(('David', 'Debate started. David will speak first.'))
        if self.observer or not self.history_path:
            return
        handler = HistoryMonitor(self.history_path, self._on_line)
        self.observer = Observer()
        self.observer.schedule(handler, os.path.dirname(self.history_path), recursive=False)
        self.observer.start()

    def _on_line(self, line):
        """Parse `[Author]` tags and content, grouping into speaker chunks."""
        if not self.auto_read_enabled:
            return
        stripped = line.rstrip('\n')
        # full tag+content on one line?
        m_cont = re.match(r'^\[(?P<author>.+?)\]\s*(?P<content>.+)', stripped)
        # standalone tag line?
        m_tag  = re.match(r'^\[(?P<author>.+?)\]\s*$', stripped)
        if m_cont:
            author  = m_cont.group('author')
            content = m_cont.group('content')
            self._flush_buffer()
            self._current_speaker = author
            self._buffer_lines   = [content]
        elif m_tag:
            author = m_tag.group('author')
            self._flush_buffer()
            self._current_speaker = author
        else:
            if not self._current_speaker:
                return
            self._buffer_lines.append(stripped)

    def manual_read(self):
        """Read the entire file top‑to‑bottom and queue all existing chunks."""
        if not self.history_path:
            return
        self._current_speaker = None
        self._buffer_lines    = []
        try:
            with open(self.history_path, 'r', encoding='utf-8') as f:
                for raw in f:
                    self._on_line(raw.rstrip('\n'))
        except Exception:
            pass
        self._flush_buffer()

    def _flush_buffer(self):
        """Flush any accumulated lines as one chunk into the queue."""
        if self._current_speaker and self._buffer_lines:
            chunk = "\n".join(self._buffer_lines)
            self.queue.put((self._current_speaker, chunk))
        self._buffer_lines = []

    def stop_monitoring(self):
        """Stop watching the file and flush any leftover text."""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None
        self._flush_buffer()

    def shutdown(self):
        """Cleanly shut down dispatcher and watcher."""
        self.stop_monitoring()
        self._stop_event.set()
        self.dispatcher.join()

    def set_voice(self, speaker: str, voice_name: str):
        """Change and persist the installed voice for a speaker."""
        for v in self.engine.getProperty('voices'):
            if voice_name.lower() in v.name.lower():
                self.voice_map[speaker]     = v.id
                self._config_names[speaker] = v.name
                self._save_config()
                break

    def toggle_auto_read(self, enabled: bool):
        """Enable or disable automatic reading of new lines."""
        self.auto_read_enabled = enabled

    def _dispatch_loop(self):
        """Continuously pull speaker chunks and delegate speaking."""
        while not self._stop_event.is_set():
            try:
                speaker, chunk = self.queue.get(timeout=0.1)
            except queue.Empty:
                continue
            # ensure only one utterance at a time
            self._semaphore.acquire()
            t = threading.Thread(
                target=self._speak_and_release,
                args=(speaker, chunk),
                daemon=True
            )
            t.start()

    def _speak_and_release(self, speaker: str, chunk: str):
        """Speak the chunk, handle errors, then release and mark done."""
        try:
            vid = self.voice_map.get(speaker, self.voice_map['David'])
            self.engine.setProperty('voice', vid)
            self.engine.say(chunk)
            self.engine.runAndWait()
        except Exception as e:
            print(f"[TTS ERROR] {speaker}: {e}")
            # retry once after a brief pause
            try:
                time.sleep(0.2)
                vid = self.voice_map.get(speaker, self.voice_map['David'])
                self.engine.setProperty('voice', vid)
                self.engine.say(chunk)
                self.engine.runAndWait()
            except Exception as e2:
                print(f"[TTS RETRY FAILED] {speaker}: {e2}")
        finally:
            self.queue.task_done()
            self._semaphore.release()
```

TTS.py – Text‑to‑Speech Manager for Debate Simulator
• Reads history.txt top‑to‑bottom, grouping by speaker tags
• Speaks each full response in one chunk with the correct voice
• Watches for new lines and continues until stopped
• Persists voice selections across sessions
**Classes:** HistoryMonitor, TTSManager



---
**Generation Parameters**


```text

You are an expert software engineer.  Carefully read every
file under the target directory (skipping any virtual environment
folders) and produce a comprehensive, well‑structured README in
Markdown.  Focus most of your attention on Python (.py) files: parse
their module‑level docstrings, enumerate classes and functions, and
describe what each does.  Summarise the purpose of non‑Python files
(such as JSON, YAML, text, images) briefly.  Provide an overview of
the project architecture and any dependencies you can infer from the
code.  Include usage notes or examples where appropriate.  Do not
invent information – base your summary solely on the source content.
Use headings, subheadings and lists to organise the README.

```